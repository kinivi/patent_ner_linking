{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-04-27 19:32:03 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# this turns on the autotimer, so that every cell has a timing information below\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime\n",
    "# stop using:\n",
    "# %unload_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.61 s (started: 2022-04-27 19:39:03 +02:00)\n"
     ]
    }
   ],
   "source": [
    "class Hearst_Patterns:\n",
    "    \"\"\" Extracts hearst patterns from a corpus\n",
    "    \"\"\"\n",
    "    def __init__(self, patterns_file=\"patterns.json\", model_path=\"spacy/model-new\", text_path=\"G06K.txt\"):\n",
    "        \"\"\" creates an instance of the class Hearst_Patterns\n",
    "\n",
    "        Args:\n",
    "            patterns_file (path, optional): the json file containing the patterns. Defaults to \"patterns.json\".\n",
    "            model_path (path, optional): the folder containing the NER model to use. Defaults to \"spacy/model-new\".\n",
    "            text_path (path, optional): the file containing the corpus analyse and extract patterns. Defaults to \"G06K.txt\".\n",
    "        \"\"\"\n",
    "\n",
    "        # read the text file\n",
    "        g06k = open(text_path).read().strip()\n",
    "        self.patent_lines = g06k.split('\\n')\n",
    "        \n",
    "        # load the models\n",
    "        self.nlp = spacy.load(model_path)\n",
    "        self.en_nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.nlp.add_pipe(\"merge_entities\")\n",
    "        self.en_nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.patterns = self.load_patterns_from_json(patterns_file)\n",
    "        for name, pattern in self.patterns:\n",
    "            self.matcher.add(name, pattern)\n",
    "\n",
    "        # this list is used in the method get_matches\n",
    "        self.continue_words = [',','and','or',';','also','as well']\n",
    "\n",
    "\n",
    "    def load_patterns_from_json(self, patterns_file):\n",
    "        \"\"\" read the json file and return the list of\n",
    "\n",
    "        Args:\n",
    "            patterns_file (path): the json file containing the patterns\n",
    "\n",
    "        Returns:\n",
    "            List: a list of the hearst patterns found in the json file  \n",
    "        \"\"\"\n",
    "        f = open(patterns_file)\n",
    "        data = json.load(f)\n",
    "        patterns = []\n",
    "        for name, pattern in data.items():\n",
    "            patterns.append((name, pattern))\n",
    "\n",
    "        return patterns\n",
    "\n",
    "\n",
    "    def extract_patterns(self, size=10, save_folder=\".\", start=0):\n",
    "        \"\"\" look for matches in a corpus (text file)\n",
    "\n",
    "        Args:\n",
    "            size (int, optional): the minimum number of matches to be found. Defaults to 10.\n",
    "            save_folder (path, optional): the folder in which save the resulted csv file. Defaults to \".\".\n",
    "            start (int, optional): the first line in which we start to look for matches (useful to continue where you stopped). Defaults to 0.\n",
    "        \"\"\"\n",
    "        extraced_patterns = []\n",
    "\n",
    "        # chose a start\n",
    "        line = start\n",
    "        count = 0\n",
    "\n",
    "        # for the output \n",
    "        print(f'{count} pattern extracted...', end='\\r')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        while count<size:\n",
    "            while True: # we read line by line until finding a match, to keep track of the count \n",
    "                try: # it bugs very rarely, don't know why XD  \n",
    "\n",
    "                    # look for a match\n",
    "                    patterns = self.get_matches(self.patent_lines[line])\n",
    "                    if patterns:\n",
    "                        extraced_patterns += patterns\n",
    "                        break\n",
    "                    print(f'{count} patterns extracted...{line}', end='\\r')\n",
    "                    sys.stdout.flush()\n",
    "                except:\n",
    "                    print(\"An error has occurred\")\n",
    "                \n",
    "                line += 1\n",
    "            count = len(extraced_patterns)\n",
    "            print(f'{count} patterns extracted...{line}', end='\\r')\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "\n",
    "        print(f'({count}) patterns extracted from lines ({start}-{line}))')\n",
    "        save_file = f\"{save_folder}/hearst_patterns.{len(extraced_patterns)}.csv\"\n",
    "        print(f'Patterns saved to {save_file}')\n",
    "        df = pd.DataFrame(extraced_patterns, columns =['word1', 'word2', 'relation', 'label', 'text'])\n",
    "        df.to_csv(save_file)\n",
    "\n",
    "    def get_matches(self, text):\n",
    "        label = {\n",
    "            'rhyper':-1,\n",
    "            'hyper':1,\n",
    "        }\n",
    "        doc = self.nlp('. '+text) # because patterns like < !(bla bla) X > don't work when X is in the beginning of the sentence \n",
    "        \n",
    "        matches = self.matcher(doc)\n",
    "        relations = []\n",
    "        for match_id, start, end in matches:\n",
    "\n",
    "            # get all entities indices in the doc\n",
    "            ent_indices = [i for i in range(start,end) if doc[i].text in [ent.text for ent in doc[start:end].ents]]\n",
    "            if not ent_indices: # no entity found\n",
    "                return []\n",
    "\n",
    "            # extract X...Y from a match ..X...Y.., so now we know that the first and the last token are the entities\n",
    "            span = doc[min(ent_indices):max(ent_indices)+1]\n",
    "\n",
    "            match_info = self.nlp.vocab.strings[match_id]  # Get string representation\n",
    "            match_name = match_info.split('-')[0]   # hyper or rhyper\n",
    "            match_type = match_info.split('-')[1]   # single or multi\n",
    "\n",
    "            np_0 = span[0]  # left term\n",
    "            np_1 = span[-1] # right term (or first right term if multiple)\n",
    "\n",
    "            # all the right terms (ex. for Y...X1, X2, ...Xn) X1...Xn are the right terms\n",
    "            right_terms = [np_1.text]\n",
    "            if match_type==\"multi\": # look for other terms (X2,X3..etc)\n",
    "\n",
    "                # we use the en_core_web_lg model to get the noun chunks\n",
    "                doc_en = self.en_nlp(doc[end:].text)\n",
    "                for d in doc_en:\n",
    "                    # look for entities inside the noun chunk\n",
    "                    matching_ents = [ent.text for ent in doc.ents if ent.text in d.text]\n",
    "                    if matching_ents:\n",
    "                        right_terms.append(matching_ents[0])\n",
    "                    elif d.text not in self.continue_words:  # stop when seeing a word that's not in the list\n",
    "                        break\n",
    "\n",
    "            for term in right_terms:\n",
    "                relations.append((np_0.text, term, match_name, label[match_name], text))\n",
    "\n",
    "        relations = set(relations)\n",
    "        return list(relations)\n",
    "\n",
    "hp = Hearst_Patterns(patterns_file=\"patterns.json\", model_path=\"../spacy/model-new\", text_path=\"../G06K.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-04-21 02:13:14 +02:00)\n"
     ]
    }
   ],
   "source": [
    "hp.extract_patterns(size=50, start=5896, save_folder=\"hearst_patterns/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last line processed is <b>26331</b>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd2970738afc46eb75cb987fc9351c35334a088948a4b8985da4d246077f4773"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
