{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prodigy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our patents, a lot of named entities are abbreviated.\n",
    "For example, `WI-FI Direct` is mentioned as `WFD`, or `P2P Group Owner` as `GO`.\n",
    "\n",
    "Our original model does not recognize these abbreviations and therefore a huge part of the entities are ignored.\n",
    "So we will fine-tune our model using prodigy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a train dataset for fine-tuning our ner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 label(s): TECH\n",
      "/Users/gaetanserre/miniconda3/envs/NLP/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n",
      "✨  Starting the web server at http://localhost:8080 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n",
      "\n",
      "\u001b[38;5;2m✔ Saved 35 annotations to database SQLite\u001b[0m\n",
      "Dataset: fine_tune_g06k\n",
      "Session ID: 2022-04-20_12-40-25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.correct fine_tune_g06k spacy_output/model-best G06K.txt --loader txt --label TECH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Exported 35 annotations from 'fine_tune_g06k' in database SQLite\u001b[0m\n",
      "/Users/gaetanserre/Documents/Projects/patent_ner_linking/terms_g06k/fine_tune_g06k.jsonl\n"
     ]
    }
   ],
   "source": [
    "!prodigy db-out fine_tune_g06k terms_g06k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fine-tune our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "========================= Generating Prodigy config =========================\u001b[0m\n",
      "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
      "\u001b[38;5;4mℹ Using config from base model\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-04-20 12:55:40,635] [INFO] Set up nlp object from config\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 28 | Evaluation: 7 (20% split)\n",
      "Training: 15 | Evaluation: 5\n",
      "Labels: ner (1)\n",
      "[2022-04-20 12:55:40,660] [INFO] Pipeline: ['tok2vec', 'transformer', 'parser', 'ner']\n",
      "[2022-04-20 12:55:40,660] [INFO] Resuming training for: ['ner', 'transformer']\n",
      "[2022-04-20 12:55:40,666] [INFO] Created vocabulary\n",
      "[2022-04-20 12:55:45,445] [INFO] Added vectors: spacy_output/model-best\n",
      "[2022-04-20 12:55:45,522] [INFO] Finished initializing nlp object\n",
      "[2022-04-20 12:55:45,707] [INFO] Initialized pipeline components: ['tok2vec']\n",
      "[2022-04-20 12:55:45,707] [WARNING] [W087] Component 'tok2vec' will be (re)trained, but the component 'parser' depends on it via a listener and is frozen. This means that the performance of 'parser' will be degraded. You can either freeze both, or neither of the two. If you're sourcing the component from an existing pipeline, you can use the `replace_listeners` setting in the config block to replace its token-to-vector listener with a copy and make it independent. For example, `replace_listeners = [\"model.tok2vec\"]` See the documentation for details: https://spacy.io/usage/training#config-components-listeners\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 28 | Evaluation: 7 (20% split)\n",
      "Training: 15 | Evaluation: 5\n",
      "Labels: ner (1)\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'transformer', 'parser', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Frozen components: ['parser']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  -------------  --------  ------  ------  ------  ------\n",
      "/Users/gaetanserre/miniconda3/envs/NLP/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!prodigy train ./prodigy_output/ --ner fine_tune_g06k --base-model spacy_output/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"spacy_output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"According to the technology of PTL 1, it is possible to more safely obtain the public key by reading the QR code.\")\n",
    "\n",
    "colors = {\"TECH\": \"#F67DE3\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9110b66360824545d7a7f0dbdfe5ab5407629db7e29fd63eb77836dd53cde7d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
