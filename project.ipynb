{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604a1483-2ecb-444d-8ffc-802648c34911",
   "metadata": {},
   "source": [
    "# Inforet 2022: Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072fe45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f9c4b-cdb4-4eab-aae2-3f4c6f81793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, regex, timeit, gzip, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import Counter\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.util import Trie\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "tqdm.pandas()\n",
    "spacy.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632e188",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb325a64-2f8b-4f2f-aae8-be361087e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you've already unzipped the file\n",
    "patent_data=open('G06K.txt').read().strip()\n",
    "\n",
    "# split into patents texts | 1 entry = 1 patent\n",
    "patent_texts = patent_data.split('\\n\\n')\n",
    "\n",
    "# split each patent into lines\n",
    "patent_lines = patent_data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb99f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(patent_lines),'patent lines')\n",
    "print(len(patent_texts),'texts of patents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891917b",
   "metadata": {},
   "source": [
    "## 👀 Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e34818-da27-492e-9714-0dec3c0e2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(ngram_range=(2, 3), min_df=10, stop_words=\"english\")\n",
    "X=cvectorizer.fit_transform(patent_lines)\n",
    "\n",
    "Xdf = pd.DataFrame(np.sum(X, axis=0), columns=cvectorizer.get_feature_names()).T.sort_values(by = 0, ascending = False)\n",
    "Xdf.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8543e66",
   "metadata": {},
   "source": [
    "### Manyterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49724c3-92a7-46b9-82df-b61d73d5f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the potential terms\n",
    "mwes = open('manyterms.lower.txt').read().lower().strip().split('\\n')\n",
    "print(mwes[44444:44456])\n",
    "print(len(mwes),'mwes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here lowercase=False option is used to keep the original case of the terms, since we possibly could have term abbreviations. Like API, CAT, etc.\n",
    "cvectorizer = CountVectorizer(ngram_range=(1, 4), stop_words=\"english\", vocabulary=mwes, lowercase=True)\n",
    "X=cvectorizer.fit_transform(patent_texts)\n",
    "\n",
    "# Show top-25 most frequent terms\n",
    "termdf_cv = pd.DataFrame(np.sum(X, axis=0), columns=cvectorizer.get_feature_names()).T.sort_values(by = 0, ascending = False)\n",
    "termdf_cv.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7dbc8d",
   "metadata": {},
   "source": [
    "- [EXPERIMENT] Longer words - more specific terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer with vocabulary\n",
    "# Here lowercase=False option is used to keep the original case of the terms, since we possibly could have term abbreviations. Like API, CAT, etc.\n",
    "cvectorizer = CountVectorizer(ngram_range=(3, 4), stop_words=\"english\", vocabulary=mwes, lowercase=False)\n",
    "X=cvectorizer.fit_transform(patent_lines)\n",
    "\n",
    "# Show top-25 most frequent terms\n",
    "term_cv_long = pd.DataFrame(np.sum(X, axis=0), columns=cvectorizer.get_feature_names()).T.sort_values(by = 0, ascending = False)\n",
    "term_cv_long.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d217e0",
   "metadata": {},
   "source": [
    "## 🪄 SpaCy NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624441b4",
   "metadata": {},
   "source": [
    "Instead of using EntityRuler, we can use the built-in PharaseMatcher and Span for annotation and saving it to the binary `.spacy` format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d85d3d",
   "metadata": {},
   "source": [
    "Let's start from understanding. Here is an example of showing part of text on one patent with default NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(patent_texts[0][18000:20000]) # \n",
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03889dc4",
   "metadata": {},
   "source": [
    "### Create DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7774a1",
   "metadata": {},
   "source": [
    "We need to create propper dataset that is compatible with SpaCy 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(text) for text in termdf_cv.index]\n",
    "matcher.add(\"Tech\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test patent_lines with sciki-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(patent_lines, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985bed1",
   "metadata": {},
   "source": [
    "We are using PharsesMatcher to find entities similar to one from mayterms.txt  \n",
    "Then Span is labeled and saved into the binary `.spacy` format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb58c6",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e334b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"TECH\"\n",
    "doc_bin_train = DocBin() # create a DocBin object\n",
    "\n",
    "# nlp.max_length = 2000000\n",
    "for training_example  in tqdm(train_lines[:40000]): #~50 patents\n",
    "    doc = nlp.make_doc(training_example) \n",
    "    ents = []\n",
    "    \n",
    "    for match_id, start, end in matcher(doc):\n",
    "        #print(i,\"Matched based on lowercase token text:\", doc[:10], '::::::::',doc[start:end],start, end)\n",
    "        span = Span(doc, start, end, label=LABEL)\n",
    "        #print(span, span.label_)\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    print(filtered_ents[:3])\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin_train.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85bdea",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e334b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"TECH\"\n",
    "doc_bin_valid = DocBin() # create a DocBin object\n",
    "nlp = spacy.blank(\"en\")\n",
    "# nlp.max_length = 2000000\n",
    "for training_example  in tqdm(test_lines[:12000]): #~15 patents\n",
    "    doc = nlp.make_doc(training_example) \n",
    "    ents = []\n",
    "    \n",
    "    for match_id, start, end in matcher(doc):\n",
    "        #print(i,\"Matched based on lowercase token text:\", doc[:10], '::::::::',doc[start:end],start, end)\n",
    "        span = Span(doc, start, end, label=LABEL)\n",
    "        #print(span, span.label_)\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    print(filtered_ents[:3])\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin_valid.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed395d11",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4afc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"TECH\"\n",
    "doc_bin_test = DocBin() # create a DocBin object\n",
    "nlp = spacy.blank(\"en\")\n",
    "# nlp.max_length = 2000000\n",
    "for training_example  in tqdm(test_lines[12000:24000]): #~5 patents\n",
    "    doc = nlp.make_doc(training_example) \n",
    "    ents = []\n",
    "    \n",
    "    for match_id, start, end in matcher(doc):\n",
    "        #print(i,\"Matched based on lowercase token text:\", doc[:10], '::::::::',doc[start:end],start, end)\n",
    "        span = Span(doc, start, end, label=LABEL)\n",
    "        #print(span, span.label_)\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    print(filtered_ents[:3])\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin_test.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d46b3d",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin_train.to_disk(\"training_data.spacy\") # save the docbin object\n",
    "doc_bin_valid.to_disk(\"valid_data.spacy\") # save the docbin object\n",
    "doc_bin_test.to_disk(\"test_data.spacy\") # save the docbin object\n",
    "\n",
    "# save train_lines to txt file\n",
    "with open('train_lines.txt', 'w') as f:\n",
    "    for line in train_lines:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "# save train_lines to txt file\n",
    "with open('valid_lines.txt', 'w') as f:\n",
    "    for line in test_lines[:12000]:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "# save test_lines to txt file\n",
    "with open('test_lines.txt', 'w') as f:\n",
    "    for line in test_lines[12000:24000]:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe328ca",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc862e4",
   "metadata": {},
   "source": [
    "Donwnload __base_config.cfg__ for your system at https://spacy.io/usage/training#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to generate full training config\n",
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50a1f1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598a837",
   "metadata": {},
   "source": [
    "Run training. All results are stored into __./spacy_output__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f812d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output ./spacy_output --paths.train ./training_data.spacy --paths.dev ./valid_data.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fcedb",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"./spacy_output/model-best\")\n",
    "\n",
    "colors = {\"TECH\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "for line in test_lines[10000:10005]:\n",
    "    doc = nlp_ner(line)\n",
    "    spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddc7ee-2193-40a8-8002-fd3282d2a8c6",
   "metadata": {},
   "source": [
    "\n",
    "## 🦄 Prodigy: Make it even better\n",
    "\n",
    "For this part i have used this tutorial: https://newscatcherapi.com/blog/train-custom-named-entity-recognition-ner-model-with-spacy-v3  \n",
    "and official documentation: https://spacy.io/usage/training#custom-ner-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c440124-5cd3-465c-8473-10899a3e924e",
   "metadata": {},
   "source": [
    "### 📖 Teach it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce55028",
   "metadata": {},
   "source": [
    "One of the besst feature of Prodigy is that you can focus annotation on the most uncertain enitties.  \n",
    "For this one we use __ner.teach__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984c6d6",
   "metadata": {},
   "source": [
    "As a dataset, used valid_data.txt, since model is already fitter on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9734cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy ner.teach ner_tech  ./spacy_output/model-best  valid_lines.txt --label TECH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0c531",
   "metadata": {},
   "source": [
    "<img src=\"./img/binary.png\" height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1cfb7",
   "metadata": {},
   "source": [
    "Also, instead of binary judging, we can correct model prediction manualy by using __ner.correct__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy ner.correct gold_tech  ./spacy_output/model-best  valid_lines.txt --label TECH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93916b90",
   "metadata": {},
   "source": [
    "<img src=\"./img/annotation.png\" height=420>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1444da",
   "metadata": {},
   "source": [
    "### 🤝 Merge it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b5afc",
   "metadata": {},
   "source": [
    "Now, we need to merge our binary annotation into the __gold dataset__  \n",
    "This means that now we are fixing annotation manually in the text which we are rejected during __ner.teach__  \n",
    "\n",
    "Those annotation can be directly merged into already created dataset(by ner.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy ner.silver-to-gold gold_tech ner_tech ./spacy_output/model-best --label TECH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20cdc7",
   "metadata": {},
   "source": [
    "### 🏋️‍♀️ .. or train it with Prodigy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2703d0c",
   "metadata": {},
   "source": [
    "We can finetune\\ train our existing SpaCy model(pipeline) inside prodigy  \n",
    "\n",
    "Here we train existing `/model_best` and output our finte-tuned model into `spacy_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy train ./prodigy_output --ner gold_tech --eval-split 0.3 --base-model ./spacy_output/model-best "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72d2bc",
   "metadata": {},
   "source": [
    "<img src=\"./img/terminal_training.png\" height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9cd20-a0b9-42e2-976e-a06bf6f244b7",
   "metadata": {},
   "source": [
    "\n",
    "### ✍️ Evaluation \n",
    "- Let's evaluate on the test-lines. They are already randomized, so it's a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"./prodigy_output/model-best\")\n",
    "\n",
    "# load test lines\n",
    "with open('test_lines.txt', 'r') as f:\n",
    "    test_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "colors = {\"TECH\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "for line in test_lines[10000:10010]:\n",
    "    doc = nlp_ner(line)\n",
    "    spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af0cf8",
   "metadata": {},
   "source": [
    "## 🏆 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffee99",
   "metadata": {},
   "source": [
    "#### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b001c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "nlp_ner = spacy.load(\"./spacy_output/model-best\")\n",
    "\n",
    "# load test lines\n",
    "with open('test_lines.txt', 'r') as f:\n",
    "    test_lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# read homonym_list.txt with pairs of homonyms\n",
    "homonyms_df = pd.read_csv('hearst_patterns.30.csv')\n",
    "homonyms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c2a40",
   "metadata": {},
   "source": [
    "#### 🏅 Manual Gold dataset\n",
    "Here we are evaluating NER model on the manually created gold dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8381d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TODO__TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6778b2",
   "metadata": {},
   "source": [
    "#### 🌐 Word-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc70f19",
   "metadata": {},
   "source": [
    "Here we are evaluating extracted Hypernyms using WordNet. Here is an example how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32036e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_rom = wn.synsets('CD-ROM', pos='n')\n",
    "computer = wn.synsets('computer', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in cd_rom:\n",
    "    for synset2 in computer:\n",
    "        print(synset, synset2)\n",
    "        print(\"Score:\", synset.wup_similarity(synset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_rom[0].shortest_path_distance(computer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d8f00",
   "metadata": {},
   "source": [
    "Run on our list of hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a41b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_distance(word1, word2):\n",
    "    parent = wn.synsets(word1.replace(' ', '_'))\n",
    "    subclass = wn.synsets(word2.replace(' ', '_'))\n",
    "    scores = [0]\n",
    "    try:\n",
    "        for synset in parent:\n",
    "            for synset2 in subclass:\n",
    "                scores.append(synset.wup_similarity(synset2))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "    return np.round(max(scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a83633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate throw homonyms_df\n",
    "results_wordnet = []\n",
    "for index, row in tqdm(homonyms_df.iterrows()):\n",
    "    if row[\"label\"] == -1:\n",
    "        parent = row['word1']\n",
    "        subclass = row['word2']\n",
    "    else:\n",
    "        parent = row['word2']\n",
    "        subclass = row['word1']\n",
    "    res = wordnet_distance(parent, subclass)\n",
    "    results_wordnet.append(res)\n",
    "    print(parent, \"⬅️\", subclass, \": \", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee313890",
   "metadata": {},
   "source": [
    "Save to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfee981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to dataframe\n",
    "homonyms_df['wordnet_distance'] = results_wordnet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e286c8",
   "metadata": {},
   "source": [
    "#### 🧪 Spacy embeddings\n",
    "What if model already has links between words? Since it's trained on the corpus data, it should be able to find similarity between words.  \n",
    "This is what can be useful while evaluationg our hyponyms list. We could run it and find low-similar elemts for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load large model for comparison\n",
    "nlp = spacy.load(\"./spacy_output/model-best\")\n",
    "\n",
    "word_1 = nlp(\"cloud platform\")\n",
    "word_2 = nlp(\"service provider\")\n",
    "\n",
    "print(word_1, \"<->\", word_2, word_1.similarity(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6560176",
   "metadata": {},
   "source": [
    "Run on our list of hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bebb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_score(word1, word2):\n",
    "    word1 = nlp(word1)\n",
    "    word2 = nlp(word2)\n",
    "    return word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate throw homonyms_df\n",
    "results_spacy = []\n",
    "for index, row in tqdm(homonyms_df.iterrows()):\n",
    "    if row[\"label\"] == -1:\n",
    "        parent = row['word1']\n",
    "        subclass = row['word2']\n",
    "    else:\n",
    "        parent = row['word2']\n",
    "        subclass = row['word1']\n",
    "\n",
    "    res = spacy_score(parent, subclass)\n",
    "    results_spacy.append(res)\n",
    "    print(parent, \"⬅️\", subclass, \": \", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5f125",
   "metadata": {},
   "source": [
    "Add results to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c635971",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df['spacy_distance'] = results_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92734f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3edec3",
   "metadata": {},
   "source": [
    "#### 📜 Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.entity import WikidataItem, WikidataLexeme, WikidataProperty\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from qwikidata.sparql import (get_subclasses_of_item,\n",
    "                              return_sparql_query_results)\n",
    "import wptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4418a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_name = \"computer\"\n",
    "candidate_name = \"iPad\"\n",
    "\n",
    "# get Wikidata item for parent\n",
    "page = wptools.page(parent_name)\n",
    "data = page.get_parse(show=False)\n",
    "q_parent_class = data.data['wikibase']\n",
    "q_parent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use convenience function to get subclasses of an item as a list of item ids\n",
    "subclasses_list = get_subclasses_of_item(q_parent_class)\n",
    "len(subclasses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb446b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some of this classes\n",
    "for subclass in subclasses_list[:5]:\n",
    "    q42_dict = get_entity_dict_from_api(subclass)\n",
    "    print(WikidataItem(q42_dict).get_label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wptools.page(candidate_name)\n",
    "data = page.get_parse(show=False)\n",
    "data.data['wikibase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84baa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is class `{parent_name}` is a subclass of `{candidate_name}`: \", data.data['wikibase'] in subclasses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a50c67",
   "metadata": {},
   "source": [
    "Run on our list of hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96deb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_is_subclass(word_1, word_2):\n",
    "    parent_name = word_1\n",
    "    candidate_name = word_2\n",
    "\n",
    "    # get Wikidata item for parent\n",
    "    page = wptools.page(parent_name)\n",
    "    try:\n",
    "        data_parent = page.get_parse(show=False)\n",
    "    except:\n",
    "        print(f\"Could not find Wikidata item for `{parent_name}`\")\n",
    "        return \"⚠️\"\n",
    "    q_parent_id = data_parent.data['wikibase']\n",
    "\n",
    "    # get Wikidata item for candidate\n",
    "    page = wptools.page(candidate_name)\n",
    "    try:\n",
    "        data_subclass = page.get_parse(show=False)\n",
    "    except:\n",
    "        print(f\"Could not find Wikidata item for `{candidate_name}`\")\n",
    "        return \"⚠️\"\n",
    "    q_subclass_id = data_subclass.data['wikibase']\n",
    "\n",
    "    # use convenience function to get subclasses of an item as a list of item ids\n",
    "    subclasses_list = get_subclasses_of_item(q_parent_id)\n",
    "\n",
    "    res = q_subclass_id in subclasses_list\n",
    "    if res:\n",
    "        return \"✅\"\n",
    "    else:\n",
    "        return \"❌\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate throw homonyms_df\n",
    "wikidata_results = []\n",
    "for index, row in tqdm(homonyms_df.iterrows()):\n",
    "    if row[\"label\"] == -1:\n",
    "        parent = row['word1']\n",
    "        subclass = row['word2']\n",
    "    else:\n",
    "        parent = row['word2']\n",
    "        subclass = row['word1']\n",
    "\n",
    "    res = wikidata_is_subclass(parent, subclass)\n",
    "    wikidata_results.append(res)\n",
    "    print(parent, \"⬅️\", subclass, \": \",wikidata_is_subclass(parent, subclass))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9e9fe",
   "metadata": {},
   "source": [
    "Add results to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb34a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df['wikidata_is_subclass'] = wikidata_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d0ab0",
   "metadata": {},
   "source": [
    "#### Process table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns wordnet_is_subclass, fill with \"⚠️\" if not 0 or np.nan, otherwise fill with \"✅\" if wordnet_distance > 0.7, otherwise fill with \"❌\"\n",
    "homonyms_df['wordnet_is_subclass'] = homonyms_df['wordnet_distance'].apply(lambda x: \"⚠️\" if x == 0 or np.isnan(x) else \"✅\" if x > 0.7 else \"❌\")\n",
    "\n",
    "# do the same for spacy\n",
    "homonyms_df['spacy_is_subclass'] = homonyms_df['spacy_distance'].apply(lambda x: \"⚠️\" if x == 0 or np.isnan(x) else \"✅\" if x > 0.3 else \"❌\")\n",
    "\n",
    "\n",
    "homonyms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40bf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df[[ \"label\", \"word1\", \"word2\", \"wordnet_is_subclass\", \"spacy_is_subclass\", \"wikidata_is_subclass\"]].to_csv(\"./homonyms_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d553a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "homonyms_df[[\"label\", \"word1\", \"word2\",  \"wordnet_distance\", \"spacy_distance\", \"wordnet_is_subclass\", \"spacy_is_subclass\", \"wikidata_is_subclass\"]].to_csv(\"./homonyms_results_detailed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "699871839e1c28f0580e7fc2e38fa027ae9eff10cc3ccb1dbb0c1a4aca421727"
  },
  "kernelspec": {
   "display_name": "uni-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
