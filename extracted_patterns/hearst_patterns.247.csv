,word1,word2,relation,label,text
0,wireless communication,wireless LAN,rhyper,-1,"104 denotes a wireless communication unit configured to perform a wireless communication such as a wireless LAN in conformity to the IEEE 802.11 series. A wireless communication unit 104 is constituted by a chip that performs a wireless communication. The wireless communication includes a short-range wireless communication such as, for example, Near Field Communication (registered trademark) (which will be hereinafter denoted as NFC). 105 denotes a display unit that performs various displays and has a function enabling output of information, which can be visually recognized by the user, such as an LCD or an LED, or sound output such as a speaker. The display unit 105 is provided with a function for outputting at least one of the visual information or the sound information. Display of the QR code is also executed by the display unit 105. With regard to the QR code, in addition to the display through the display unit 105, the QR code may be affixed to a casing of the communication device in the form of a sticker or the like, or may also be affixed to an instruction manual or a package such as cardboard at the time of sale of the communication device. 106 denotes a wireless LAN antenna control unit, and 107 denotes a wireless LAN antenna. 108 denotes an operation unit for the user to perform various inputs and the like and operate the communication device. 109 denotes an imaging unit functioning as reading means by capturing the QR code or the like."
1,wireless communication,wireless LAN,rhyper,-1,"204 denotes a wireless communication unit configured to perform a wireless communication such as a wireless LAN in conformity to the IEEE 802.11 series. The wireless communication unit 204 is constituted by a chip that performs a wireless communication. The wireless communication includes a short-range wireless communication such as, for example, NFC. 205 denotes a display unit that performs various displays and has a function enabling output of information, which can be visually recognized by the user, such as an LCD or an LED, or sound output such as a speaker. The display unit 205 is provided with a function for outputting at least one of the visual information or the sound information. Display of the QR code is also executed by the display unit 205. With regard to the QR code, in addition to the display through the display unit 205, the QR code may be affixed to a casing of the communication device in the form of a sticker or the like or may also be affixed to an instruction manual or a package such as cardboard at the time of sale of the communication device. 206 denotes a wireless LAN antenna control unit, and 207 denotes a wireless LAN antenna. 208 denotes an operation unit for the user to perform various inputs and the like and operate the communication device. It should be noted that the configuration illustrated in Fig. 1 is an example, and the communication device may also include another hardware configuration. For example, in a case where the communication device is a printer, the communication device may also include a printing unit in addition to the configuration illustrated in Fig. 1. In addition, in a case where the communication device is a camera, the communication device may also include a capturing unit."
2,semiconductor memory,random access,rhyper,-1,"In the case where the processing circuit 1801 corresponds to the CPU 1804, the function of each of the to-be-classified data generating unit 11, the division candidate selecting unit 12, the connection density calculating unit 13, and the classifying unit 14 are implemented by software, firmware, or a combination of software and firmware. That is, the to-be-classified data generating unit 11, the division candidate selecting unit 12, the connection density calculating unit 13, and the classifying unit 14 are implemented by a processing circuit such as the CPU 1804, a system large scale integration (LSI) that execute programs stored in a hard disk drive (HDD) 1802, the memory 1803, and the like. It is also said that programs stored in the HDD 1802, the memory 1803, and the like causes a computer to execute the procedures and methods of the to-be-classified data generating unit 11, the division candidate selecting unit 12, the connection density calculating unit 13, and the classifying unit 14. Here, the memory 1803 may be a nonvolatile or volatile semiconductor memory such as a random access memory (RAM), a read only memory (ROM), a flash memory, an erasable programmable read only memory (EPROM), or an electrically erasable programmable read only memory (EEPROM), a magnetic disc, a flexible disc, an optical disc, a compact disc, a mini disc, a digital versatile disk (DVD), or the like."
3,visible light,ultraviolet light,rhyper,-1,"Fig. 1 shows the driver monitoring system 1. The driver monitoring system 1 comprises the camera 10, the mirror device 20 and the control unit 30. Further, an instrument cluster 3, on which the mirror device 20 is located, is shown. The face of the driver 2 is located in front of the instrument cluster 3, wherein the mirror device 20 is located between the face of the driver 2 and the instrument cluster 3. The light beam 16 or the image is represented by the dotted lines from the camera 10 over the mirror device 20 to the face of the driver 2. Further, it is shown that the camera 10 faces not directly the face of the driver 2. In contrast, the camera 10 captures a reflected image by the mirror device 20 of the face of the driver 2. In other words, the camera 10 is virtually positioned at the position of the mirror device 20. The mirror device 20 may be a micro mirror foil, which is transparent for visible light and reflective for non-visible light, such as infrared light or ultraviolet light. Thus, the driver 2 is still capable of seeing the instruments of the instrument cluster 3, while the camera 10 captures an image, reflected by the mirror device 20, of the face of the driver 2. The image captured by the camera 10 may be an infrared or ultraviolet light image. Further, the camera 10 may be positioned at an arbitrary position in/on the instrument cluster 3, since the mirror device 20 can be adapted to each arbitrary position of the camera 10, such that the camera 10 is virtually positioned at the centre of the instrument cluster 3 directly facing the driver 2. The mirror device 20 may comprise a plurality of mirror elements, which are aligned according to the particular use case."
4,visible light,infrared light,rhyper,-1,"Fig. 1 shows the driver monitoring system 1. The driver monitoring system 1 comprises the camera 10, the mirror device 20 and the control unit 30. Further, an instrument cluster 3, on which the mirror device 20 is located, is shown. The face of the driver 2 is located in front of the instrument cluster 3, wherein the mirror device 20 is located between the face of the driver 2 and the instrument cluster 3. The light beam 16 or the image is represented by the dotted lines from the camera 10 over the mirror device 20 to the face of the driver 2. Further, it is shown that the camera 10 faces not directly the face of the driver 2. In contrast, the camera 10 captures a reflected image by the mirror device 20 of the face of the driver 2. In other words, the camera 10 is virtually positioned at the position of the mirror device 20. The mirror device 20 may be a micro mirror foil, which is transparent for visible light and reflective for non-visible light, such as infrared light or ultraviolet light. Thus, the driver 2 is still capable of seeing the instruments of the instrument cluster 3, while the camera 10 captures an image, reflected by the mirror device 20, of the face of the driver 2. The image captured by the camera 10 may be an infrared or ultraviolet light image. Further, the camera 10 may be positioned at an arbitrary position in/on the instrument cluster 3, since the mirror device 20 can be adapted to each arbitrary position of the camera 10, such that the camera 10 is virtually positioned at the centre of the instrument cluster 3 directly facing the driver 2. The mirror device 20 may comprise a plurality of mirror elements, which are aligned according to the particular use case."
5,mobile device,mobile phone,rhyper,-1,"ID devices 120 may be physical, like a chipcard, or virtual, such as a software application on a mobile device. In combination with interaction technology such as NFC or Bluetooth, a mobile device, such as mobile phone, can be used as a virtual ID device 120 to initiate contactless ID transactions 100 and to interact with merchant POI terminals 110. Such applications include Apple Pay, Android Pay, X Pay and Samsung Pay. These standardized data interactions are very limited - the ID device 120 limits itself to operation as a chipcard to ensure the largest degree of interoperability."
6,telecommunication network,the internet,rhyper,-1,"In an embodiment, the memory 13 can be part of the processing unit 10. In an alternative embodiment, the memory can be separate from the processing unit 10 and located at a distance thereof. The memory can in that case be accessed by the processing unit by wireless connection. In that case, the processing unit 10 preferably comprises a communication interface for remote connection to the memory 13 through a telecommunication network such as the internet."
7,reflective glass,one-way mirror,rhyper,-1,"The picture input device 12 is preferably a camera installed in a photo studio installed in the shop or beauty salon, and configured for taking pictures in controlled conditions. The photo studio may comprise a screen or background of neutral color (white, grey), at least one lamp such as a softbox, and the camera at a fixed location relative to the screen or background. The camera may be hidden behind a reflective glass such as a one-way mirror for a better integration in the shop or beauty salon."
8,the system,digital camera,hyper,1,"In an alternative embodiment, the camera of the system is a digital camera of a mobile phone, tablet or smartphone, with which a user can take a picture of himself/herself. The picture is then transferred to the processing unit 10 for further processing."
9,mass memory device,hard drive,rhyper,-1,"The client computer of the example comprises a central processing unit (CPU) 1010 connected to an internal communication BUS 1000, a random access memory (RAM) 1070 also connected to the BUS. The client computer is further provided with a graphical processing unit (GPU) 1110 which is associated with a video random access memory 1100 connected to the BUS. Video RAM 1100 is also known in the art as frame buffer. A mass storage device controller 1020 manages accesses to a mass memory device, such as hard drive 1030. Mass memory devices suitable for tangibly embodying computer program instructions and data include all forms of nonvolatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM disks 1040. Any of the foregoing may be supplemented by, or incorporated in, specially designed ASICs (application-specific integrated circuits). A network adapter 1050 manages accesses to a network 1060. The client computer may also include a haptic device 1090 such as cursor control device, a keyboard or the like. A cursor control device is used in the client computer to permit the user to selectively position a cursor at any desired location on display 1080. In addition, the cursor control device allows the user to select various commands, and input control signals. The cursor control device includes a number of signal generation devices for input control signals to system. Typically, a cursor control device may be a mouse, the button of the mouse being used to generate the signals. Alternatively or additionally, the client computer system may comprise a sensitive pad, and/or a sensitive screen."
10,virtual environment,virtual environment,hyper,1,"The three-dimensional virtual environment is a virtual environment obtained by a processing unit through modeling. The three-dimensional virtual environment may be a room, a building, a game scene, or the like. Optionally, the three-dimensional virtual environment includes a virtual coordinate system formed by an x axis, a y axis, and a z axis. Any two axes of the x axis, y axis, and z axis are perpendicular to each other."
11,two-dimensional code,a square,rhyper,-1,"To describe the structure of the two-dimensional code in more detail, the following embodiments are all described by using the embodiments in FIG. 2 to FIG. 4 and using a two-dimensional code including a square module array consisting of 19∗19 modules as an example."
12,the image,two-dimensional code,rhyper,-1,"In an implementation, after obtaining the image including the two-dimensional code, the terminal performs binarization on the image. Binarization is an image processing manner, in which a pixel whose gray scale is greater than a threshold is changed into a black pixel, and a pixel whose gray scale is less than the threshold is changed into a white pixel. In an image on which binarization has been performed, the first-type modules in the square module array are unitized as black modules, and the second-type modules are unitized as white modules."
13,geographical location data,longitude and latitude,rhyper,-1,"For example, geographical location data, such as longitude and latitude coordinates, corresponding to each three-dimensional laser point in a three-dimensional laser point cloud set is obtained according to a correspondence between three-dimensional laser point cloud data and GPS data that is collected by a navigation system. Longitude and latitude coordinates of shoulder location points on a straight line are obtained. The shoulder line is drawn on a map according to the longitude and latitude coordinates. The shoulder line drawn on the map may be used for automated vehicle operation."
14,geographical location data,longitude and latitude,rhyper,-1,"For example, geographical location data, such as longitude and latitude coordinates, corresponding to each three-dimensional laser point in a three-dimensional laser point cloud set may be obtained according to a correspondence between three-dimensional laser point cloud data and GPS data that is collected by a navigation system. Longitude and latitude coordinates of shoulder location points on a shoulder line are obtained. The shoulder line is drawn on a map according to the longitude and latitude coordinates. The shoulder line drawn on the map may be used for automated vehicle operation."
15,information processing system,electronic pen,rhyper,-1,"In this way, in the information processing system including the electronic pen 1 and the tablet device 2 according to the first embodiment, the electronic tag transmitter 6 that transmits specific identification information as tag information is prepared, and the electronic pen 1 is provided with the function of receiving the tag information from the electronic tag transmitter 6. When the electronic pen 1 receives the tag information from the electronic tag transmitter 6, the electronic pen 1 transmits the received tag information as additional information of the position detection signal to the tablet device 2 included in the position detection apparatus. The tablet device 2 detects the information of the position instructed by the electronic pen 1 based on the position detection signal and associates and stores (including temporary storage) the detected information of the position instructed by the electronic pen 1 and the tag information."
16,the signal,start signal,hyper,1,"In this case, the first predetermined cycle (Td) after the position detection signal transmission period is always in the high level, and the signal is a start signal of (C) of FIG. 28. The start signal is a timing signal that allows the coordinate data formation unit 201D of the tablet device 2D to accurately determine the subsequent transmission timing of additional information. Note that instead of the start signal, the burst signal of the position detection signal transmission period can be used as the timing signal."
17,storage medium,computer readable,rhyper,-1,"These comprehensive or specific embodiments can be materialized with a system, method, integrated circuit, computer program, or a storage medium such as a computer readable CD-ROM or any combination of a system, method, integrated circuit, computer program and storage medium."
18,storage medium,computer readable,rhyper,-1,"These comprehensive or specific embodiments of the invention can be materialized with a system, method, integrated circuit, computer program, or a storage medium such as a computer readable CD-ROM or any combination of a system, method, integrated circuit, computer program, and storage medium."
19,storage medium,hard disk,rhyper,-1,"In each of the above embodiments, each constituent element can be materialized by being set up with a dedicated hardware or by executing software program suited to each constituent element. Each constituent element can be materialized by a program execution unit such as a CPU or a processor reading out and executing a software program recorded on a storage medium such as a hard disk or semiconductor memory. In this regard, software materialized with the pain estimation apparatus of each of the above embodiments is a program such as those described below."
20,storage medium,semiconductor memory,rhyper,-1,"In each of the above embodiments, each constituent element can be materialized by being set up with a dedicated hardware or by executing software program suited to each constituent element. Each constituent element can be materialized by a program execution unit such as a CPU or a processor reading out and executing a software program recorded on a storage medium such as a hard disk or semiconductor memory. In this regard, software materialized with the pain estimation apparatus of each of the above embodiments is a program such as those described below."
21,storage medium,hard disk,rhyper,-1,"In each of the above embodiments, each constituent element can be materialized by being configured with a dedicated hardware or by executing software program suited to each constituent element. Each constituent element can be materialized by a program execution unit such as a CPU or a processor reading out and executing a software program recorded on a storage medium such as a hard disk or semiconductor memory. In this regard, software materialized with the pain estimation apparatus of each of the above embodiments is a program such as those described below."
22,storage medium,semiconductor memory,rhyper,-1,"In each of the above embodiments, each constituent element can be materialized by being configured with a dedicated hardware or by executing software program suited to each constituent element. Each constituent element can be materialized by a program execution unit such as a CPU or a processor reading out and executing a software program recorded on a storage medium such as a hard disk or semiconductor memory. In this regard, software materialized with the pain estimation apparatus of each of the above embodiments is a program such as those described below."
23,electronic device,mobile phone,rhyper,-1,"Fig 2A schematically shows an example embodiment of the fingerprint sensing system according to the present invention, in the form of a semiconductor-based fingerprint sensor 3. As can be seen in fig 2A, the fingerprint sensor 3 comprises a sensor array 7, and an interface 9 for receiving power for operation of the fingerprint sensor 3 and for interacting with processing circuitry comprised in the electronic device, such as the mobile phone 1 in fig 1A or the smart card 5 in fig 1B. The sensor array 7 comprises a large number of sensing structures 10 (only one of the sensing structures has been indicated with a reference numeral to avoid cluttering the drawing). As is schematically indicated in the enlarged illustrations of the fingerprint sensor 3 in fig 2A, the fingerprint sensor 3 further comprises edge-compensating structures, including left proximal edge-compensating structures 11, right proximal edge-compensating structures 13, left distal edge-compensating structures 15, right distal edge-compensating structures 17, top proximal edge-compensating structures 19, bottom proximal edge-compensating structures 21, top distal edge-compensating structures 23, and bottom distal edge-compensating structures 25."
24,neural network,convolutional neural network,rhyper,-1,"First, an example of a situation in which the present invention is applied will be described using FIG. 1. FIG. 1 is a functional block diagram schematically illustrating an example of a situation to which a data generation apparatus 100 according to the present embodiment is applied. The data generation apparatus 100 according to the present embodiment includes identification devices (a, b, c, A, B, ...) using a neural network such as a convolutional neural network (hereinafter referred to as ""CNN""), for example. The identification devices (a, b, c, A, B, ...) are identification devices that are generated by machine learning using learning data that includes at least images of objects to be inspected and information (labels) indicating whether or not a part to be detected is included in the images, and can output information indicating whether or not a part to be detected is included in the input image. The data generation apparatus 100 is an apparatus that determines, using the identification devices (a, b, c, A, B, ...), whether or not an object to be inspected includes the part to be detected based on the image of the object to be inspected, accepts an input regarding whether or not the determination result is correct with a user interface 170, evaluates whether or not the determination result is correct based on the input with an evaluation unit 127, and if the determination result is evaluated to be not correct, generates new learning data by associating the image of the object to be inspected or a composite image generated based on the image of the object to be inspected with information in which the determination result is corrected. Here, the information in which the determination result is corrected may be information regarding a correct determination result, and may be information indicating that the image of the object to be inspected does not include the part to be detected, or information indicating that the image of the object to be inspected includes the part to be detected, for example. Also, the information indicating that a part to be detected is not included may be information indicating that a specific region of the image does not include the part to be detected, and the information indicating that a part to be detected is included may be information indicating that a specific region of the image includes the part to be detected. In the present embodiment, a case will be described where the part to be detected is a defective part included in an object to be inspected. However, the part to be detected is not limited to a defective part included in an object to be inspected, and may also be any part included in the object to be inspected."
25,communication network,the Internet,rhyper,-1,"As shown in FIG. 1, the data generation apparatus 100 includes an image capturing apparatus 153 that acquires an image of an object to be inspected, a determination unit 152 that determines whether or not the object to be inspected includes a defect based on the image using the identification devices (a, b, c, A, B, ...) that have been trained using learning data, a user interface 170 that accepts an input indicating whether or not the result determined by the determination unit 152 is correct, and a generation unit 121 that generates new learning data by associating information in which the determination result is corrected with at least one of the image and a composite image generated based on the image when the user interface 170 has accepted an input indicating that the determination result is not correct. Here, the measurement unit 150 is an example of an ""acquisition unit"" of this invention. In this specification, an image of the object to be inspected that is newly captured by the image capturing apparatus 153 is referred to as a ""measurement image"", and images of the objects to be inspected that are collected in advance for training identification devices (a, b, c, A, B ...) are referred to as ""sample images"". Note that the data generation apparatus 100 need not include the image capturing apparatus 153, and may also acquire a measurement image from an image capturing apparatus that is provided separately. Also, the data generation apparatus 100 need not include the determination unit 152, and may also acquire a determination result from a determination unit that is provided separately. Furthermore, the data generation apparatus 100 need not include the measurement unit 150. When a measurement image or a determination result is to be acquired from an image capturing apparatus or a determination unit that is provided separately, the data generation apparatus 100 may also acquire the measurement image or the determination result via a communication network such as the Internet."
26,communication network,the Internet,rhyper,-1,"Note that the data generation apparatus 100 need not include the measurement unit 150. For example, capturing of a measurement image and determination made by the determination unit 152 need not be performed by the data generation apparatus 100, and may be performed by a measurement apparatus that is provided separately from the data generation apparatus 100. In this case, the measurement apparatus provided separately from the data generation apparatus 100 may be used by an operator that operates the data generation apparatus 100. Also, the measurement apparatus that is provided separately from the data generation apparatus 100 may be used by an operator that is different from the operator that operates the data generation apparatus 100, and the data generation apparatus 100 may acquire the measurement image captured by the measurement apparatus and the result determined by the determination unit 152 via a communication network such as the Internet."
27,semiconductor memory,flash memory,rhyper,-1,"A storage medium is a medium that stores information such as programs and the like via an electrical, magnetic, optical, mechanical or chemical effect such that the stored information such as programs can be read by an apparatus such as a computer. The storage medium may be a CD (Compact Disk) or a DVD (Digital Versatile Disk), for example, and may store the data generation program 102a. The data generation program 102a stored in the storage medium is copied to the auxiliary storage unit 103, and may also be copied to the auxiliary storage unit 102 and the main storage unit 105. In FIG. 4, a disk-type storage medium such as a CD or a DVD is illustrated as an example of the storage medium. However, the type of the storage medium is not limited to the disk type, and may be a type other than the disk type. A semiconductor memory such as a flash memory can be given as an example of a storage medium other than a disk-type storage medium. Also, the data generation apparatus 100 may acquire data such as the data generation program 102a using wired or wireless communication via a network."
28,optical media,optical discs,rhyper,-1,"The method according to the above-described example embodiments may be recorded in non-transitory computer-readable media including program instructions to implement various operations that may be performed by a computer. The media may also include, alone or in combination with the program instructions, data files, data structures, and the like. The program instructions recorded on the media may be those designed and constructed for the purposes of the example embodiments, or they may be of the well-known kind and available to those having skill in the computer software arts. Examples of non-transitory computer-readable media include magnetic media such as hard disks, floppy disks, and magnetic tape; optical media such as CD ROM discs and DVDs; magneto-optical media such as optical discs; and hardware devices that are configured to store and perform program instructions, such as read-only memory (ROM), random access memory (RAM), flash memory, and the like. Examples of program instructions include both machine code, such as code produced by a compiler, and files containing higher level code that may be executed by the computer using an interpreter. The described hardware devices may be configured to act as one or more software modules to perform the operations of the above-described example embodiments, or vice versa."
29,magnetic media,hard disks,rhyper,-1,"The method according to the above-described example embodiments may be recorded in non-transitory computer-readable media including program instructions to implement various operations that may be performed by a computer. The media may also include, alone or in combination with the program instructions, data files, data structures, and the like. The program instructions recorded on the media may be those designed and constructed for the purposes of the example embodiments, or they may be of the well-known kind and available to those having skill in the computer software arts. Examples of non-transitory computer-readable media include magnetic media such as hard disks, floppy disks, and magnetic tape; optical media such as CD ROM discs and DVDs; magneto-optical media such as optical discs; and hardware devices that are configured to store and perform program instructions, such as read-only memory (ROM), random access memory (RAM), flash memory, and the like. Examples of program instructions include both machine code, such as code produced by a compiler, and files containing higher level code that may be executed by the computer using an interpreter. The described hardware devices may be configured to act as one or more software modules to perform the operations of the above-described example embodiments, or vice versa."
30,communication network,cellular network,rhyper,-1,"The communication module 190 may support establishing a direct (e.g., wired) communication channel or a wireless communication channel between the electronic device 101 and the external electronic device (e.g., the first external electronic device 102, the second external electronic device 104, or the server 108) and performing communication via the established communication channel. The communication module 190 may include one or more communication processors that are operable independently from the processor 120 (e.g., the AP) and supports a direct (e.g., wired) communication or a wireless communication. According to an embodiment of the disclosure, the communication module 190 may include a wireless communication module 192 (e.g., a cellular communication module, a short-range wireless communication module, or a global navigation satellite system (GNSS) communication module) or a wired communication module 194 (e.g., a local area network (LAN) communication module or a power line communication (PLC) module). A corresponding one of these communication modules may communicate with the external electronic device via the first network 198 (e.g., a short-range communication network, such as BluetoothTM, wireless-fidelity (Wi-Fi) direct, or infrared data association (IrDA)) or the second network 199 (e.g., a long-range communication network, such as a cellular network, the Internet, or a computer network (e.g., LAN or wide area network (WAN)). These various types of communication modules may be implemented as a single component (e.g., a single chip), or may be implemented as multi components (e.g., multi chips) separate from each other. The wireless communication module 192 may identify and authenticate the electronic device 101 in a communication network, such as the first network 198 or the second network 199, using subscriber information (e.g., international mobile subscriber identity (IMSI)) stored in the subscriber identification module 196."
31,communication network,the Internet,rhyper,-1,"The communication module 190 may support establishing a direct (e.g., wired) communication channel or a wireless communication channel between the electronic device 101 and the external electronic device (e.g., the first external electronic device 102, the second external electronic device 104, or the server 108) and performing communication via the established communication channel. The communication module 190 may include one or more communication processors that are operable independently from the processor 120 (e.g., the AP) and supports a direct (e.g., wired) communication or a wireless communication. According to an embodiment of the disclosure, the communication module 190 may include a wireless communication module 192 (e.g., a cellular communication module, a short-range wireless communication module, or a global navigation satellite system (GNSS) communication module) or a wired communication module 194 (e.g., a local area network (LAN) communication module or a power line communication (PLC) module). A corresponding one of these communication modules may communicate with the external electronic device via the first network 198 (e.g., a short-range communication network, such as BluetoothTM, wireless-fidelity (Wi-Fi) direct, or infrared data association (IrDA)) or the second network 199 (e.g., a long-range communication network, such as a cellular network, the Internet, or a computer network (e.g., LAN or wide area network (WAN)). These various types of communication modules may be implemented as a single component (e.g., a single chip), or may be implemented as multi components (e.g., multi chips) separate from each other. The wireless communication module 192 may identify and authenticate the electronic device 101 in a communication network, such as the first network 198 or the second network 199, using subscriber information (e.g., international mobile subscriber identity (IMSI)) stored in the subscriber identification module 196."
32,communication network,computer network,rhyper,-1,"The communication module 190 may support establishing a direct (e.g., wired) communication channel or a wireless communication channel between the electronic device 101 and the external electronic device (e.g., the first external electronic device 102, the second external electronic device 104, or the server 108) and performing communication via the established communication channel. The communication module 190 may include one or more communication processors that are operable independently from the processor 120 (e.g., the AP) and supports a direct (e.g., wired) communication or a wireless communication. According to an embodiment of the disclosure, the communication module 190 may include a wireless communication module 192 (e.g., a cellular communication module, a short-range wireless communication module, or a global navigation satellite system (GNSS) communication module) or a wired communication module 194 (e.g., a local area network (LAN) communication module or a power line communication (PLC) module). A corresponding one of these communication modules may communicate with the external electronic device via the first network 198 (e.g., a short-range communication network, such as BluetoothTM, wireless-fidelity (Wi-Fi) direct, or infrared data association (IrDA)) or the second network 199 (e.g., a long-range communication network, such as a cellular network, the Internet, or a computer network (e.g., LAN or wide area network (WAN)). These various types of communication modules may be implemented as a single component (e.g., a single chip), or may be implemented as multi components (e.g., multi chips) separate from each other. The wireless communication module 192 may identify and authenticate the electronic device 101 in a communication network, such as the first network 198 or the second network 199, using subscriber information (e.g., international mobile subscriber identity (IMSI)) stored in the subscriber identification module 196."
33,machine learning algorithm,deep learning,rhyper,-1,"The machine learning may include an algorithm technology of classifying/learning features of input data by itself, and the element technology may include a technology of simulating functions such as recognition, decision, and the like, of a human brain using a machine learning algorithm such as deep learning, or the like, and may include technical fields such as linguistic understanding, visual understanding, inference/prediction, knowledge representation, a motion control, and the like."
34,principal component analysis,statistical method,hyper,1,"Meanwhile, the principal component analysis is a statistical method of extracting principal components concisely expressing variance type patterns of many variables as a linear combination of original variables. That is, in the case in which p variables exist, information obtained from the p variables may be reduced to k variables significantly smaller than p. In the above example, c feature dimensions may be considered as being reduced to c' feature dimensions. However, the number of reduced feature dimensions is not limited, but may also be changed."
35,mobile terminal,personal digital assistant,rhyper,-1,"FIG. 1 is a schematic diagram of an example apparatus configured for performing any of the operations described herein. Apparatus 20 is an example embodiment that may be embodied by or associated with any of a variety of computing devices that include or are otherwise associated with a device configured for providing a navigation system user interface. For example, the computing device may be a mobile terminal, such as a personal digital assistant (PDA), mobile telephone, smart phone, personal navigation device, smart watch, tablet computer, camera or any combination of the aforementioned and other types of voice and text communications systems. However, in a preferred embodiment the apparatus 20 is embodied or partially embodied by a electronic control unit of a vehicle that supports safety-critical systems such as the powertrain (engine, transmission, electric drive motors, etc.), steering (e.g., steering assist or steer-by-wire), and braking (e.g., brake assist or brake-by-wire). Optionally, the computing device may be a fixed computing device, such as a built-in vehicular navigation device, assisted driving device, or the like."
36,propulsion systems,gas turbine engine,rhyper,-1,"With reference to FIG. 1, an aircraft 1 in accordance with various embodiments may comprise wings 10, fuselage 20, empennage 22 and aircraft systems, for example, landing gear such as landing gear 32 and propulsion systems such as gas turbine engine 12. An XYZ axes is used throughout the drawings to illustrate the axial (y), forward (x) and vertical (z) directions relative to aircraft 1. Landing gear 32, may generally support aircraft 1 when aircraft is not in flight, allowing aircraft 1 to taxi, take off, and land without damage. Gas turbine engine 12 is housed in nacelle 16 and has an inlet 26 defined by lip 18 generally provides forward thrust to aircraft 1. Wings 10 and fuselage 20 may generally support aircraft 1 and provide lift while control surfaces 34 and empennage 22 provide directional control when in flight. In various embodiments, in response to aircraft 1 operating in an icing environment, ice may build up on surfaces of aircraft 1 such as, for example, leading edge 14 of wings 10, within inlet 26, or around lip 18 of nacelle 16. In various embodiments, ice may be present on any part of aircraft 1 and may be dislodged and, in response, ingested through inlet 26 tending to damage gas turbine engine 12. In various embodiments, ice buildup on surfaces of aircraft 1 may tend to inhibit the function of control surfaces 34, empennage 22, landing gear 32, and generally tend to reduce the overall performance of aircraft 1."
37,landing gear,propulsion systems,rhyper,-1,"With reference to FIG. 1, an aircraft 1 in accordance with various embodiments may comprise wings 10, fuselage 20, empennage 22 and aircraft systems, for example, landing gear such as landing gear 32 and propulsion systems such as gas turbine engine 12. An XYZ axes is used throughout the drawings to illustrate the axial (y), forward (x) and vertical (z) directions relative to aircraft 1. Landing gear 32, may generally support aircraft 1 when aircraft is not in flight, allowing aircraft 1 to taxi, take off, and land without damage. Gas turbine engine 12 is housed in nacelle 16 and has an inlet 26 defined by lip 18 generally provides forward thrust to aircraft 1. Wings 10 and fuselage 20 may generally support aircraft 1 and provide lift while control surfaces 34 and empennage 22 provide directional control when in flight. In various embodiments, in response to aircraft 1 operating in an icing environment, ice may build up on surfaces of aircraft 1 such as, for example, leading edge 14 of wings 10, within inlet 26, or around lip 18 of nacelle 16. In various embodiments, ice may be present on any part of aircraft 1 and may be dislodged and, in response, ingested through inlet 26 tending to damage gas turbine engine 12. In various embodiments, ice buildup on surfaces of aircraft 1 may tend to inhibit the function of control surfaces 34, empennage 22, landing gear 32, and generally tend to reduce the overall performance of aircraft 1."
38,landing gear,landing gear,rhyper,-1,"With reference to FIG. 1, an aircraft 1 in accordance with various embodiments may comprise wings 10, fuselage 20, empennage 22 and aircraft systems, for example, landing gear such as landing gear 32 and propulsion systems such as gas turbine engine 12. An XYZ axes is used throughout the drawings to illustrate the axial (y), forward (x) and vertical (z) directions relative to aircraft 1. Landing gear 32, may generally support aircraft 1 when aircraft is not in flight, allowing aircraft 1 to taxi, take off, and land without damage. Gas turbine engine 12 is housed in nacelle 16 and has an inlet 26 defined by lip 18 generally provides forward thrust to aircraft 1. Wings 10 and fuselage 20 may generally support aircraft 1 and provide lift while control surfaces 34 and empennage 22 provide directional control when in flight. In various embodiments, in response to aircraft 1 operating in an icing environment, ice may build up on surfaces of aircraft 1 such as, for example, leading edge 14 of wings 10, within inlet 26, or around lip 18 of nacelle 16. In various embodiments, ice may be present on any part of aircraft 1 and may be dislodged and, in response, ingested through inlet 26 tending to damage gas turbine engine 12. In various embodiments, ice buildup on surfaces of aircraft 1 may tend to inhibit the function of control surfaces 34, empennage 22, landing gear 32, and generally tend to reduce the overall performance of aircraft 1."
39,communication network,the Internet,rhyper,-1,"The core part that performs the procedures that are executed by the controller 110 or 210 that comprises a CPU, a RAM, a ROM, and the like can be executed using, instead of a dedicated system, a conventional portable information terminal (a smartphone or a tablet personal computer (PC)), a personal computer, or the like. For example, a portable information terminal that executes the foregoing procedures may be configured by saving and distributing the computer program for executing the foregoing operations on a non-transitory computer-readable recording medium (a flexible disk, a compact disc read only memory (CD-ROM), a digital versatile disc read only memory (DVD-ROM), or the like) and installing the computer program on a portable information terminal. Moreover, an information processing device may be configured by saving the computer program on a storage device of a server device on a communication network such as the Internet and allowing a conventional portable information terminal to download the computer program. A controller that executes a robot operation may be separated from a controller that functions as the image acquirer 111, the image analyzer 112, and the determiner 113."
40,handheld devices,mobile telephones,rhyper,-1,"Computer system 800 also includes one or more instances of a communications interface 870 coupled to bus 810. Communication interface 870 provides a one-way or two-way communication coupling to a variety of external devices that operate with their own processors, such as printers, scanners and external disks. In general the coupling is with a network link 878 that is connected to a local network 880 to which a variety of external devices with their own processors are connected. For example, communication interface 870 may be a parallel port or a serial port or a universal serial bus (USB) port on a personal computer. In some embodiments, communications interface 870 is an integrated services digital network (ISDN) card or a digital subscriber line (DSL) card or a telephone modem that provides an information communication connection to a corresponding type of telephone line. In some embodiments, a communication interface 870 is a cable modem that converts signals on bus 810 into signals for a communication connection over a coaxial cable or into optical signals for a communication connection over a fiber optic cable. As another example, communications interface 870 may be a local area network (LAN) card to provide a data communication connection to a compatible LAN, such as Ethernet. Wireless links may also be implemented. For wireless links, the communications interface 870 sends or receives or both sends and receives electrical, acoustic or electromagnetic signals, including infrared and optical signals, that carry information streams, such as digital data. For example, in wireless handheld devices, such as mobile telephones like cell phones, the communications interface 870 includes a radio band electromagnetic transmitter and receiver called a radio transceiver. In certain embodiments, the communications interface 870 enables connection to the communication network 115 for providing unknown object detection."
41,communications network,cellular telephone,rhyper,-1,"An optional display interface 830 may permit information from the bus 800 to be displayed on a display device 835 in visual, graphic or alphanumeric format. An audio interface and audio output (such as a speaker) also may be provided. Communication with external devices may occur using various communication devices 840 such as a transmitter and/or receiver, antenna, an RFID tag and/or short-range or near-field communication circuitry. A communication device 840 may be attached to a communications network, such as the Internet, a local area network or a cellular telephone data network."
42,communications network,the Internet,rhyper,-1,"An optional display interface 830 may permit information from the bus 800 to be displayed on a display device 835 in visual, graphic or alphanumeric format. An audio interface and audio output (such as a speaker) also may be provided. Communication with external devices may occur using various communication devices 840 such as a transmitter and/or receiver, antenna, an RFID tag and/or short-range or near-field communication circuitry. A communication device 840 may be attached to a communications network, such as the Internet, a local area network or a cellular telephone data network."
43,communications network,local area network,rhyper,-1,"An optional display interface 830 may permit information from the bus 800 to be displayed on a display device 835 in visual, graphic or alphanumeric format. An audio interface and audio output (such as a speaker) also may be provided. Communication with external devices may occur using various communication devices 840 such as a transmitter and/or receiver, antenna, an RFID tag and/or short-range or near-field communication circuitry. A communication device 840 may be attached to a communications network, such as the Internet, a local area network or a cellular telephone data network."
44,programming languages,object oriented programming language,rhyper,-1,"Computer readable program instructions for carrying out operations of the present disclosure may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++ or the like, and conventional procedural programming languages, such as the ""C"" programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present disclosure."
45,field of view,field of view,rhyper,-1,"The camera mounting system (e.g., rotational component 108, camera attachment 110, etc.) may be calibrated so that the physical position of the camera device 112 is known within a virtual coordinate system or the surgical field at any rotational position. In some examples, the rotational component 108 may include encoders or other rotational position sensors to provide an accurate rotational location of the camera attachment 110 and camera device 112. With the ability to accurate track rotational position, the exact field of view, such as field of view 114, may be calculated at any point in the rotation of the camera device 112. In some examples, additional fixed tracking markers can be positioned though out the surgical field to further assist in transforming the virtual coordinate system between rotational positions of camera device 112. The fixed tracking markers (not specifically shown in FIG. 1) may be mounted to a surgical table or other immovable objects within the surgical field. The fixed tracking markers may be utilized to account for any movement of the tracked object 116 between positional movements of the camera device 112. In other examples, movements of the camera device 112 are sufficient fast to make any movement of the tracked object 116 negligible between frames. In yet other examples, tracked instruments and/or robotic arms can be utilized to assist in transforming the virtual coordinate system to the new camera position. For example, in scenarios utilizing a robotic arm, a portion of the robot can include tracking marker, which may be tracked by the camera device 112 as well as having a known fixed relationship to the robotic arm. The known fixed relationship to the robotic arm can be utilized to validate or calculate any interim movement of the tracked object 116 during movement of the camera device 112 from one position to a second position."
46,integrated circuit,central processing unit,rhyper,-1,"A general integrated circuit such as a central processing unit (CPU) or an application specific integrated circuit (ASIC) may implement the modules, units or subunits in all embodiments of the present disclosure."
47,display device,flexible display,hyper,1,"Further, the display device is a flexible display screen, configured to change the image projected onto the curved semi-transparent reflecting screen by adjusting a shape of the flexible display screen, and correct image distortion in the display device. The flexible display screen, which uses PHOLED (phosphorescent OLED) technology and has a low-consumption, directly visual flexible panel, can effectively overcome the image distortion."
48,display device,laser projector,hyper,1,"Further, the display device is a laser projector, configured to project the image through laser beams; moreover, a micro-lens array (hereinafter referred to as MLA) is provided between the curved semi-transparent reflecting screen and the laser projector, and configured to modulate the display of the laser beams on the curved semi-transparent reflecting screen. Light sources in the display device are converged through the MLA. Diffuse reflection effects of the laser beams through the curved semi-transparent reflecting screen are modulated, thus avoiding damage to human eyes and relieving visual fatigue, reasonably controlling scattering angles of light and ensuring luminance."
49,display device,laser projector,hyper,1,"As shown in FIG. 6, in some embodiments, the display device is a laser projector, configured to project the image through laser beams; moreover, an MLA is provided between the curved semi-transparent reflecting screen and the laser projector, and configured to modulate the display of the laser beams on the curved semi-transparent reflecting screen. Light sources in the display device are converged through the MLA. Specifically, the display device is the laser projector, configured to project the image through the laser beams, and modulate diffuse reflection effects of the laser beams through the curved semi-transparent reflecting screen, thus avoiding damage to human eyes and relieving visual fatigue, reasonably controlling scattering angles of light and ensuring luminance."
50,display device,laser projector,hyper,1,"In some embodiments, the display device is a laser projector configured to project the image through laser beams; moreover, an MLA micro-lens array is provided between the curved semi-transparent reflecting screen and the laser projector, and configured to modulate display of the laser beams on the curved semi-transparent reflecting screen."
51,display device,flexible display,hyper,1,"2. The head-up display device according to claim 1, wherein the display device is a flexible display screen, configured to change the image projected onto the curved semi-transparent reflecting screen by adjusting a shape of the flexible display screen, and correct image distortion in the display device."
52,inclination angle,inclination angle,hyper,1,"S105: The mobile device determines, based on the four sides, a first included angle corresponding to the document image, where the first included angle may be used to represent an inclination degree of the document image relative to the target document. Specifically, the first included angle is positively correlated with a photographing inclination angle. The photographing inclination angle is an inclination angle of the image plane of the camera relative to the target document. Referring to content in FIG. 2A and FIG. 2B, it can be learned that the image plane of the camera is a plane on which the document image is located, and the photographing inclination angle is the included angle α in FIG. 2A and FIG. 2B."
53,inclination angle,inclination angle,hyper,1,"S205: The mobile device determines, based on the four sides, a first included angle corresponding to the document image, where the first included angle may be used to represent an inclination degree of the document image relative to the target document. Specifically, the first included angle is positively correlated with a photographing inclination angle. The photographing inclination angle is an inclination angle of the image plane of the camera relative to the target document. Referring to content in FIG. 2A and FIG. 2B, it can be learned that the image plane of the camera is a plane on which the document image is located, and the photographing inclination angle is the included angle α in FIG. 2A and FIG. 2B."
54,non-volatile RAM,read-only memory,rhyper,-1,"The memory could comprise the forms of volatile memory on computer-readable media, random access memory (RAM), and/or non-volatile RAM, such as read-only memory (ROM) or flash RAM. The memory comprises at least one storage chip."
55,non-volatile RAM,read-only memory,rhyper,-1,"The memory could comprise the forms of volatile memory on computer-readable media, random access memory (RAM), and/or non-volatile RAM, such as read-only memory (ROM) or flash RAM. Memory is an example of computer-readable media."
56,wireless communication,inter-vehicle communication,rhyper,-1,"The communication unit 25 transmits/receives information to/from a peripheral vehicle, a portable terminal device possessed by a pedestrian, a roadside device, or an external server by various kinds of wireless communication such as inter-vehicle communication, vehicle-to-pedestrian communication, and road-to-vehicle communication. For example, the communication unit 25 performs inter-vehicle communication with a peripheral vehicle, receives, from the peripheral vehicle, peripheral vehicle information including information indicating the number of occupants and the travelling state, and supplies it to the integrated ECU 31."
57,electromagnetic waves,millimeter waves,rhyper,-1,"The steering mechanism 26 performs control of the traveling direction of the vehicle 11, i.e., steering angle control, in accordance with the driver's steering wheel operation or the control signal supplied from the integrated ECU 31. The radar 27 is a distance measuring sensor that measures the distance to an object such as a vehicle and a pedestrian in each direction such as forward and backward by using electromagnetic waves such as millimeter waves, and outputs the result of measuring the distance to the object to the integrated ECU 31 or the like. The lidar 28 is a distance measuring sensor that measures the distance to an object such as a vehicle and a pedestrian in each direction such as forward and backward by using light waves, and outputs the result of measuring the distance to the object to the integrated ECU 31 or the like."
58,the service,payment service,hyper,1,"10. The vehicle state control apparatus according to claim 9, wherein the service is a payment service."
59,wireless communication,inter-vehicle communication,rhyper,-1,"The communication unit 25 transmits/receives information to/from a peripheral vehicle, a portable terminal device possessed by a pedestrian, a roadside device, an external server, or the like by various kinds of wireless communication such as inter-vehicle communication, vehicle-to-pedestrian communication, and road-to-vehicle communication. For example, the communication unit 25 performs inter-vehicle communication with a peripheral vehicle, receives, from the peripheral vehicle, peripheral vehicle information including information indicating the number of occupants and the travelling state, and supplies it to the integrated ECU 31."
60,electromagnetic waves,millimeter waves,rhyper,-1,"The steering mechanism 26 performs control of the traveling direction of the vehicle 11, i.e., steering angle control, in accordance with the driver's steering wheel operation or the control signal supplied from the integrated ECU 31. The radar 27 is a distance measuring sensor that measures the distance to an object such as a vehicle and a pedestrian in each direction such as forward and backward by using electromagnetic waves such as millimeter waves, and outputs the result of measuring the distance to the object to the integrated ECU 31 or the like. The lidar 28 is a distance measuring sensor that measures the distance to an object such as a vehicle and a pedestrian in each direction such as forward and backward by using light waves, and outputs the result of measuring the distance to the object to the integrated ECU 31 or the like."
61,geographical coordinates,GPS coordinates,rhyper,-1,"Here, the three-dimensional spaces in the respective worlds are previously associated one-to-one with absolute geographical coordinates such as GPS coordinates or latitude/longitude coordinates. Alternatively, each three-dimensional space may be represented as a position relative to a previously set reference position. The directions of the x axis, the y axis, and the z axis in the three-dimensional space are represented by directional vectors that are determined on the basis of the latitudes and the longitudes, etc. Such directional vectors are stored together with the encoded data as meta-information."
62,real-time information,traffic jam,rhyper,-1,"Here, the map data such as an HD map is capable of including, together with three-dimensional map 711 formed by a three-dimensional point cloud: two-dimensional map data (a two-dimensional map); simplified map data obtained by extracting, from the two-dimensional map data, characteristic information such as a road shape and an intersection; and meta-data representing real-time information such as a traffic jam, an accident, and a roadwork. For example, the map data has a layer structure in which three-dimensional data (three-dimensional map 711), two-dimensional data (a two-dimensional map), and meta-data are disposed from the bottom layer in the stated order."
63,recording medium,hard disk,rhyper,-1,"Moreover, in the above embodiments, the structural components may be implemented as dedicated hardware or may be realized by executing a software program suited to such structural components. Alternatively, the structural components may be implemented by a program executor such as a CPU or a processor reading out and executing the software program recorded in a recording medium such as a hard disk or a semiconductor memory."
64,recording medium,semiconductor memory,rhyper,-1,"Moreover, in the above embodiments, the structural components may be implemented as dedicated hardware or may be realized by executing a software program suited to such structural components. Alternatively, the structural components may be implemented by a program executor such as a CPU or a processor reading out and executing the software program recorded in a recording medium such as a hard disk or a semiconductor memory."
65,moving images,digital camera,rhyper,-1,"Camera ex213 is a device capable of capturing moving images, such as a digital camcorder. Camera ex216 is a device capable of capturing still images and moving images, such as a digital camera. Moreover, smartphone ex214 is, for example, a smartphone conforming to a global system for mobile communication (GSM) (registered trademark) scheme, a code division multiple access (CDMA) scheme, a wideband-code division multiple access (W-CDMA) scheme, an long term evolution (LTE) scheme, an high speed packet access (HSPA) scheme, or a communication scheme using high-frequency bands, or a personal handyphone system (PHS), and smartphone ex214 may be any of them."
66,moving images,digital camcorder,rhyper,-1,"Camera ex213 is a device capable of capturing moving images, such as a digital camcorder. Camera ex216 is a device capable of capturing still images and moving images, such as a digital camera. Moreover, smartphone ex214 is, for example, a smartphone conforming to a global system for mobile communication (GSM) (registered trademark) scheme, a code division multiple access (CDMA) scheme, a wideband-code division multiple access (W-CDMA) scheme, an long term evolution (LTE) scheme, an high speed packet access (HSPA) scheme, or a communication scheme using high-frequency bands, or a personal handyphone system (PHS), and smartphone ex214 may be any of them."
67,light source,laser diode,rhyper,-1,"The light source 130 is a Light Emitting Diode (LED) that generates an infrared ray (having a wavelength of 0.7 µm to 1.0 mm) in the present example embodiment. The light source 130 irradiates the user A with an infrared ray when performing capturing by the camera 110. The light source 130 is provided near the lens of the camera 110, and the light from the light source 130 is emitted to the user A via the movable mirror 120. Any number of the light sources 130 may be provided, and it is desirable that a plurality of light sources 130 be provided so as to surround the side face of the lens of the camera 110. Further, the light source 130 may be provided at any position between the camera 110 and the mirror 120 without being limited to being provided near the camera 110. Alternatively, the light source 130 may be provided between the mirror 120 and the user A. As the light source 130, any light source such as a laser diode (LD), a lamp, or the like may be used without being limited to the LED. As a light irradiated to the user A by the light source 130, without being limited to an infrared ray, a light of any wavelength may be used in accordance with a purpose or an environment where iris capturing is performed."
68,optical system,plane mirror,rhyper,-1,"The movable mirror 120 is provided on a light path of a light entering the camera 110. The movable mirror 120 is a mirror that guides a light from the light source 130 to the user A and further reflects the reflected light B, which is reflected on the user A, to guide the reflected light B to the lens of the camera 110. As the movable mirror 120, any optical system such as a plane mirror, a curved surface mirror, a prism, or the like that is capable of guiding the reflected light B to the lens of the camera 110 may be used."
69,computer readable,optical disk,rhyper,-1,"The memory 102 stores therein information within the iris capture apparatus 100. For example, the memory 102 may be a volatile memory unit, a non-volatile memory unit, or the combination thereof. The memory 102 may be another computer readable storage medium, such as a magnetic disk, an optical disk, or the like, for example."
70,computer readable,magnetic disk,rhyper,-1,"The memory 102 stores therein information within the iris capture apparatus 100. For example, the memory 102 may be a volatile memory unit, a non-volatile memory unit, or the combination thereof. The memory 102 may be another computer readable storage medium, such as a magnetic disk, an optical disk, or the like, for example."
71,personal computer,laptop computer,rhyper,-1,"The iris capture apparatus 100 can be implemented in many different forms without being limited to the form described above. For example, the iris capture apparatus 100 can be implemented in a form of a typical server or a plurality of servers in a form of a group of such servers. Further, the iris capture apparatus 100 can be implemented as a part of the rack server system. Furthermore, the iris capture apparatus 100 can be implemented in a form of a personal computer such as a laptop computer, a desktop computer, or the like."
72,personal computer,desktop computer,rhyper,-1,"The iris capture apparatus 100 can be implemented in many different forms without being limited to the form described above. For example, the iris capture apparatus 100 can be implemented in a form of a typical server or a plurality of servers in a form of a group of such servers. Further, the iris capture apparatus 100 can be implemented as a part of the rack server system. Furthermore, the iris capture apparatus 100 can be implemented in a form of a personal computer such as a laptop computer, a desktop computer, or the like."
73,flash memory,USB memory,rhyper,-1,"Note that a part or a whole of the program executed by the processor 101 of the iris capture apparatus 100 can be provided by a computer readable storage medium storing the above, such as a digital versatile disc-read only memory (DVD-ROM), a compact disc-read only memory (CD-ROM), a flash memory such as a USB memory or the like."
74,magnetic media,hard disks,rhyper,-1,"Also, the technical features described above can be implemented in the form of program instructions that may be performed using various computer means and can be recorded in a computer-readable medium. Such a computer-readable medium can include program instructions, data files, data structures, etc., alone or in combination. The program instructions recorded on the medium can be designed and configured specifically for the present invention or can be a type of medium known to and used by the skilled person in the field of computer software. Examples of a computer-readable medium may include magnetic media such as hard disks, floppy disks, magnetic tapes, etc., optical media such as CD-ROM's, DVD's, etc., magneto-optical media such as floptical disks, etc., and hardware devices such as ROM, RAM, flash memory, etc. Examples of the program of instructions may include not only machine language codes produced by a compiler but also high-level language codes that can be executed by a computer through the use of an interpreter, etc. The hardware mentioned above can be made to operate as one or more software modules that perform the actions of the embodiments of the invention, and vice versa."
75,machine learning,deep learning,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
76,machine learning,artificial intelligence,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
77,Deep learning,artificial intelligence,hyper,1,"Meanwhile, unlike the conventional machine learning, deep learning technologies have been being developed recently. Deep learning is an artificial intelligence technology that enables a computer to learn on its own like a human on the basis of Artificial Neural Networks (ANN). That is, deep learning means that a computer discovers and determines properties on its own."
78,Deep learning,artificial intelligence,hyper,1,Deep learning is an artificial intelligence technology that trains a computer to learn human thinking based on Artificial Neural Networks (ANN) for constructing artificial intelligence so that the computer is capable of learning on its own without a user's instruction.
79,deep learning,Convolutional Neural Network,rhyper,-1,"For example, the object recognition module 144 may include a Depp Neural Network (DNN) trained through deep learning, such as Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Deep Belief Network (DBN), etc."
80,home appliance,mobile robot,rhyper,-1,"Meanwhile, the server 70 may be a server operated by a manufacturer of a home appliance such as the mobile robot 100, 100a, or 100b, a server operated by a service provider, or a kind of Cloud server."
81,home appliance,mobile robot,rhyper,-1,"The communication unit 820 may receive a diversity of data, such as state information, operation information, controlled information, etc., from a mobile terminal, a home appliance such as the mobile robot 100, 100a, or 100b, etc., a gate way, or the like."
82,home appliance,mobile robot,rhyper,-1,"In addition, the communication unit 820 may transmit data responsive to the received diversity of data to a mobile terminal, a home appliance such as the mobile robot 100, 100a, 100b, etc., a gate way or the like."
83,home appliance,mobile robot,rhyper,-1,"The learning module 840 may serve as a learning machine of a home appliance such as the mobile robot 100, 100a, or 100b, etc."
84,home appliance,mobile robot,rhyper,-1,"Meanwhile, according to a setting, the controller 810 may perform control to update an artificial network architecture of a home appliance such as the mobile robot 100, 100a, or 100b, into a learned artificial neural network architecture."
85,machine learning,deep learning,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
86,machine learning,artificial intelligence,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
87,Deep learning,artificial intelligence,hyper,1,"Meanwhile, unlike the conventional machine learning, deep learning technologies have been being developed recently. Deep learning is an artificial intelligence technology that enables a computer to learn on its own like a human on the basis of Artificial Neural Networks (ANN). That is, deep learning means that a computer discovers and determines properties on its own."
88,Deep learning,artificial intelligence,hyper,1,Deep learning is an artificial intelligence technology that trains a computer to learn human thinking based on Artificial Neural Networks (ANN) for constructing artificial intelligence so that the computer is capable of learning on its own without a user's instruction.
89,deep learning,Convolutional Neural Network,rhyper,-1,"For example, the object recognition module 144 may include a Depp Neural Network (DNN) trained through deep learning, such as Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Deep Belief Network (DBN), etc."
90,home appliance,mobile robot,rhyper,-1,"Meanwhile, the server 70 may be a server operated by a manufacturer of a home appliance such as the mobile robot 100, 100a, or 100b, a server operated by a service provider, or a kind of Cloud server."
91,home appliance,mobile robot,rhyper,-1,"The communication unit 820 may receive a diversity of data, such as state information, operation information, controlled information, etc., from a mobile terminal, a home appliance such as the mobile robot 100, 100a, or 100b, etc., a gate way, or the like."
92,home appliance,mobile robot,rhyper,-1,"In addition, the communication unit 820 may transmit data responsive to the received diversity of data to a mobile terminal, a home appliance such as the mobile robot 100, 100a, 100b, etc., a gate way or the like."
93,home appliance,mobile robot,rhyper,-1,"The learning module 840 may serve as a learning machine of a home appliance such as the mobile robot 100, 100a, or 100b, etc."
94,home appliance,mobile robot,rhyper,-1,"Meanwhile, according to a setting, the controller 810 may perform control to update an artificial network architecture of a home appliance such as the mobile robot 100, 100a, or 100b, into a learned artificial neural network architecture."
95,machine learning,deep learning,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
96,machine learning,artificial intelligence,rhyper,-1,"Meanwhile, interests for machine learning such as artificial intelligence and deep learning have grown significantly in recent years."
97,Deep learning,artificial intelligence,hyper,1,"Meanwhile, unlike the conventional machine learning, deep learning technologies have been being developed recently. Deep learning is an artificial intelligence technology that enables a computer to learn on its own like a human on the basis of Artificial Neural Networks (ANN). That is, deep learning means that a computer discovers and determines properties on its own."
98,Deep learning,artificial intelligence,hyper,1,Deep learning is an artificial intelligence technology that trains a computer to learn human thinking based on Artificial Neural Networks (ANN) for constructing artificial intelligence so that the computer is capable of learning on its own without a user's instruction.
99,deep learning,Convolutional Neural Network,rhyper,-1,"For example, the object recognition module 144 may include a Depp Neural Network (DNN) trained through deep learning, such as Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Deep Belief Network (DBN), etc."
100,home appliance,mobile robot,rhyper,-1,"Meanwhile, the server 70 may be a server operated by a manufacturer of a home appliance such as the mobile robot 100, 100a, or 100b, a server operated by a service provider, or a kind of Cloud server."
101,home appliance,mobile robot,rhyper,-1,"The communication unit 820 may receive a diversity of data, such as state information, operation information, controlled information, etc., from a mobile terminal, a home appliance such as the mobile robot 100, 100a, or 100b, etc., a gate way, or the like."
102,home appliance,mobile robot,rhyper,-1,"In addition, the communication unit 820 may transmit data responsive to the received diversity of data to a mobile terminal, a home appliance such as the mobile robot 100, 100a, 100b, etc., a gate way or the like."
103,home appliance,mobile robot,rhyper,-1,"The learning module 840 may serve as a learning machine of a home appliance such as the mobile robot 100, 100a, or 100b, etc."
104,home appliance,mobile robot,rhyper,-1,"Meanwhile, according to a setting, the controller 810 may perform control to update an artificial network architecture of a home appliance such as the mobile robot 100, 100a, or 100b, into a learned artificial neural network architecture."
105,artificial light,fluorescent light,rhyper,-1,"The lighting apparatus 303 includes an LED (Light Emitting Diode) or the like as a light source and applies light to the measured object 1, such as a plant. Note that the light source in the lighting apparatus 303 is not limited to the LED, and for example, artificial light, such as fluorescent light, can be used."
106,environmental conditions,carbon dioxide,rhyper,-1,"That is, it is known that the photosynthesis of a plant is affected by the number of photons that are particles of light, instead of the energy of light. However, the number of photons that allows the plant to effectively utilize the light is significantly affected by environmental conditions, such as carbon dioxide concentration (CO2 concentration), temperature, humidity, and nutrients, as well as the type and the state of the plant. Therefore, the PPFD value that can be effectively utilized by the plant is predicted from the environmental conditions as well as the type and the state of the plant to calculate and display the effective PPFD value in the present technique. In addition, the effective PPFD value is further utilized here to improve the environment in real time to increase the growth of the plant."
107,environmental conditions,carbon dioxide,rhyper,-1,"That is, it is known that the photosynthesis of a plant is affected by the number of photons that are particles of light, instead of the energy of light. However, the number of photons that allows the plant to effectively utilize the light is significantly affected by environmental conditions, such as carbon dioxide concentration (CO2 concentration), temperature, humidity, and nutrients, as well as the type and the state of the plant. Therefore, the PPFD value that can be effectively utilized by the plant is predicted from the environmental conditions as well as the type and the state of the plant to calculate and display the effective PPFD value in the present technique. In addition, the effective PPFD value is further utilized here to improve the environment by using the prediction information to increase the growth of the plant."
108,neural network,convolutional neural network,hyper,1,"6. The method of any preceding claim, wherein the neural network is a convolutional neural network that is trained using the updated knowledge based engine."
109,neural network,convolutional neural network,hyper,1,"13. The image processing device of any of claims 8 - 12, wherein the neural network is a convolutional neural network that is trained using the updated knowledge based engine."
110,machine learning method,logistic regression,rhyper,-1,"At present, some personalized recommendation models on the market are realized based on a machine learning method such as a logistic regression (LR) or a RANKLR. A machine learning model can be trained through user's previous behaviors (such as clicking, browsing, purchasing, commenting and the like) in a system. A trained model can be configured to estimate a relevance score or a click-through-rate (CTR) between the user and an item to be recommended. Thereafter, personalized recommendation for the user with selected items can be achieved according to the relevance score or the CTR."
111,machine learning method,logistic regression,rhyper,-1,"The target model refers to a model, in the information recommendation system, configured to execute a recommendation task for items to be recommended. The target model can be implemented according to a machine learning method such as a logistic regression (LR) or a RANKLR. Features of the target model include features of attribute of the items to be recommended, features of a target user himself, features of a scenario in which the target user is located, and the like."
112,Volatile memory,storage medium,hyper,1,"In one example, memory in the pooled memory subsystem can be volatile memory, nonvolatile memory (NVM), or a combination thereof. Volatile memory can include any type of volatile memory, and is not considered to be limiting. Volatile memory is a storage medium that requires power to maintain the state of data stored by the medium. Non-limiting examples of volatile memory can include random access memory (RAM), such as static random-access memory (SRAM), dynamic random-access memory (DRAM), synchronous dynamic random-access memory (SDRAM), and the like, including combinations thereof. SDRAM memory can include any variant thereof, such as single data rate SDRAM (SDR DRAM), double data rate (DDR) SDRAM, including DDR, DDR2, DDR3, DDR4, DDR5, and so on, described collectively as DDRx, and low power DDR (LPDDR) SDRAM, including LPDDR, LPDDR2, LPDDR3, LPDDR4, and so on, described collectively as LPDDRx. In some examples, DRAM complies with a standard promulgated by JEDEC, such as JESD79F for DDR SDRAM, JESD79-2F for DDR2 SDRAM, JESD79-3F for DDR3 SDRAM, JESD79-4A for DDR4 SDRAM, JESD209B for LPDDR SDRAM, JESD209-2F for LPDDR2 SDRAM, JESD209-3C for LPDDR3 SDRAM, and JESD209-4A for LPDDR4 SDRAM (these standards are available at www.jedec.org; DDR5 SDRAM is forthcoming). Such standards (and similar standards) may be referred to as DDR-based or LPDDR-based standards, and communication interfaces that implement such standards may be referred to as DDR-based or LPDDR-based interfaces. In one specific example, the system memory can be DRAM. In another specific example, the system memory can be DDRx SDRAM. In yet another specific aspect, the system memory can be LPDDRx SDRAM."
113,volatile memory,high density,rhyper,-1,"NVM is a storage medium that does not require power to maintain the state of data stored by the medium. NVM has traditionally been used for the task of data storage, or long-term persistent storage, but new and evolving memory technologies allow the use of NVM in roles that extend beyond traditional data storage. One example of such a role is the use of NVM as main or system memory. Non-volatile system memory (NVMsys) can combine data reliability of traditional storage with ultra-low latency and high bandwidth performance, having many advantages over traditional volatile memory, such as high density, large capacity, lower power consumption, and reduced manufacturing complexity, to name a few. Byte-addressable, write-in-place NVM such as three-dimensional (3D) cross-point memory, for example, can operate as byte-addressable memory similar to dynamic random-access memory (DRAM), or as block-addressable memory similar to NAND flash. In other words, such NVM can operate as system memory or as persistent storage memory (NVMstor). In some situations where NVM is functioning as system memory, stored data can be discarded or otherwise rendered unreadable when power to the NVMsys is interrupted. NVMsys also allows increased flexibility in data management by providing non-volatile, low-latency memory that can be located closer to a processor in a computing device. In some examples, NVMsys can reside on a DRAM bus, such that the NVMsys can provide ultra-fast DRAM-like access to data. NVMsys can also be useful in computing environments that frequently access large, complex data sets, and environments that are sensitive to downtime caused by power failures or system crashes."
114,programmable memory,household appliances,rhyper,-1,"Various techniques, or certain aspects or portions thereof, can take the form of program code (i.e., instructions) embodied in tangible media, such as, CD-ROMs, hard drives, non-transitory computer readable storage medium, or any other machine-readable storage medium wherein, when the program code is loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the various techniques. Circuitry can include hardware, firmware, program code, executable code, computer instructions, and/or software. A non-transitory computer readable storage medium can be a computer readable storage medium that does not include signal. In the case of program code execution on programmable computers, the computing device can include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and/or storage elements), at least one input device, and at least one output device. The volatile and non-volatile memory and/or storage elements can be a RAM, EPROM, flash drive, optical drive, magnetic hard drive, solid state drive, or other medium for storing electronic data. The node and wireless device can also include a transceiver module, a counter module, a processing module, and/or a clock module or timer module. One or more programs that can implement or utilize the various techniques described herein can use an application programming interface (API), reusable controls, and the like. Such programs can be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However, the program(s) can be implemented in assembly or machine language, if desired. In any case, the language can be a compiled or interpreted language, and combined with hardware implementations. Exemplary systems or devices can include without limitation, laptop computers, tablet computers, desktop computers, smart phones, computer terminals and servers, storage databases, and other electronics which utilize circuitry and programmable memory, such as household appliances, smart televisions, digital video disc (DVD) players, heating, ventilating, and air conditioning (HVAC) controllers, light switches, and the like."
115,storage medium,optical disk,rhyper,-1,"The external media 224 is a storage medium that stores image data and audio data, and it is removably connected to the image capture device 20. The I/O control portion 226 controls reading of the image data and the audio data that are stored in the external media 224, as well as writing of the image data and the audio data to the external media 224. Note that the external media 224 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, a magneto-optical (MO) disk, or the like. The non-volatile memory may be, for example, a flash memory, an SD card, a micro-SD card, a USB memory, an electrically erasable programmable read-only memory (EEPROM), or an erasable programmable ROM (EPROM). Further, the magnetic disk may be a hard disk, a disk-shaped magnetic disk, or the like. Furthermore, the optical disk may be a compact disc (CD), a digital versatile disc (DVD), a Blu-Ray disc (BD) (registered trademark), or the like."
116,storage medium,non-volatile memory,rhyper,-1,"The external media 224 is a storage medium that stores image data and audio data, and it is removably connected to the image capture device 20. The I/O control portion 226 controls reading of the image data and the audio data that are stored in the external media 224, as well as writing of the image data and the audio data to the external media 224. Note that the external media 224 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, a magneto-optical (MO) disk, or the like. The non-volatile memory may be, for example, a flash memory, an SD card, a micro-SD card, a USB memory, an electrically erasable programmable read-only memory (EEPROM), or an erasable programmable ROM (EPROM). Further, the magnetic disk may be a hard disk, a disk-shaped magnetic disk, or the like. Furthermore, the optical disk may be a compact disc (CD), a digital versatile disc (DVD), a Blu-Ray disc (BD) (registered trademark), or the like."
117,storage medium,magnetic disk,rhyper,-1,"The external media 224 is a storage medium that stores image data and audio data, and it is removably connected to the image capture device 20. The I/O control portion 226 controls reading of the image data and the audio data that are stored in the external media 224, as well as writing of the image data and the audio data to the external media 224. Note that the external media 224 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, a magneto-optical (MO) disk, or the like. The non-volatile memory may be, for example, a flash memory, an SD card, a micro-SD card, a USB memory, an electrically erasable programmable read-only memory (EEPROM), or an erasable programmable ROM (EPROM). Further, the magnetic disk may be a hard disk, a disk-shaped magnetic disk, or the like. Furthermore, the optical disk may be a compact disc (CD), a digital versatile disc (DVD), a Blu-Ray disc (BD) (registered trademark), or the like."
118,storage medium,optical disk,rhyper,-1,"The internal memory 246 is a storage medium that stores image data and audio data. In the same manner as the external media 224, the internal memory 246 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, an MO disk, or the like."
119,storage medium,magnetic disk,rhyper,-1,"The internal memory 246 is a storage medium that stores image data and audio data. In the same manner as the external media 224, the internal memory 246 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, an MO disk, or the like."
120,storage medium,non-volatile memory,rhyper,-1,"The internal memory 246 is a storage medium that stores image data and audio data. In the same manner as the external media 224, the internal memory 246 may be a storage medium such as a non-volatile memory, a magnetic disk, an optical disk, an MO disk, or the like."
121,information processing system,mobile phone,hyper,1,"(1) An information processing system including circuitry configured to acquire a first captured image; acquire a plurality of second images; and generate display data including the first captured image and at least a subset of the plurality of second images based on an evaluation parameter corresponding to each of the plurality of second images.(2) The information processing system of (1), wherein the evaluation parameter indicates a degree of attention of a user to who is not using the information processing system.(3) The information processing system of (2), wherein the evaluation parameter is based on an operation of the user.(4) The information processing system of any of (1) to (3), further including a communication interface configured to acquire the plurality of second images from a service provider via a network.(5) The information processing system of any of (1) to (4), further including an interface configured to acquire the plurality of second images from a plurality of service providers via a network.(6) The information processing system of (5), further including a user interface configured to receive a user input selecting at least one of the plurality of service providers from which the plurality of second images is acquired, wherein the circuitry is configured to control acquiring the plurality of second images from the at least one of the plurality of service providers based on the received user input.(7) The information processing system of any of (1) to (6), further including a user interface configured to receive a user input selecting at least one of a plurality of classification groups corresponding to the plurality of second images, wherein the circuitry is configured to control acquiring the plurality of second images corresponding to the at least one of the plurality of classification groups based on the received user input.(8) The information processing system of any of (1) to (7), further including a user interface configured to receive a user input selecting at least one of a plurality of characteristics corresponding to a popularity of the plurality of second images, wherein the circuitry is configured to control acquiring the plurality of second images corresponding to the at least one of the plurality of characteristics based on the received user input.(9) The information processing system of any of (1) to (8), further including a display configured to display the display data generated by the circuitry; and an image capturing device configured to capture the first captured image.(10) The information processing system of any of (1) to (9), further including a memory configured to store the plurality of second images and the evaluation parameter corresponding to each of the plurality of second images.(11) The information processing system of any of (2) to (10), wherein the evaluation parameter for each of the plurality of second images is based on the at least one of a group classification of the image, a number of times the image has been accessed, a number of times the image has been cited, and a number of times the image has been commented on.(12) The information processing system of (11), wherein the circuitry is configured to acquire, for each of the plurality of second images, at least one of the group classification of the image, the number of times the image has been accessed, the number of times the image has been cited, and the number of times the image has been commented on; and determine the evaluation parameter for each of the plurality of second images based on the at least one of the group classification of the image, the number of times the image has been accessed, the number of times the image has been cited, and the number of times the image has been commented on.(13) The information processing system of (12), wherein the circuitry is configured to select the at least a subset of the plurality of second images to be included in the generated display data based on the determined evaluation parameter for each of the plurality of second images.(14) The information processing system of (11), wherein the circuitry is configured to include, in the generated display data, at least one of the group classification of the image, the number of times the image has been accessed, the number of times the image has been cited, and the number of times the image has been commented on for each of the at least a subset of the plurality of second images to be included in the generated display data.(15) The information processing system of any of (1) to (14), wherein the circuitry is configured to perform an image analysis on each of the plurality of second images to identify an image capture setting corresponding to each of the plurality of second images.(16) The information processing system of (15), wherein the circuitry is configured to select the at least a subset of the plurality of second images to be included in the generated display data by comparing the identified image capture settings corresponding to each of the plurality of second images with image capturing setting corresponding to the first captured image.(17) The information processing system of any of (15) to (16), further including a user interface configured to display the at least the subset of the plurality of second images and receive a user selection corresponding to one of the at least the subset of the plurality of second images.(18) The information processing system of (17), wherein the circuitry is configured to change image capture settings corresponding to the first captured image data based on the identified image capture settings of the one of the at least the subset of the plurality of second images selected based on the user input.(19) The information processing system of (9), further including a wireless interface configured to send and receive data via a wireless communication network.(20) An information processing method performed by an information processing system, the method comprising: acquiring a first captured image; acquiring a plurality of second images; and generating display data including the first captured image and at least a subset of the plurality of second images based on an evaluation parameter corresponding to each of the plurality of second images.(21) A non-transitory computer-readable medium including computer-program instructions, which when executed by an information processing system, cause the system to: acquire a first captured image; acquire a plurality of second images; and generate display data including the first captured image and at least a subset of the plurality of second images based on an evaluation parameter corresponding to each of the plurality of second images.(22)The information processing system of (1), wherein the plurality of second images are each thumbnail images.(23) The information processing system of (2), wherein the information processing system is a mobile phone including a camera unit configured to capture the first captured image.(24) The information processing system of (19), wherein the mobile phone includes a wireless interface configured to send and receive voice communication via a wireless communication network.(25) A display control device including:an acquisition portion that acquires, from at least one service provider server on a network that controls image data, data that pertain to at least one set of image data; anda display control portion that generates a display screen that includes an object image and a display that is based on data pertaining to image data according to a degree of attention for the at least one set of image data among the data that have been acquired by the acquisition portion.(26) The display control device according to (25),wherein the data pertaining to the image data include one of the image data and thumbnail data of the image data.(27) The display control device according to (25) or (26), further including:an image selection portion that makes a preferential selection of data pertaining to image data with a high degree of attention from the data that have been acquired by the acquisition portion,wherein the display control portion generates a display screen that includes a display that is based on the data that the image selection portion has made the selection of.(28) The display control device according to (27),wherein the image selection portion makes the selection from the data acquired from the service provider server designated by a user.(29) The display control device according to (28),wherein the image selection portion makes the selection in accordance with a degree of attention within a group designated by the user among a plurality of groups that are controlled by the service provider server designated by the user.(30) The display control device according to any one of (27) to (29),wherein the image selection portion identifies the degree of attention in accordance with a condition designated by a user.(31) The display control device according to any one of (25) to (30), further including:a processing portion that performs processing on the object image, the processing being associated with a display selected from the display that is based on the data pertaining to the at least one set of image data.(32) The display control device according to any one of (25) to (31), further including:an image capture portion,wherein the object image is an image captured by the image capture portion.(33) The display control device according to any one of (25) to (31), further including:a storage portion,wherein the object image is an image acquired by playing back image data stored in the storage portion.(34) The display control device according to (33),wherein the display control portion, in a case where, among the data that have been acquired by the acquisition portion, data exist that pertain to image data whose degree of similarity with the object image is greater than a similarity threshold value and whose degree of attention is greater than an attention threshold value, generates a display screen that includes a display that facilitates the object image to be uploaded to the at least one service provider server.(35) A display control method including:acquiring, from at least one service provider server on a network that controls image data, data that pertain to at least one set of image data; andgenerating a display screen that includes an object image and a display that is based on data pertaining to image data, among the at least one set of image data according to a degree of attention for the at least one set of image data among the data that pertain to the at least one set of image data.(36) A program for causing a computer to function as:an acquisition portion that acquires, from at least one service provider server on a network that controls image data, data that pertain to at least one set of image data; anda display control portion that generates a display screen that includes an object image and a display that is based on data pertaining to image data according to a degree of attention for the at least one set of image data among the data that have been acquired by the acquisition portion."
122,mobile device,tablet PC,rhyper,-1,"Referring to FIG. 1, a terminal 100 is connected to a server 200 for providing a video call service via a communication network. The server 200 may store data and various programs or applications for assisting a plurality of users to use a video call service by using the terminal 100 of each user. The server 200 for providing a video call service may perform both local communication and remote communication. The_server 200 for providing a video call service may be connected to a plurality of terminals 100 via a communication network. The terminal 100 may include various types of user terminals connected to the server 200 for providing a video call service. For example, the terminal 100 is a device for communicating with the server 200 for providing a video call service, and may include a wearable device such as a smart watch, a mobile device such as a smart phone, a tablet PC, and a laptop computer, and a stationary device such as a desktop computer. Furthermore, the terminal 100 may be a video call device for supporting a video call and may be capable of capturing and replaying a video so that a video call may be performed between users connected through a video call service."
123,mobile device,laptop computer,rhyper,-1,"Referring to FIG. 1, a terminal 100 is connected to a server 200 for providing a video call service via a communication network. The server 200 may store data and various programs or applications for assisting a plurality of users to use a video call service by using the terminal 100 of each user. The server 200 for providing a video call service may perform both local communication and remote communication. The_server 200 for providing a video call service may be connected to a plurality of terminals 100 via a communication network. The terminal 100 may include various types of user terminals connected to the server 200 for providing a video call service. For example, the terminal 100 is a device for communicating with the server 200 for providing a video call service, and may include a wearable device such as a smart watch, a mobile device such as a smart phone, a tablet PC, and a laptop computer, and a stationary device such as a desktop computer. Furthermore, the terminal 100 may be a video call device for supporting a video call and may be capable of capturing and replaying a video so that a video call may be performed between users connected through a video call service."
124,mobile device,smart phone,rhyper,-1,"Referring to FIG. 1, a terminal 100 is connected to a server 200 for providing a video call service via a communication network. The server 200 may store data and various programs or applications for assisting a plurality of users to use a video call service by using the terminal 100 of each user. The server 200 for providing a video call service may perform both local communication and remote communication. The_server 200 for providing a video call service may be connected to a plurality of terminals 100 via a communication network. The terminal 100 may include various types of user terminals connected to the server 200 for providing a video call service. For example, the terminal 100 is a device for communicating with the server 200 for providing a video call service, and may include a wearable device such as a smart watch, a mobile device such as a smart phone, a tablet PC, and a laptop computer, and a stationary device such as a desktop computer. Furthermore, the terminal 100 may be a video call device for supporting a video call and may be capable of capturing and replaying a video so that a video call may be performed between users connected through a video call service."
125,the triangle,number of,rhyper,-1,"For example, in a fingerprint authentication process, a triangle whose vertices are a target feature point, which is extracted from a fingerprint image that is to be authenticated, and two feature points around the target feature point is generated. The shape of the triangle and information related to the triangle, such as the number of fingerprint ridges crossing each side, are used to compare the triangle generated from the fingerprint image with a triangle included in enrolled fingerprint information."
126,semiconductor memory,flash memory,rhyper,-1,"The memory 1102 indicates, for example, a read only memory (ROM), a random access memory (RAM), and a semiconductor memory such as a flash memory, and stores programs and data used in the process. The memory 1102 may be used as the storage unit 111 in FIGs. 1 and 3."
127,communication network,local area network,rhyper,-1,"The network connection device 1107, which is connected to a communication network, such as a local area network or a wide area network, is a communication interface circuit which performs data conversion in communication. The information processing apparatus may receive programs and data from an external apparatus via the network connection device 1107, and may load, for use, the programs and the data onto the memory 1102. The network connection device 1107 may be used as the output unit 303 in FIG. 3."
128,communication network,wide area network,rhyper,-1,"The network connection device 1107, which is connected to a communication network, such as a local area network or a wide area network, is a communication interface circuit which performs data conversion in communication. The information processing apparatus may receive programs and data from an external apparatus via the network connection device 1107, and may load, for use, the programs and the data onto the memory 1102. The network connection device 1107 may be used as the output unit 303 in FIG. 3."
129,storage media,solid-state drive,rhyper,-1,"Fig. 2B is a schematic diagram illustrating respective hardware configurations of the client apparatus 110 and the network camera 100. A central processing unit (CPU) 201 is a central processing unit for controlling the client apparatus 110. A hard disk drive (HDD) 202 is a large-capacity storage device (a secondary storage device) for storing a program and a parameter for the CPU 201 to control the client apparatus 110. The program and the parameter do not necessarily need to be stored in an HDD. Alternatively, various storage media such as a solid-state drive (SSD) and a flash memory may be used. A random-access memory (RAM) 203 is a memory into which the CPU 201 loads a program read from the HDD 202 and in which the CPU 201 executes processing described below. Further, the RAM 203 as a primary storage device is occasionally used as a storage area for temporarily storing data and a parameter on which various processes are to be performed."
130,non-volatile memory,hard disk,rhyper,-1,"The image processing apparatus 100 includes a CPU 101, a main memory 102, a storage unit 103, an input unit 104, a display unit 105, and an external I/F 106. Each of the above units is connected with each other via a bus 107 for data transfer. The CPU 101 is an arithmetic processing unit (processor) for performing overall control of the image processing apparatus 100, and is capable of performing various processes by executing various programs stored in the storage unit 103 and the like. The main memory 102 is capable of temporarily storing data, parameters or the like used for various processes, and is a storing medium capable of providing a work area for the CPU 101. The main memory 102 may be a volatile memory such as a RAM, for instance. The storage unit 103 is a storage medium capable of storing various data necessary for various programs or for displaying a Graphical User Interface (GUI). The storage unit 103 may be, for instance, a mass storage device, or a non-volatile memory such as a hard disk or a silicon disk."
131,output device,liquid crystal,rhyper,-1,"The input unit 104 is a device for accepting an operation input from a user. The input unit 104 may be an input device such as a keyboard, a mouse, an electronic pen, or a touch panel. The display unit 105 is a device for displaying a processing result, an image, a GUI, or the like, and may be an output device such as a liquid crystal panel, for instance. Further, the display unit 105 is capable of displaying a processing result, such as a determination result of an object region or a separation result of an object and a background. The external I/F 106 is an interface for the image processing apparatus 100 to transmit/receive data with an external apparatus. In the present embodiment, the external I/F 106 is connected via a LAN 108 with each image capturing apparatus constiting the group of image capturing apparatuses 109. The image processing apparatus 100 can transmit/receive data such as image data or control signal data with each image capturing apparatus via the external I/F 106. The group of image capturing apparatuses 109 includes one or more image capturing apparatuses. Each of the image capturing apparatuses, according to the control signal data received from the image processing apparatus 100, is able to start or stop capturing an image, to change capturing parameters (shutter speed, an aperture, or the like), to transfer image data obtained by capturing an image, and the like. Note that, the image processing system may include various components other than the above described components, but descriptions thereof will be omitted."
132,storage media,main memory,rhyper,-1,"In a further embodiment, it is possible to store a background image generated by the background generation unit 405 according to brightness of a frame image, in a storage media such as the main memory 102. Additionally, the background generation unit 405, in a case where the background image obtained from the selected images selected according to the brightness of the target image 320 is stored in the main memory 102, may select this image to use as the background image 340. As an example, the background generation unit 405 may obtain a background image obtained from selected images for which similarity of a feature amount reflecting brightness with the target image 320 is equal to or larger than a threshold value, from the main memory 102. Additionally, the background generation unit 405 may obtain from the main memory 102 a background image generated for a frame image with similarity of a feature amount reflecting brightness with the target image 320 equal to or larger than a threshold value."
133,display device,liquid crystal display,rhyper,-1,"The I/F unit 150 enables connection between the display system 100 and an external apparatus. In this embodiment, a navigation apparatus 152 (or an electronic apparatus having a navigation function) is connected through the I/F unit 150. The navigation apparatus 152 has a function of calculating a position of the own vehicle using a global positioning system (GPS) signal or a self-contained navigation sensor (such as an acceleration sensor or an angular rate sensor), a function of displaying data on a road map in a position near the own vehicle, a function of retrieving a route from a present location to a destination and guiding the vehicle along retrieved route. The display function of the navigation apparatus 152 may include a display device, such as a liquid crystal display device, separately from the HUD 130. When the navigation apparatus 152 is connected, the controller 160 may operate in combination with the navigation apparatus 152 and display a virtual marker which guides a traveling direction, such as a right/left turn at an intersection or a lane change, based on guide information supplied from the navigation apparatus 152 when the navigation apparatus 152 guides the route to the destination."
134,computer network,the Internet,rhyper,-1,"A plurality of components in the device 900 are coupled to the I/O interface 905, including: an input unit 906, such as a keyboard or a mouse; an output unit 907, such as various types of displays, or speakers; the storage unit 908, such as a disk or an optical disk; and a communication unit 909 such as a network card, a modem, or a wireless communication transceiver. The communication unit 909 allows the device 900 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
135,computer network,the Internet,rhyper,-1,"A plurality of components in the device 1000 are coupled to the I/O interface 1005, including: an input unit 1006, such as a keyboard or a mouse; an output unit 1007, such as various types of displays, or speakers; the storage unit 1008, such as a disk or an optical disk; and a communication unit 1009 such as a network card, a modem, or a wireless communication transceiver. The communication unit 1009 allows the device 1000 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
136,computer network,the Internet,rhyper,-1,"A plurality of components in the device 800 are coupled to the I/O interface 805, including: an input unit 806, such as a keyboard or a mouse; an output unit 807, such as various types of displays, or speakers; the storage unit 808, such as a disk or an optical disk; and a communication unit 809 such as a network card, a modem, or a wireless communication transceiver. The communication unit 809 allows the device 800 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
137,main memory,volatile memory,rhyper,-1,"The processor 1512 of the illustrated example includes a local memory 1513 (e.g., a cache). The processor 1512 of the illustrated example is in communication with a main memory including a volatile memory 1514 and a non-volatile memory 1516 via a bus 1518. The volatile memory 1514 may be implemented by Synchronous Dynamic Random Access Memory (SDRAM), Dynamic Random Access Memory (DRAM), RAMBUS® Dynamic Random Access Memory (RDRAM®) and/or any other type of random access memory device. The non-volatile memory 1516 may be implemented by flash memory and/or any other desired type of memory device. Access to the main memory 1514, 1516 is controlled by a memory controller."
138,main memory,volatile memory,rhyper,-1,"The processor 1612 of the illustrated example includes a local memory 1613 (e.g., a cache). The processor 1612 of the illustrated example is in communication with a main memory including a volatile memory 1614 and a non-volatile memory 1616 via a bus 1618. The volatile memory 1614 may be implemented by Synchronous Dynamic Random Access Memory (SDRAM), Dynamic Random Access Memory (DRAM), RAMBUS® Dynamic Random Access Memory (RDRAM®) and/or any other type of random access memory device. The non-volatile memory 1616 may be implemented by flash memory and/or any other desired type of memory device. Access to the main memory 1614, 1616 is controlled by a memory controller."
139,optimization algorithm,stochastic gradient descent,hyper,1,"The neural network may comprise weights, and the learning may comprise, with an optimization algorithm, updating the weights according to the dataset and to a loss function. In examples, the optimization algorithm is a stochastic gradient descent. In examples, the loss function is a cross-entropy loss function."
140,mass memory device,hard drive,rhyper,-1,"The client computer of the example comprises a central processing unit (CPU) 1010 connected to an internal communication BUS 1000, a random access memory (RAM) 1070 also connected to the BUS. The client computer is further provided with a graphical processing unit (GPU) 1110 which is associated with a video random access memory 1100 connected to the BUS. Video RAM 1100 is also known in the art as frame buffer. A mass storage device controller 1020 manages accesses to a mass memory device, such as hard drive 1030. Mass memory devices suitable for tangibly embodying computer program instructions and data include all forms of nonvolatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM disks 1040. Any of the foregoing may be supplemented by, or incorporated in, specially designed ASICs (application-specific integrated circuits). A network adapter 1050 manages accesses to a network 1060. The client computer may also include a haptic device 1090 such as cursor control device, a keyboard or the like. A cursor control device is used in the client computer to permit the user to selectively position a cursor at any desired location on display 1080. In addition, the cursor control device allows the user to select various commands, and input control signals. The cursor control device includes a number of signal generation devices for input control signals to system. Typically, a cursor control device may be a mouse, the button of the mouse being used to generate the signals. Alternatively or additionally, the client computer system may comprise a sensitive pad, and/or a sensitive screen."
141,optimization algorithm,stochastic gradient descent,hyper,1,"4. The method of claim 3, wherein the optimization algorithm is a stochastic gradient descent."
142,computer network,the Internet,rhyper,-1,"A plurality of components in the device 800 are coupled to the I/O interface 805, including: an input unit 806, such as a keyboard or a mouse; an output unit 807, such as various types of displays, or speakers; the storage unit 808, such as a disk or an optical disk; and a communication unit 809 such as a network card, a modem, or a wireless communication transceiver. The communication unit 809 allows the device 800 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
143,point cloud data,Iterative Closest Point,rhyper,-1,"When using the acquired point cloud data for subsequent operations, especially for comprehensive use of the point clouds acquired dynamically or through a plurality of acquisitions, it is generally considered that the removal of point cloud ghost of dynamic targets is a technical point. Since the point cloud data is used to represent point clouds, it is determined that the matching relationship between the point clouds is directly related to the determination of the matching relationship between the point cloud data. A conventional method for determining a matching relationship between point cloud data, such as the Iterative Closest Point (ICP) algorithm, uses almost all the point cloud data (i.e., related data for almost all points in the point clouds) to determine the matching relationship between different point cloud data. In the above conventional method, points having specific characteristics in a point cloud are manually extracted from point cloud data for the same object, for determining the matching relationship between the point cloud data. However, performing manual extraction puts forward very strict requirements for point cloud data. Only when the amount of data of the points in the point cloud is large enough and the acquisition accuracy is high enough, can some details such as corresponding corners in the point cloud be found, requiring for an investment of a lot of labor costs."
144,computer network,the Internet,rhyper,-1,"A plurality of components in the device 400 are coupled to the I/O interface 405, including: an input unit 406, such as a keyboard or a mouse; an output unit 407, such as various types of displays, or speakers; the storage unit 408, such as a disk or an optical disk; and a communication unit 409 such as a network card, a modem, or a wireless communication transceiver. The communication unit 409 allows the device 400 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
145,programming languages,object oriented programming languages,rhyper,-1,"Computer program instructions for performing the operations of the present disclosure may be assembly instructions, instruction set architecture (ISA) instructions, machine instructions, machine related instructions, microcode, firmware instructions, state setting data, or source code or object code written in any combination of one or more programming languages, the programming languages including object oriented programming languages such as Smalltalk, and C++, as well as conventional procedural programming languages such as the ""C"" language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer, or entirely on a remote computer or server. In the case of a remote computer, the remote computer may be connected to the user computer through any kind of network, including a local area network (LAN) or a wide area network (WAN), or may be connected to an external computer (e.g., using an Internet service provider to connect via the Internet). In some embodiments, electronic circuits are personalizing customized by using state information of computer readable program instructions, such as programmable logic circuits, field programmable gate arrays (FPGA), or programmable logic arrays (PLA). The electronic circuit may execute computer readable program instructions to implement various aspects of the present disclosure."
146,point cloud data,first position,hyper,1,"In Fig. 3, the locations of the first candidate object 322 and the second candidate object 332 in the coordinate system of the point cloud data are the first position (X1, Y, Z) and the second position (X2, Y, Z) respectively. It should be noted that the coordinate system refers to the coordinate system of the point cloud data, which corresponds to the real space, and the definition of the coordinate system in each frame in the point cloud data is unified. According to an exemplary implementation of the present disclosure, the coordinate system may be defined in different approaches. For example, the coordinate system may use a reference location in the real space (for example, the position that the acquisition entity 112 starts the acquisition process) as the origin of the coordinate system, and use the moving direction as the direction of the X-axis, and then is defined according to the Cartesian coordinate system. Alternatively, other location may be defined as the origin of the coordinate system, and the direction of the X-axis may be defined according to other directions."
147,computer network,the Internet,rhyper,-1,"A plurality of components in the device 1200 are coupled to the I/O interface 1205, including: an input unit 1206, such as a keyboard or a mouse; an output unit 1207, such as various types of displays, or speakers; the storage unit 1208, such as a disk or an optical disk; and a communication unit 1209 such as a network card, a modem, or a wireless communication transceiver. The communication unit 1209 allows the device 1200 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
148,computer network,the Internet,rhyper,-1,"A plurality of components in the device 700 are coupled to the I/O interface 705, including: an input unit 706, such as a keyboard or a mouse; an output unit 707, such as various types of displays, or speakers; the storage unit 708, such as a disk or an optical disk; and a communication unit 709 such as a network card, a modem, or a wireless communication transceiver. The communication unit 709 allows the device 700 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
149,lane line,speed limit,rhyper,-1,"As used herein, the term ""map element"" refers to any appropriate element capable of forming an electronic map. The map element may be any appropriate object in a real environment denoted by the electronic map. As an example, in an electronic map denoting an autonomous driving road segment, the map element may include a traffic signboard and a lane line, such as speed limit boards, parking signposts, entrance and exit indicators, and special road segment signs. In other scenarios, the map element may be a landmark in an environment."
150,a vehicle,digital camera,rhyper,-1,"In the environment 100, images of the surrounding environment are collected by collection entities 110-1 and 110-2 (collectively referred to as the collection entities 110). In this example, the collection entities 110 are vehicles traveling on the express highways. Specifically, images in the environment 100 may be collected by an image capturing device (not shown) installed on the vehicles. The image capturing device may be any appropriate device having an image capturing function. For example, the image capturing device may be a camera or a dash cam recorder installed on a vehicle. As an alternative, the image capturing device may be a portable device carried by a passenger on a vehicle, such as a mobile phone, a laptop computer, a tablet computer, and a digital camera. The collection entities 110 may also be any appropriate other device capable of collecting images of the surrounding environment. For example, in an embodiment where the environment 100 is an indoor environment, the collection entity may be a portable device carried by a pedestrian."
151,a vehicle,tablet computer,rhyper,-1,"In the environment 100, images of the surrounding environment are collected by collection entities 110-1 and 110-2 (collectively referred to as the collection entities 110). In this example, the collection entities 110 are vehicles traveling on the express highways. Specifically, images in the environment 100 may be collected by an image capturing device (not shown) installed on the vehicles. The image capturing device may be any appropriate device having an image capturing function. For example, the image capturing device may be a camera or a dash cam recorder installed on a vehicle. As an alternative, the image capturing device may be a portable device carried by a passenger on a vehicle, such as a mobile phone, a laptop computer, a tablet computer, and a digital camera. The collection entities 110 may also be any appropriate other device capable of collecting images of the surrounding environment. For example, in an embodiment where the environment 100 is an indoor environment, the collection entity may be a portable device carried by a pedestrian."
152,a vehicle,mobile phone,rhyper,-1,"In the environment 100, images of the surrounding environment are collected by collection entities 110-1 and 110-2 (collectively referred to as the collection entities 110). In this example, the collection entities 110 are vehicles traveling on the express highways. Specifically, images in the environment 100 may be collected by an image capturing device (not shown) installed on the vehicles. The image capturing device may be any appropriate device having an image capturing function. For example, the image capturing device may be a camera or a dash cam recorder installed on a vehicle. As an alternative, the image capturing device may be a portable device carried by a passenger on a vehicle, such as a mobile phone, a laptop computer, a tablet computer, and a digital camera. The collection entities 110 may also be any appropriate other device capable of collecting images of the surrounding environment. For example, in an embodiment where the environment 100 is an indoor environment, the collection entity may be a portable device carried by a pedestrian."
153,a vehicle,laptop computer,rhyper,-1,"In the environment 100, images of the surrounding environment are collected by collection entities 110-1 and 110-2 (collectively referred to as the collection entities 110). In this example, the collection entities 110 are vehicles traveling on the express highways. Specifically, images in the environment 100 may be collected by an image capturing device (not shown) installed on the vehicles. The image capturing device may be any appropriate device having an image capturing function. For example, the image capturing device may be a camera or a dash cam recorder installed on a vehicle. As an alternative, the image capturing device may be a portable device carried by a passenger on a vehicle, such as a mobile phone, a laptop computer, a tablet computer, and a digital camera. The collection entities 110 may also be any appropriate other device capable of collecting images of the surrounding environment. For example, in an embodiment where the environment 100 is an indoor environment, the collection entity may be a portable device carried by a pedestrian."
154,computer network,the Internet,rhyper,-1,"A plurality of components in the device 500 are coupled to the I/O interface 505, including: an input unit 506, such as a keyboard or a mouse; an output unit 507, such as various types of displays, or speakers; the storage unit 508, such as a disk or an optical disk; and a communication unit 509 such as a network card, a modem, or a wireless communication transceiver. The communication unit 509 allows the device 500 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
155,computer network,the Internet,rhyper,-1,"A plurality of components in the device 900 are coupled to the I/O interface 905, including: an input unit 906, such as a keyboard or a mouse; an output unit 907, such as various types of displays, or speakers; the storage unit 908, such as a disk or an optical disk; and a communication unit 909 such as a network card, a modem, or a wireless communication transceiver. The communication unit 909 allows the device 900 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
156,computer network,the Internet,rhyper,-1,"A plurality of components in the device 1100 are coupled to the I/O interface 705, including: an input unit 1106, such as a keyboard or a mouse; an output unit 1107, such as various types of displays, or speakers; the storage unit 1108, such as a disk or an optical disk; and a communication unit 1109 such as a network card, a modem, or a wireless communication transceiver. The communication unit 1109 allows the device 1100 to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks."
157,surgical device,surgical instrument,rhyper,-1,"Various systems and methods for controlling surgical devices are disclosed. A computer system, such as a surgical hub, can be configured to be communicably coupled to a surgical device and a camera configured to view an operating room. The computer system can be programmed to receive images of surgical staff members and surgical devices within the operating room during a surgical procedure. The computer system can further determine conditions or characteristics associated with the surgical staff members and surgical devices, such as whether a surgical staff member is performing a particular gesture or the pose of a surgical device, such as a surgical instrument, during a surgical procedure. The computer system can then control the paired surgical devices accordingly."
158,optical disc drive,compact disc,rhyper,-1,"The computer system 210 also includes removable/non-removable, volatile/non-volatile computer storage media, such as for example disk storage. The disk storage includes, but is not limited to, devices like a magnetic disk drive, floppy disk drive, tape drive, Jaz drive, Zip drive, LS-60 drive, flash memory card, or memory stick. In addition, the disk storage can include storage media separately or in combination with other storage media including, but not limited to, an optical disc drive such as a compact disc ROM device (CD-ROM), compact disc recordable drive (CD-R Drive), compact disc rewritable drive (CD-RW Drive), or a digital versatile disc ROM drive (DVD-ROM). To facilitate the connection of the disk storage devices to the system bus, a removable or non-removable interface may be employed."
159,non-volatile memory,magnetic storage,rhyper,-1,"In addition, surgical instruments 7012 may comprise transceivers for data transmission to and from their corresponding surgical hubs 7006 (which may also comprise transceivers). Combinations of surgical instruments 7012 and corresponding hubs 7006 may indicate particular locations, such as operating theaters in healthcare facilities (e.g., hospitals), for providing medical operations. For example, the memory of a surgical hub 7006 may store location data. As shown in FIG. 12, the cloud 7004 comprises central servers 7013 (which may be same or similar to remote server 113 in FIG. 1 and/or remote server 213 in FIG. 9), hub application servers 7002, data analytics modules 7034, and an input/output (""I/O"") interface 7007. The central servers 7013 of the cloud 7004 collectively administer the cloud computing system, which includes monitoring requests by client surgical hubs 7006 and managing the processing capacity of the cloud 7004 for executing the requests. Each of the central servers 7013 comprises one or more processors 7008 coupled to suitable memory devices 7010 which can include volatile memory such as random-access memory (RAM) and non-volatile memory such as magnetic storage devices. The memory devices 7010 may comprise machine executable instructions that when executed cause the processors 7008 to execute the data analytics modules 7034 for the cloud-based data analysis, operations, recommendations and other operations described below. Moreover, the processors 7008 can execute the data analytics modules 7034 independently or in conjunction with hub applications independently executed by the hubs 7006. The central servers 7013 also comprise aggregated medical data databases 2212, which can reside in the memory 2210."
160,volatile memory,random-access memory,rhyper,-1,"In addition, surgical instruments 7012 may comprise transceivers for data transmission to and from their corresponding surgical hubs 7006 (which may also comprise transceivers). Combinations of surgical instruments 7012 and corresponding hubs 7006 may indicate particular locations, such as operating theaters in healthcare facilities (e.g., hospitals), for providing medical operations. For example, the memory of a surgical hub 7006 may store location data. As shown in FIG. 12, the cloud 7004 comprises central servers 7013 (which may be same or similar to remote server 113 in FIG. 1 and/or remote server 213 in FIG. 9), hub application servers 7002, data analytics modules 7034, and an input/output (""I/O"") interface 7007. The central servers 7013 of the cloud 7004 collectively administer the cloud computing system, which includes monitoring requests by client surgical hubs 7006 and managing the processing capacity of the cloud 7004 for executing the requests. Each of the central servers 7013 comprises one or more processors 7008 coupled to suitable memory devices 7010 which can include volatile memory such as random-access memory (RAM) and non-volatile memory such as magnetic storage devices. The memory devices 7010 may comprise machine executable instructions that when executed cause the processors 7008 to execute the data analytics modules 7034 for the cloud-based data analysis, operations, recommendations and other operations described below. Moreover, the processors 7008 can execute the data analytics modules 7034 independently or in conjunction with hub applications independently executed by the hubs 7006. The central servers 7013 also comprise aggregated medical data databases 2212, which can reside in the memory 2210."
161,storage medium,Electrically Erasable and Programmable ROM,rhyper,-1,"The storage device 20C is to store programs and data for controlling the operations of the control unit 20, and is realized by a storage medium such as an Electrically Erasable and Programmable ROM (EEPROM (trademark))."
162,visible light spectrum,near-infrared light,rhyper,-1,"As shown in FIG. 3, recognition members 40A and 40B are provided as recognition markings in the area of the strap 40 that is exposed when the seat belt 4 is worn. In FIG. 3, the recognition members 40A and 40B are illustrated as being recognizable, but they are actually designed in an invisible manner in a normal visible light environment. According to the first embodiment, the recognition members 40A and 40B are designed to reflect light of a specific wavelength outside the visible light spectrum, such as near-infrared light."
163,storage medium,magnetic disk,rhyper,-1,"The method that has been described in connection with each of the above embodiments may be stored and distributed as a computer-executable program (wearing detection program) in a storage medium such as a magnetic disk (e.g., a flexible disk and a hard disk), an optical disk (e.g., a CD-ROM and a DVD), a magneto-optical disk (MO), or a semiconductor memory."
164,storage medium,flexible disk,rhyper,-1,"The method that has been described in connection with each of the above embodiments may be stored and distributed as a computer-executable program (wearing detection program) in a storage medium such as a magnetic disk (e.g., a flexible disk and a hard disk), an optical disk (e.g., a CD-ROM and a DVD), a magneto-optical disk (MO), or a semiconductor memory."
165,storage medium,hard disk,rhyper,-1,"The method that has been described in connection with each of the above embodiments may be stored and distributed as a computer-executable program (wearing detection program) in a storage medium such as a magnetic disk (e.g., a flexible disk and a hard disk), an optical disk (e.g., a CD-ROM and a DVD), a magneto-optical disk (MO), or a semiconductor memory."
166,storage device,hard disk,rhyper,-1,"The storage unit 260 is configured of a storage device such as a hard disk or a memory, and has a function of storing processing information, a storage area, and a program 261 necessary for various kinds of processing performed in the arithmetic processing unit 270. The program 261 is a program for implementing various processing unit by being read into the arithmetic processing unit 270 and executed, and is read in advance from an external device (not illustrated) or a storage medium (not illustrated) via a data input/output function such as the communication I/F unit 220, and is saved in the storage unit 260. The main processing information and the storage area, stored in the storage unit 260, include a video buffer 262 and a video saving area 263."
167,storage device,hard disk,rhyper,-1,"The storage unit 560 is configured of a storage device such as a hard disk, a memory, or the like, and has a function of storing processing information, a storage area, and a program 561 that are necessary for various kinds of processing performed in the arithmetic processing unit 570. The program 561 is a program for implementing various processing units by being read into the arithmetic processing unit 570 and executed, and is read in advance from an external device (not illustrated) or a storage medium (not illustrated) via a data input/output function such as the communication I/F units 510 and 520, and is saved in the storage unit 560. The main processing information and the storage area stored in the storage unit 560 include the driving condition information buffer 262, a received file buffer 563, and a file saving area 564."
168,brain waves,pulse rate,rhyper,-1,"Patent Document 2 (see Japanese Patent Laid-Open No. 2015-109964) offers a method that involves measuring biological information other than brain waves such as pulse rate and blood flow regarding a target person and, based on the correlation between the measured biological information and brain waves, estimating the target person's brain waves associated with the measured biological information, the estimated brain waves constituting a characteristic pattern that permits estimation of the target person's emotion."
169,brain waves,blood flow,rhyper,-1,"Patent Document 2 (see Japanese Patent Laid-Open No. 2015-109964) offers a method that involves measuring biological information other than brain waves such as pulse rate and blood flow regarding a target person and, based on the correlation between the measured biological information and brain waves, estimating the target person's brain waves associated with the measured biological information, the estimated brain waves constituting a characteristic pattern that permits estimation of the target person's emotion."
170,electronic pen,the height,rhyper,-1,"In general, the hand and fingertips of the worker holding the electronic pen move in a manner reflecting the worker's emotional state at the time of work. Thus the electronic pen held by the worker is at a height position, is tilted relative to the sensor input sur-face, and is under writing pressure in a manner reflecting the worker's emotional state during work. That is, there are correlations between the emotional state of the worker holding the electronic pen on the one hand and the pen state information regarding the state of the electronic pen such as the height position and its variations, the tilt and its variations, and the writing pressure and its variations of the electronic pen on the other hand."
171,brain waves,blood flow,rhyper,-1,"A third example of the system for building the database, to be explained hereunder, also dispenses with the association processing part 4 of the first example, as with the second example. In the third example, the emotional state of the electronic pen user is estimated not by use of brain wave data but by recourse to the technique of Patent Document 2 to measure biological information other than the user's brain waves, such as pulse rate and blood flow."
172,brain waves,pulse rate,rhyper,-1,"A third example of the system for building the database, to be explained hereunder, also dispenses with the association processing part 4 of the first example, as with the second example. In the third example, the emotional state of the electronic pen user is estimated not by use of brain wave data but by recourse to the technique of Patent Document 2 to measure biological information other than the user's brain waves, such as pulse rate and blood flow."
173,state estimation,brain wave,rhyper,-1,"That is, the emotion server apparatus of this embodiment acquires various characteristic amounts (coordinates, pressure, tilt, height, etc.) making up the pen state information as the information about the handwriting (called the handwriting data hereunder) of the person who wrote with the electronic pen. At this time, the person's emotions are estimated from the biological information intended for emotional state estimation such as brain wave data from a brain wave data detection apparatus. The information regarding the relations between the characteristic amounts of handwriting data (coordinates, pressure, tilt, height, etc.) on the one hand and the estimated emotions on the other hand is then accumulated in large quantities in a database. In this case, the characteristic amounts of handwriting data (coordinates, pressure, tilt, height, etc.) manifests a specific distribution with regard to each emotional state. Thus the database permits acquisition of the distribution of the characteristic amounts of handwriting data applicable to each emotional state. When the characteristic amounts of handwriting data at the time of writing with the electronic pen are determined to fall within the distribution of the characteristic amounts associated with a specific emotional state, the determination permits estimation of the emotional state of the person at the time of writing with the electronic pen."
174,semiconductor memory,flash memory,rhyper,-1,"Such storage unit 101 is provided by use of a rewritable, nonvolatile semiconductor memory such as a flash memory. In addition, a read-only memory (ROM), which is non-rewritable, or a Random Access Memory (RAM), which is volatile, can be used as storage unit 101 according to whether data to be saved will be overwritten, how long the data has to be stored, or the like."
175,wide-angle lens,fisheye lens,rhyper,-1,"Optical system 103 is a constituent component by which light from the capturing area is formed into an image on image sensor 104, and is provided by use of optical elements including a lens. Optical system 103 may allow its focal distance and angle of view to be changed. A wide-angle lens or a super-wide-angle lens such as a fisheye lens may be used. For example, in a case where videos captured by multiple viewpoint image capturing system 1000 are used in a monitoring system, wide-angle lenses may be used to obtain a wide capturing area. Properties of optical system 103 that include focal distance, aberration, image center, and the like are used in the three-dimensional space reconstruction in a form of the internal parameters described above. That is, in a case where the focal distance of optical system 103 is changed, or a lens of optical system 103 is changed, it is necessary to change the camera parameters used in the three-dimensional space reconstruction as in the case where there is a change in a position of a camera. That is, the camera calibration is needed."
176,moving images,digital camera,rhyper,-1,"Camera ex213 is a device capable of capturing moving images, such as a digital camcorder. Camera ex216 is a device capable of capturing still images and moving images, such as a digital camera. Moreover, smartphone ex214 is, for example, a smartphone conforming to a global system for mobile communication (GSM) (registered trademark) scheme, a code division multiple access (CDMA) scheme, a wideband-code division multiple access (W-CDMA) scheme, an long term evolution (LTE) scheme, an high speed packet access (HSPA) scheme, or a communication scheme using high-frequency bands, or a personal handyphone system (PHS), and smartphone ex214 may be any of them."
177,moving images,digital camcorder,rhyper,-1,"Camera ex213 is a device capable of capturing moving images, such as a digital camcorder. Camera ex216 is a device capable of capturing still images and moving images, such as a digital camera. Moreover, smartphone ex214 is, for example, a smartphone conforming to a global system for mobile communication (GSM) (registered trademark) scheme, a code division multiple access (CDMA) scheme, a wideband-code division multiple access (W-CDMA) scheme, an long term evolution (LTE) scheme, an high speed packet access (HSPA) scheme, or a communication scheme using high-frequency bands, or a personal handyphone system (PHS), and smartphone ex214 may be any of them."
178,electronic device,electronic component,rhyper,-1,"Various embodiments of the present disclosure relate to an electronic component. For example, the various embodiments relate to an electronic component that detect various kinds of information (e.g., a fingerprint, an iris, proximity of an object, illuminance, temperature, humidity, etc.) and an electronic device including the electronic component."
179,electronic device,electronic component,rhyper,-1,"According to various embodiments, it is possible to provide an electronic component capable of suppressing occurrence of surface imperfections (e.g., a burr) in appearance so as to facilitate assembly with a counterpart component, and an electronic device including the electronic component."
180,electronic device,electronic component,rhyper,-1,"According to various embodiments, it is possible to provide an electronic component, which is easy to process so as to facilitate mass production thereof and to meet the specification of an individual electronic device, and an electronic device including the electronic component."
181,electronic device,electronic component,rhyper,-1,"An electronic component (and/or an electronic device including the electronic component) according to various embodiments of the present disclosure may include: a substrate having a sensor element mounted on one face thereof;a flexible printed circuit board coupled to face a remaining face of the substrate and extending to one side of the substrate in a first direction; andat least one recess formed at an edge of the remaining face of the substrate,in which the recess may be located at least in a region facing the flexible printed circuit board on the remaining face of the substrate, and may extend in a second direction intersecting the first direction."
182,storage device,main memory,rhyper,-1,"The stocker controller 6 controls and manages respective parts of the automated warehouse S including the transporter 1. The stocker controller 6, for example, controls the operation of each part of the transporter 1 and manages storage history of the articles 2 and so forth on the storage racks 4. The stocker controller 6 is arranged outside the housing 3. The stocker controller 6 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. The stocker controller 6 is connected to the respective parts of the automated warehouse S so as to be capable of communicating therewith in a wired or wireless manner. Also, the stocker controller 6 is connected to a controller 37 of a manager 16 so as to be capable of communicating therewith in a wired or wireless manner. Note that the stocker controller 6 may be arranged inside or outside the housing 3."
183,storage device,input device,rhyper,-1,"The stocker controller 6 controls and manages respective parts of the automated warehouse S including the transporter 1. The stocker controller 6, for example, controls the operation of each part of the transporter 1 and manages storage history of the articles 2 and so forth on the storage racks 4. The stocker controller 6 is arranged outside the housing 3. The stocker controller 6 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. The stocker controller 6 is connected to the respective parts of the automated warehouse S so as to be capable of communicating therewith in a wired or wireless manner. Also, the stocker controller 6 is connected to a controller 37 of a manager 16 so as to be capable of communicating therewith in a wired or wireless manner. Note that the stocker controller 6 may be arranged inside or outside the housing 3."
184,storage device,hard disk,rhyper,-1,"The stocker controller 6 controls and manages respective parts of the automated warehouse S including the transporter 1. The stocker controller 6, for example, controls the operation of each part of the transporter 1 and manages storage history of the articles 2 and so forth on the storage racks 4. The stocker controller 6 is arranged outside the housing 3. The stocker controller 6 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. The stocker controller 6 is connected to the respective parts of the automated warehouse S so as to be capable of communicating therewith in a wired or wireless manner. Also, the stocker controller 6 is connected to a controller 37 of a manager 16 so as to be capable of communicating therewith in a wired or wireless manner. Note that the stocker controller 6 may be arranged inside or outside the housing 3."
185,storage device,wireless communication,rhyper,-1,"The stocker controller 6 controls and manages respective parts of the automated warehouse S including the transporter 1. The stocker controller 6, for example, controls the operation of each part of the transporter 1 and manages storage history of the articles 2 and so forth on the storage racks 4. The stocker controller 6 is arranged outside the housing 3. The stocker controller 6 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. The stocker controller 6 is connected to the respective parts of the automated warehouse S so as to be capable of communicating therewith in a wired or wireless manner. Also, the stocker controller 6 is connected to a controller 37 of a manager 16 so as to be capable of communicating therewith in a wired or wireless manner. Note that the stocker controller 6 may be arranged inside or outside the housing 3."
186,storage device,input device,rhyper,-1,"The manager 16 is arranged outside the housing 3. The manager 16 includes a display 34, an inputter 35, the recorder 36, the controller 37, and the data bus 38 used for transmitting data of the respective parts. The display 34, the inputter 35, the recorder 36, and the controller 37 are each connected to the data bus 38 and mutually exchange data. The manager 16 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. Note that the stocker controller 6 may also serve as a part of or the entire manager 16. The manager 16 may be arranged inside or outside the housing 3."
187,storage device,hard disk,rhyper,-1,"The manager 16 is arranged outside the housing 3. The manager 16 includes a display 34, an inputter 35, the recorder 36, the controller 37, and the data bus 38 used for transmitting data of the respective parts. The display 34, the inputter 35, the recorder 36, and the controller 37 are each connected to the data bus 38 and mutually exchange data. The manager 16 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. Note that the stocker controller 6 may also serve as a part of or the entire manager 16. The manager 16 may be arranged inside or outside the housing 3."
188,storage device,wireless communication,rhyper,-1,"The manager 16 is arranged outside the housing 3. The manager 16 includes a display 34, an inputter 35, the recorder 36, the controller 37, and the data bus 38 used for transmitting data of the respective parts. The display 34, the inputter 35, the recorder 36, and the controller 37 are each connected to the data bus 38 and mutually exchange data. The manager 16 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. Note that the stocker controller 6 may also serve as a part of or the entire manager 16. The manager 16 may be arranged inside or outside the housing 3."
189,storage device,main memory,rhyper,-1,"The manager 16 is arranged outside the housing 3. The manager 16 includes a display 34, an inputter 35, the recorder 36, the controller 37, and the data bus 38 used for transmitting data of the respective parts. The display 34, the inputter 35, the recorder 36, and the controller 37 are each connected to the data bus 38 and mutually exchange data. The manager 16 is configured with a computer device including a CPU, a memory storage device such as a main memory and a hard disk, a wired or wireless communication device, an input device such as a keyboard or a mouse, and a display device such as a display. The computer device reads various programs stored in the memory storage device and executes processes according to the programs. Note that the stocker controller 6 may also serve as a part of or the entire manager 16. The manager 16 may be arranged inside or outside the housing 3."
190,neural network,convolutional neural network,rhyper,-1,"The mobile computing device 104 can be, e.g., a mobile phone, a tablet computer, or a wearable computing device, such as smart glasses. The mobile computing device 104 includes one or more data processors 118, one or more cameras 120, one or more motion sensors 122, and a touch screen display 138. Each camera 120 includes one or more image sensors that are sensitive to visible light and optionally, infrared light. The mobile computing device 104 includes a storage device 124 storing program instructions for an augmented reality toolkit 126 and program instructions for a spot cleaning program 132. The storage device 124 can store one or more image detection or recognition modules. For example, the image detection or recognition module can include a neural network, such as a convolutional neural network. In this example, the storage device 124 stores a first image detection module 133 that includes a first neural network 134 for recognizing the mobile cleaning robot 102, and a second image detection module 135 that includes a second neural network 136 for recognizing the orientation angle of the mobile cleaning robot 102."
191,electronic devices,mobile phone,rhyper,-1,"In the example shown in FIG. 4, a second mobile cleaning robot 238 is located in the room 214C. The second mobile cleaning robot 238, similar to the mobile cleaning robot 102, performs a mission, e.g., a cleaning mission, within the room 214C. In some examples, the mobile computing device 104 is wirelessly connected to the multiple robotic devices, including the mobile cleaning robot 102 and the second mobile cleaning robot 238, thus enabling the user 212 to interact with the mobile computing device 104 to control and monitor multiple robotic devices 102, 238. In some examples, the controller for each of the mobile cleaning robot 102, the linked devices 292A, 292B, the second mobile cleaning robot 238, and other devices may initiate and maintain wireless links directly with one another, for example, to initiate and maintain a wireless link between the mobile cleaning robot 102 and one of the linked devices 292A, 292B. Wireless links also may be formed with other remote electronic devices, such as a mobile phone, a tablet, a laptop, another mobile computing device, one or more environmental control devices, or other types of electronic devices. In certain implementations, the wireless links permit communication with one or more devices including, but not limited to smart light bulbs, thermostats, garage door openers, door locks, remote controls, televisions, security systems, security cameras, smoke detectors, video game consoles, other robotic systems, or other communication enabled sensing and/or actuation devices or appliances."
192,storage media,hard drives,rhyper,-1,"The controllers described in this document can include one or more processors. Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only storage area or a random access storage area or both. Elements of a computer include one or more processors for executing instructions and one or more storage area devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from, or transfer data to, or both, one or more machine-readable storage media, such as hard drives, magnetic disks, magneto-optical disks, or optical disks. Machine-readable storage media suitable for embodying computer program instructions and data include various forms of non-volatile storage area, including by way of example, semiconductor storage devices, e.g., EPROM, EEPROM, and flash storage devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM discs."
193,the environment,foot traffic,rhyper,-1,"In this document, we describe a novel system for enabling a mobile robot to learn about its environment and perform tasks based an understanding about the environment, including an understanding about how the environment changes over time. For example, a mobile cleaning robot may perform a cleaning task more thoroughly when cleaning a dining room that is used daily than when cleaning a guest room that is used infrequently. The mobile cleaning robot may schedule cleaning tasks at times when no one is at home so as to reduce interruption to the home owners. The mobile robot can have one or more sensors, such as one or more cameras, that sense information about the environment. A machine learning module enables the mobile robot to recognize objects in the environment, as well as recognize the position of the robot in the environment, based on the information provided by the one or more sensors. The machine learning module also enables the mobile robot to learn about patterns in the environment, such as foot traffic in various parts of the house, and changes in positions of objects such as doors and chairs, to help the mobile robot schedule tasks."
194,mobile device,mobile phone,hyper,1,"In some implementations, the owner can use a mobile device, such as a mobile phone or a tablet computer, to identify objects and communicate with the mobile robot. In the following example, the mobile device is a mobile phone. The mobile phone is equipped with one or more cameras that can capture images of the mobile robot and the environment. The mobile phone executes an application program (e.g., an augmented reality toolkit) that establishes a coordinate system and determines the position of the mobile robot and the positions of the objects in the environment within the coordinate system."
195,mobile device,tablet computer,rhyper,-1,"In some implementations, the owner can use a mobile device, such as a mobile phone or a tablet computer, to identify objects and communicate with the mobile robot. In the following example, the mobile device is a mobile phone. The mobile phone is equipped with one or more cameras that can capture images of the mobile robot and the environment. The mobile phone executes an application program (e.g., an augmented reality toolkit) that establishes a coordinate system and determines the position of the mobile robot and the positions of the objects in the environment within the coordinate system."
196,mobile device,mobile phone,rhyper,-1,"In some implementations, the owner can use a mobile device, such as a mobile phone or a tablet computer, to identify objects and communicate with the mobile robot. In the following example, the mobile device is a mobile phone. The mobile phone is equipped with one or more cameras that can capture images of the mobile robot and the environment. The mobile phone executes an application program (e.g., an augmented reality toolkit) that establishes a coordinate system and determines the position of the mobile robot and the positions of the objects in the environment within the coordinate system."
197,the environment,foot traffic,rhyper,-1,"The mobile robot 102 includes a learning module 126 that is configured to learn about patterns in the environment, such as foot traffic in a home. For example, the learning module 126 can be configured to store certain parameter values over time and perform statistical analyses of the stored parameter values to detect patterns in the data. The learning module 126 may store counts of human presence at each grid point on a map for each time period of the day for each day of the week. By analyzing the stored data, the learning module 126 can determine, e.g., for a given time during a given day of the week, which grid points on the map have higher or lower foot traffic. The learning module 126 can determine, e.g., for a given room in the house, which periods of time have less or no foot traffic."
198,electronic devices,mobile phone,rhyper,-1,"In the example shown in FIG. 3, a second mobile cleaning robot 103 is located in the room 302C. The second mobile cleaning robot 103, similar to the mobile cleaning robot 102, performs a mission, e.g., a cleaning mission, within the room 302C. In some examples, the mobile computing device 104 is wirelessly connected to the multiple robotic devices, including the mobile cleaning robot 102 and the second mobile cleaning robot 103, thus enabling the user 10 to interact with the mobile computing device 104 to control and monitor multiple robotic devices 102, 103. In some examples, the controller for each of the mobile cleaning robots 102, 103, the linked devices 328A, 328B, and other devices may initiate and maintain wireless links directly with one another, for example, to initiate and maintain a wireless link between the mobile cleaning robot 102 or 103 and one of the linked devices 328A, 328B. Wireless links also may be formed with other remote electronic devices, such as a mobile phone, a tablet, a laptop, another mobile computing device, one or more environmental control devices, or other types of electronic devices. In certain implementations, the wireless links permit communication with one or more devices including, but not limited to smart light bulbs, thermostats, garage door openers, door locks, remote controls, televisions, security systems, security cameras, smoke detectors, video game consoles, other robotic systems, or other communication enabled sensing and/or actuation devices or appliances."
199,storage media,hard drives,rhyper,-1,"The controllers described in this document can include one or more processors. Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only storage area or a random access storage area or both. Elements of a computer include one or more processors for executing instructions and one or more storage area devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from, or transfer data to, or both, one or more machine-readable storage media, such as hard drives, magnetic disks, magneto-optical disks, or optical disks. Machine-readable storage media suitable for embodying computer program instructions and data include various forms of non-volatile storage area, including by way of example, semiconductor storage devices, e.g., EPROM, EEPROM, and flash storage devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM discs."
200,occupancy grid,living room,hyper,1,"The map merge module 124 honors the room labels provided by the user. If the user 10 labeled a room as the ""Living Room,"" the map merge module 124 will try to find out which region in the new occupancy grid is the living room and associate it with the label ""Living Room."""
201,the environment,foot traffic,rhyper,-1,"The mobile robot 202a includes a learning module 210 that is configured to learn about patterns in the environment, such as foot traffic in a home. For example, the learning module 210 can be configured to store certain parameter values over time and perform statistical analyses of the stored parameter values to detect patterns in the data. The learning module 210 may store counts of human presence at each grid point on a map for each time period of the day for each day of the week. By analyzing the stored data, the learning module 210 can determine, e.g., for a given time during a given day of the week, which grid points on the map have higher or lower foot traffic. The learning module 210 can determine, e.g., for a given room in the house, which periods of time have less or no foot traffic."
202,electronic devices,mobile phone,rhyper,-1,"In the example shown in FIG. 3, a second mobile cleaning robot 202b is located in the room 302C. The second mobile cleaning robot 202b, similar to the first mobile cleaning robot 202a, performs a mission, e.g., a cleaning mission, within the room 302C. In some examples, the mobile computing device 94 is wirelessly connected to the multiple robotic devices, including the first mobile cleaning robot 202a and the second mobile cleaning robot 202b, thus enabling the user 10 to interact with the mobile computing device 94 to control and monitor multiple robotic devices 202a, 202b. In some examples, the controller 104 for each of the mobile cleaning robots 202a, 202b, the linked devices 328A, 328B, and other devices may initiate and maintain wireless links directly with one another, for example, to initiate and maintain a wireless link between the mobile cleaning robot 202a or 202b and one of the linked devices 328A, 328B. Wireless links also may be formed with other remote electronic devices, such as a mobile phone, a tablet, a laptop, another mobile computing device, one or more environmental control devices, or other types of electronic devices. In certain implementations, the wireless links permit communication with one or more devices including, but not limited to smart light bulbs, thermostats, garage door openers, door locks, remote controls, televisions, security systems, security cameras, smoke detectors, video game consoles, other robotic systems, or other communication enabled sensing and/or actuation devices or appliances."
203,storage media,hard drives,rhyper,-1,"The controllers described in this document can include one or more processors. Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only storage area or a random access storage area or both. Elements of a computer include one or more processors for executing instructions and one or more storage area devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from, or transfer data to, or both, one or more machine-readable storage media, such as hard drives, magnetic disks, magneto-optical disks, or optical disks. Machine-readable storage media suitable for embodying computer program instructions and data include various forms of non-volatile storage area, including by way of example, semiconductor storage devices, e.g., EPROM, EEPROM, and flash storage devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM discs."
204,objective function,loss function,hyper,1,"The facial expression training apparatus defines an objective function to measure how close currently set connection weights are to an optimal value, continues to change the connection weights based on a result of the objective function, and repeatedly performs training. For example, the objective function is a loss function used to calculate a loss between an expected value to be output and an actual output value based on a training input of training data in the neural network 100. The facial expression training apparatus updates the connection weights by reducing a value of the loss function."
205,facial expression,facial expression,hyper,1,"Referring to FIG. 8, in operation 810, a facial expression training apparatus acquires a reference image and a reference facial expression. The reference image is, for example, an image including an object provided for training. The reference facial expression is a facial expression mapped to the reference image, for example, a label that indicates a facial expression of an object included in the reference image. Training data includes a pair of the reference image and the reference facial expression. In an example, the facial expression training apparatus loads and acquires training data from a memory. However, examples are not limited thereto, and the facial expression training apparatus acquires training data from, for example, an external device. Also, the facial expression training apparatus further acquires a reference voice corresponding to the reference image."
206,peripheral bus,Peripheral Component Interconnect,rhyper,-1,"In some embodiments, the platform controller hub 130 enables peripherals to connect to memory device 120 and processor 102 via a high-speed I/O bus. The I/O peripherals include, but are not limited to, an audio controller 146, a network controller 134, a firmware interface 128, a wireless transceiver 126, touch sensors 125, a data storage device 124 (e.g., hard disk drive, flash memory, etc.). The data storage device 124 can connect via a storage interface (e.g., SATA) or via a peripheral bus, such as a Peripheral Component Interconnect bus (e.g., PCI, PCI Express). The touch sensors 125 can include touch screen sensors, pressure sensors, or fingerprint sensors. The wireless transceiver 126 can be a Wi-Fi transceiver, a Bluetooth transceiver, or a mobile network transceiver such as a 3G, 4G, or Long Term Evolution (LTE) transceiver. The firmware interface 128 enables communication with system firmware, and can be, for example, a unified extensible firmware interface (UEFI). The network controller 134 can enable a network connection to a wired network. In some embodiments, a high-performance network controller (not shown) couples with the interface bus 110. The audio controller 146, in one embodiment, is a multi-channel high definition audio controller. In one embodiment, the system 100 includes an optional legacy I/O controller 140 for coupling legacy (e.g., Personal System 2 (PS/2)) devices to the system. The platform controller hub 130 can also connect to one or more Universal Serial Bus (USB) controllers 142 connect input devices, such as keyboard and mouse 143 combinations, a camera 144, or other USB input devices."
207,display device,virtual reality,rhyper,-1,"In some embodiments, graphics processor 300 also includes a display controller 302 to drive display output data to a display device 320. Display controller 302 includes hardware for one or more overlay planes for the display and composition of multiple layers of video or user interface elements. The display device 320 can be an internal or external display device. In one embodiment, the display device 320 is a head mounted display device, such as a virtual reality (VR) display device or an augmented reality (AR) display device. In some embodiments, graphics processor 300 includes a video codec engine 306 to encode, decode, or transcode media to, from, or between one or more media encoding formats, including, but not limited to Moving Picture Experts Group (MPEG) formats such as MPEG-2, Advanced Video Coding (AVC) formats such as H.264/MPEG-4 AVC, as well as the Society of Motion Picture & Television Engineers (SMPTE) 421M/VC-1, and Joint Photographic Experts Group (JPEG) formats such as JPEG, and Motion JPEG (MJPEG) formats."
208,display device,display device,rhyper,-1,"In some embodiments, graphics processor 300 also includes a display controller 302 to drive display output data to a display device 320. Display controller 302 includes hardware for one or more overlay planes for the display and composition of multiple layers of video or user interface elements. The display device 320 can be an internal or external display device. In one embodiment, the display device 320 is a head mounted display device, such as a virtual reality (VR) display device or an augmented reality (AR) display device. In some embodiments, graphics processor 300 includes a video codec engine 306 to encode, decode, or transcode media to, from, or between one or more media encoding formats, including, but not limited to Moving Picture Experts Group (MPEG) formats such as MPEG-2, Advanced Video Coding (AVC) formats such as H.264/MPEG-4 AVC, as well as the Society of Motion Picture & Television Engineers (SMPTE) 421M/VC-1, and Joint Photographic Experts Group (JPEG) formats such as JPEG, and Motion JPEG (MJPEG) formats."
209,display device,augmented reality,rhyper,-1,"In some embodiments, graphics processor 300 also includes a display controller 302 to drive display output data to a display device 320. Display controller 302 includes hardware for one or more overlay planes for the display and composition of multiple layers of video or user interface elements. The display device 320 can be an internal or external display device. In one embodiment, the display device 320 is a head mounted display device, such as a virtual reality (VR) display device or an augmented reality (AR) display device. In some embodiments, graphics processor 300 includes a video codec engine 306 to encode, decode, or transcode media to, from, or between one or more media encoding formats, including, but not limited to Moving Picture Experts Group (MPEG) formats such as MPEG-2, Advanced Video Coding (AVC) formats such as H.264/MPEG-4 AVC, as well as the Society of Motion Picture & Television Engineers (SMPTE) 421M/VC-1, and Joint Photographic Experts Group (JPEG) formats such as JPEG, and Motion JPEG (MJPEG) formats."
210,high-level shader language,High Level Shader Language,rhyper,-1,"In some embodiments, 3D graphics application 1010 contains one or more shader programs including shader instructions 1012. The shader language instructions may be in a high-level shader language, such as the High Level Shader Language (HLSL) or the OpenGL Shader Language (GLSL). The application also includes executable instructions 1014 in a machine language suitable for execution by the general-purpose processor core 1034. The application also includes graphics objects 1016 defined by vertex data."
211,machine learning algorithm,neural network,hyper,1,"An exemplary type of machine learning algorithm is a neural network. There are many types of neural networks; a simple type of neural network is a feedforward network. A feedforward network may be implemented as an acyclic graph in which the nodes are arranged in layers. Typically, a feedforward network topology includes an input layer and an output layer that are separated by at least one hidden layer. The hidden layer transforms input received by the input layer into a representation that is useful for generating output in the output layer. The network nodes are fully connected via edges to the nodes in adjacent layers, but there are no edges between nodes within each layer. Data received at the nodes of an input layer of a feedforward network are propagated (i.e., ""fed forward"") to the nodes of the output layer via an activation function that calculates the states of the nodes of each successive layer in the network based on coefficients (""weights"") respectively associated with each of the edges connecting the layers. Depending on the specific model being represented by the algorithm being executed, the output from the neural network algorithm can take various forms."
212,neural network,Convolutional Neural Network,hyper,1,"A second exemplary type of neural network is the Convolutional Neural Network (CNN). A CNN is a specialized feedforward neural network for processing data having a known, grid-like topology, such as image data. Accordingly, CNNs are commonly used for compute vision and image recognition applications, but they also may be used for other types of pattern recognition such as speech and language processing. The nodes in the CNN input layer are organized into a set of ""filters"" (feature detectors inspired by the receptive fields found in the retina), and the output of each set of filters is propagated to nodes in successive layers of the network. The computations for a CNN include applying the convolution mathematical operation to each filter to produce the output of that filter. Convolution is a specialized kind of mathematical operation performed by two functions to produce a third function that is a modified version of one of the two original functions. In convolutional network terminology, the first function to the convolution can be referred to as the input, while the second function can be referred to as the convolution kernel. The output may be referred to as the feature map. For example, the input to a convolution layer can be a multidimensional array of data that defines the various color components of an input image. The convolution kernel can be a multidimensional array of parameters, where the parameters are adapted by the training process for the neural network."
213,embedded devices,tablet computers,rhyper,-1,"As previously described, conventional deep learning-based recognition techniques often require and consume large amount of memory and computing power and thus such techniques are not suitable for low-end, smaller, or embedded devices, such as tablet computers, smartphones, HMDs, etc. For example, in visual recognition, there may be over 60 million parameters (needing a 250MB network model size), while performing over 1.5 million floating-point operations per image (measured with normalized size 224x224). Certain deeper networks require even larger network model size (such as over 500MB) having even bigger computing requirements (such as greater than 16 million floating operations). At least due to these limitations, most current training and scoring operations are performed with bigger or high-end computing devices and are prevented from being deployed on smaller or low-end computing devices."
214,computing device,computing device,rhyper,-1,"Throughout this document, terms like ""logic"", ""component"", ""module"", ""framework"", ""engine"", ""mechanism"", ""circuit"", and ""circuitry"", and/or the like, are referenced interchangeably and may include, by way of example, software, hardware, firmware, or any combination thereof. In one example, ""logic"" may refer to or include a software component that is capable of working with one or more of an operating system (e.g., operating system 2006), a graphics driver (e.g., graphics driver 2016), etc., of a computing device, such as computing device 2000. In another example, ""logic"" may refer to or include a hardware component that is capable of being physically installed along with or as part of one or more system hardware elements, such as an application processor (e.g., CPU 2012), a graphics processor (e.g., GPU 2014), etc., of a computing device, such as computing device 2000. In yet another embodiment, ""logic"" may refer to or include a firmware component that is capable of being part of system firmware, such as firmware of an application processor (e.g., CPU 2012) or a graphics processor (e.g., GPU 2014), etc., of a computing device, such as computing device 2000."
215,light source,point light source,hyper,1,"Preferably, the light source is a point light source."
216,light source,point light source,hyper,1,"Preferably, the light source is a point light source."
217,light source,point light source,hyper,1,"Preferably, the light source is a point light source."
218,light source,point light source,hyper,1,"I. An object position and orientation detection system comprising: an object, at least part of which comprises a patterned marker having a detectable pattern arranged thereon; imaging means adapted to create one or more images of the object; and image processing means adapted to calculate the position and orientation of the object in space from the appearance of the patterned marker in the one or more images.II. A system as described in clause I wherein, the patterned marker is substantially spherical.III. A system as described in clause I wherein, the patterned marker consists of a single sphere.IV. A system as described in any of clauses I to III wherein, the image processing means calculates the position and orientation of the object by solving a formula representative of the appearance of the patterned marker in all possible positions and orientations for the appearance of the patterned marker in the one or more images.V. A system as described in any preceding clause wherein, the pattern is periodic.VI. A system as described in any preceding clause wherein, the pattern on the spherical patterned marker comprises a set of rings around the sphere which create a variety of patterns depending on the orientation of the sphere with respect to the imaging means.VII. A system as described in any preceding clause wherein, the position and orientation of the object are calculated in real time.VIII. A system as described in any preceding clause wherein, the pattern as viewed by the imaging means changes dependent on the orientation of the patterned marker relative to the imaging means.IX. A system as described in any preceding clause wherein, the pattern as viewed by the imaging means changes dependent on the position of the patterned marker relative to the imaging means.X. A system as described in any preceding clause wherein, the pattern as viewed by the imaging means is used to convey the position and orientation of the device relative to a surface.XI. A system as described in any preceding clause wherein, the image processing means detects a feature of the pattern on the patterned marker which is dependent upon the orientation and position of the pattern relative to the camera.XII. A system as described in any preceding clause wherein, the patterned marker is hollow with a surface adapted to diffuse light.XIII. A system as described in clause XII wherein, the surface comprises celluloid.XIV. A system as described in clause XII or clause XIII wherein, the surface has a matt finish.XV. A system as described in any of clauses XII to XIV wherein, the surface of the patterned marker and the material from which the marking is made have the same reflective properties, but different transmission properties.XVI. A system as described in any preceding clause wherein, the pattern is enhanced by illuminating it with a light source.XVII. A system as described in clause XVI wherein, the pattern is made detectable by the imaging means by illuminating it with a light source.XVIII. A system as described in clause XVI or clause XVII wherein, the light source is a point light source.XIX. A system as described in any of clauses XVI to XVIII wherein, the light source is positioned behind the patterned marker.XX. A system as described in any of clauses XVI to XIX wherein, the light source provides infra-red illumination.XXI. A system as described in any of clauses XVI to XIX wherein, the light source provides visible illumination.XXII. A system as described in any of clauses XVI to XIX wherein, the light source provides ultraviolet illumination.XXIII. A system as described in any of clauses XVI to XXII wherein, the imaging means is configured to detect the illumination provided by the light source.XXIV. A system as described in any preceding clause wherein, the imaging means is provided with an optical filter.XXV. A system as described in clause XXIV wherein, the optical filter is selected to prevent the imaging means from receiving electromagnetic radiation at frequencies other than those provided by the light source.XXVI. A system as described in any preceding clause wherein, the pattern can be switched on and off.XXVII. A system as described in any preceding clause wherein, the pattern is not detectable when the light source is switched off.XXVIII. A system as described in any preceding clause wherein, the system uses the switching to communicate information to the computer system.XXIX. A system as described in clause XXVIII wherein, the information may be used to select a computer function.XXX. A system as described in clause XXVIII or clause XXIX wherein, the information may be used to convey the position and orientation of the device relative to a surface.XXXI. A system as described in any preceding clause wherein, the image processing means further comprises matching software which matches an artificial pattern to the pattern in the image.XXXII. A system as described in clause XXXI wherein, position, size and orientation parameters which define the artificial pattern can be fine-tuned until it correlates best with the pattern in the image.XXXIII. A method for operating an object position and orientation detection device, the method comprising the steps of: obtaining one or more images of an object, at least part of which comprises a patterned marker having a detectable pattern arranged thereon; and calculating the position and orientation of the object in space from the appearance of the patterned marker in the one or more images.XXXIV. A method as described in clause XXXIII wherein, calculating the position and orientation of the object includes solving a formula representative of the appearance of the patterned marker in all possible positions and orientations for the appearance of the patterned marker in the one or more images.XXXV. A method as described in clause XXXIII or clause XXXIV wherein, the position and orientation of the object are calculated in a. real time.XXXVI. A method as described in any of clauses XXXIII to XXXV wherein, the pattern changes dependent on the orientation of the patterned marker in respective images.XXXVII. A method as described in any of clauses XXXIII to XXXVI wherein, the pattern changes dependent on the position of the patterned marker in respective images.XXXVIII. A method as described in any of clauses XXXIII to XXXVII wherein, the patterned marker is substantially spherical.XXXIX. A method as described in any of clauses XXXIII to XXXVIII wherein, the method further comprises detecting features of the pattern on the patterned marker which are dependent upon one or both of the orientation and position of the pattern relative to the camera.XL. A method as described in any of clauses XXXIII to XXXIX wherein, the pattern on the spherical patterned marker comprises a set of rings around the sphere which create a variety of patterns depending on the orientation of the sphere with respect to the imaging means.XLI. A method as described in any of clauses XXXIII to XL wherein, the patterned marker is hollow with a surface adapted to diffuse light.XLII. A method as described in clause XLI wherein, the surface comprises celluloid.XLIII. A method as described in clause XLI or clause XLII wherein, the surface has a matt finish.XLIV. A method as described in any of clauses XLI to XLIII wherein, the surface of the patterned marker and the material from which the marking is made have the same reflective properties, but different transmission properties,XLV. A method as described in any of clauses XXXIII to XLIV wherein, the method further comprises illuminating the pattern with a light source to allow detection of the pattern.XLVI. A method as described in clause XLV wherein, the light source is a point light source.XLVII. A method as described in clause XLV or clause XLVI wherein, the light source is positioned behind the patterned marker.XLVIII. A method as described in any of clauses XLV to XLVII wherein, the light source provides infra-red illumination.XLIX. A method as described in any of clauses XLV to XLVII wherein, the light source provides visible illumination.L. A method as described in any of clauses XLV to XLVII wherein, the light source provides ultraviolet illumination.LI. A method as described in any of clauses XXXIII to L wherein, the method further comprises optical filtering.LII. A method as described in any of clauses XXXIII to LI wherein, a filter is used to prevent imaging of electromagnetic radiation of frequencies other than those provided by the light source.LIII. A method as described in any of clauses XXXIII to LII wherein, the pattern can be switched on and off.LIV. A method as described in clause LIII wherein, the pattern is not detectable when the light source is switched off.LV. A method as described in clause LIII or clause LIV wherein, switching communicates information to the computer system.LVI. A method as described in clause LV wherein, the information may be used to select a computer function.LVII. A method as described in clause LV or clause LVI wherein, the information may be used to convey one or both of the position and orientation of the device relative to a surface.LVIII. A method as described in any of clauses XXXIII to LVII wherein, the method further comprises matching an artificial pattern to the pattern in the image.LIX. A method as described in clause LVIII wherein, the position, size and orientation parameters which define the artificial pattern can be fine-tuned until it correlates best with the pattern in the image.LX. An object position and orientation detection device comprising a patterned marker wherein the pattern is detectable by an imaging means in successive images in order to detect changes in one or both of an orientation and a position of the device.LXI. A device as described clause LX wherein, the patterned marker is substantially spherical.LXII. A device as described in clause LX wherein, the patterned marker consists of a sphere.LXIII. LXII A device as described in clause LX wherein, the pattern on the spherical patterned marker comprises a set of rings around the sphere which create a variety of patterns depending on the orientation of the sphere with respect to the imaging means.LXIV. A device as described in any of clauses LXI to LXIII wherein, the patterned marker is hollow with a surface adapted to diffuse light.LXV. A device as described in clause LXIV wherein, the surface comprises celluloid.LXVI. A device as described in clause LXIV or clause LXV wherein, the surface has a matt finish.LXVII. A device as described in any of clauses LXIV to LXVI wherein, the surface of the patterned marker and the material from which the marking is made have the same reflective properties, but different transmission properties,LXVIII. A device as described in any of clauses LXI to LXVII wherein, the pattern is enhanced by illuminating it with a light source.LXIX. A device as described in clause LXVIII wherein, the pattern is made detectable by the imaging means by illuminating it with a light source.LXX. A device as described in clause LXVIII or clause LXIX wherein, the light source is a point light source.LXXI. A device as described in any of clauses LXVII to LXX wherein, the light source is positioned behind the patterned marker.LXXII. A device as described in any of clauses LXVIII to LXXI wherein, the light source provides infra-red illumination.LXXIII. A device as described in any of clauses LXVIII to LXXI wherein, the light source provides visible illumination.LXXIV. A device as described in any of clauses LXVIII to LXXI wherein, the light source provides ultraviolet illumination.LXXV. A device as described in any of clauses LXVIII to LXXIV wherein, the pattern can be switched on and off.LXXVI. A device as described in any of clauses LXVIII to LXXV wherein, the pattern is not detectable when the light source is switched off.LXXVII. A device as described in clause LXXVI wherein, the device uses the switching to communicate information to the computer system.LXXVIII. A device as described in clause LXXVII wherein, the information may be used to select a computer function.LXXIX. A device as described in clause LXXVII or clause LXXVIII wherein, the information may be used to convey one or both of the position and orientation of the device relative to a surface.LXXX. A device as described in any of clauses LXVIII to LXXIX wherein, the device further comprises a pointer extending from the patterned marker.LXXXI. A device as described in clause LXXX wherein, the light source is attached to the pointer.LXXXII. A device as described in any of clauses LXVIII to LXXXI wherein, the light source is located within the sphere.LXXXIII. A method of calibrating an object position and orientation detection system according to any of Clauses I to XXXII with a corresponding display, comprising the steps of: (a) providing a calibration image on the display; (b) detecting the calibration image using the imaging means; and (c) determining a mapping between the calibration image and the detected calibration image.LXXXIV. The method of clause LXXXIII wherein, the step of determining a mapping between the calibration image and the detected calibration image comprises: (i) providing a plurality of calibration markers in the calibration image; and (ii) comparing the position of the plurality of calibration markers in the calibration image with the position of the plurality of calibration markers in the detected calibration image.LXXXV. The method of clause LXXXIV wherein, comparing the position of the markers comprises detecting the position of the calibration markers in the calibration image and correlating them with corresponding markers in the detected calibration image.LXXXVI. The method of clause LXXXIV or clause LXXXV wherein, comparing the position of the markers comprises comparing at least four pairs of calibration markers and detected calibration markers.LXXXVII. A method of aligning the imaging device of an object position and orientation detection system comprising the steps of: (a) providing an image on a display; (b) detecting the image using the imaging device; and (c) providing a sub-image on the display corresponding to a virtual projection on the display originating from the imaging device. WLXXXVIII. A method as described in clause LXXXVII wherein, the virtual projection mimics the projection of a predetermined shape onto the display.LXXXIX. A method as described in clause LXXXVII or clause LXXXVIII further comprising the step of calibrating the object position and orientation detection system with the display.XC. A method as described in clause LXXXIX wherein, calibrating the object position and orientation detection system with the display comprises the method of any of clauses LXXXIII to LXXXVI.XCI. A pattern for the patterned marker of any of clauses I to LXXXVI or clause XC wherein, the pattern comprises only two singular points corresponding to the intersection of an axis through the marker with the surface of the marker.XCII. A pattern for the patterned marker of any of clauses I to LXXXVI or clause XC wherein, the pattern comprises at least three radial symmetries around one distinguished axis through the marker.XCIII. A pattern as described in clause XCI wherein, the pattern exhibits continuous radial symmetry around a distinguished axis through the marker.XCIV. A pattern as described in clause XCII or clause XCIII wherein, the pattern comprises regularly spaced rings.XCV. A pattern as described in any of clauses XCI to XCIV wherein, the pattern comprises a gradual transition from a low intensity region to a high intensity region.XCVI. A calibration method for an object position and orientation detection system and a corresponding display, comprising:providing a calibration image on the display;detecting the calibration image using an imaging means of the object position and orientation detection system; anddetermining a mapping between the calibration image and the detected calibration image.XCVII. The calibration method of clause XCVI, further comprising filtering to block light from the calibration image in one or more colour channels of the imaging means.XCVIII. The calibration method of clause XCVI or clause XCVII, wherein determining a mapping between the calibration image and the detected calibration image comprises:employing a mathematical model describing how light rays emerge from a projector of the object position and orientation detection system, hit the display, and are reflected to the imaging means; anddetermining parameters of the mapping based on distortion between the calibration image emerging from the projector and calibration image as detected by the imaging means.XCIX. The calibration method of any of clauses XCVI to XCVIII, wherein determining a mapping between the calibration image and the detected calibration image comprises:providing a plurality of calibration markers in the calibration image; andcomparing the position of the plurality of calibration markers in the calibration image with the position of the plurality of calibration markers in the detected calibration image.C. The calibration method of clause XCIX wherein, comparing the position of the markers comprises detecting the position of the calibration markers in the calibration image and correlating them with corresponding markers in the detected calibration image.CI. The calibration method of clause XCIX or clause C wherein, comparing the position of the markers comprises comparing at least four pairs of calibration markers and detected calibration markers.CII. An alignment method for an imaging device of an object position and orientation detection system, comprising:providing an image on a display;detecting the image using the imaging device; andproviding a sub-image on the display corresponding to a virtual projection on the display originating from the imaging device.CIII. The alignment method of clause CII wherein, the virtual projection mimics the projection of a predetermined shape onto the display.CIV. The alignment method of clause CII or clause CIII, wherein the virtual projection creates the illusion of a beam hitting the screen from the direction of the camera.CV. The alignment method of clause CIV, wherein the camera is moved until the beam overlaps a displayed target.CVI. The alignment method of any of clauses CII to CV, comprising filtering to block light from the sub-image in one or more colour channels of the imaging device.CVII. The alignment method of any of clauses CII to CVI, further comprising calibrating the object position and orientation detection system with the display.CVIII. The alignment method of clause CVII, wherein calibrating the object position and orientation detection system with the display comprises the method of any of clauses XCVI to CI.CIX. The alignment method of any of clauses CII to CVIII, wherein the method is repeated real-time at the frame rate of the imaging device.CX. A method for detecting the position and orientation of an object comprising:providing the object with a patterned marker having a detectable image arranged thereon;providing an imaging means adapted to create one or more images of the object; andproviding an image processing means adapted to calculate the position and orientation of the object in space from the appearance of the patterned marker in the one or more images;wherein the method further comprises performing the calibration method of any of clauses XCVI to CI or performing the alignment method of any of clauses CII to CIX."
219,storage device,Hard Disk Drive,rhyper,-1,The storage unit 304 is a storage device such as a Hard Disk Drive (HDD) or a Solid State Drive (SSD). The storage unit 304 stores a control wireless tag reading method 314. The control wireless tag reading method 314 is a wireless tag reading method for fulfilling the functions of the operating system and the POS terminal 30. The control wireless tag reading method 314 includes a wireless tag reading method for fulfilling the characteristic functions according to an exemplary embodiment.
220,storage medium,magnetic material,rhyper,-1,"The printing unit 310 is a printer that prints receipts or the like showing transaction details, point status, or the like on paper. The card reader and writer 311 is an apparatus that performs writing and read information stored in a storage medium such as a magnetic material attached to a card. The contactless card reader and writer 312 performs writing and reading of information to or from the IC tag of the contactless IC card by near field communication (NFC) or the like."
221,light sensor,Complementary Metal Oxide Semiconductor,rhyper,-1,"The sensor component 414 includes one or more sensors arranged to provide status assessment in various aspects for the device 400. For instance, the sensor component 414 may detect an on/off status of the device 400 and relative positioning of components, such as a display and small keyboard of the device 400, and the sensor component 414 may further detect a change in a position of the device 400 or a component of the device 400, presence or absence of contact between the user and the device 400, orientation or acceleration/deceleration of the device 400 and a change in temperature of the device 400. The sensor component 414 may include a proximity sensor arranged to detect presence of an object nearby without any physical contact. The sensor component 414 may also include a light sensor, such as a Complementary Metal Oxide Semiconductor (CMOS) or Charge Coupled Device (CCD) image sensor, configured for use in an imaging application. In the embodiment of the present disclosure, the sensor component 414 at least includes san accelerometer, a gyroscope and a magnetometer."
222,wireless network,Wireless Fidelity,rhyper,-1,"The communication component 416 is arranged to facilitate wired or wireless communication between the device 400 and other equipment. The device 400 may access a communication-standard-based wireless network, such as a Wireless Fidelity (WiFi) network, a 2nd-Generation (2G) or 4th-Generation (4G) network or a combination thereof. In an exemplary embodiment, the communication component 416 receives a broadcast signal or broadcast associated information from an external broadcast management system through a broadcast channel. In an exemplary embodiment, the communication component 416 further includes a Near Field Communication (NFC) module to facilitate short-range communication. For example, the NFC module may be implemented on the basis of a Radio Frequency Identification (RFID) technology, an Infrared Data Association (IrDA) technology, an Ultra-WideBand (UWB) technology, a Bluetooth (BT) technology and another technology."
223,machine learning,neural networks,rhyper,-1,"In a preferred embodiment, image sensor 704 acquires the image of the user's ear and processor 706 is configured to extract the pertinent properties for the user and sends them to remote server 710. For example, in one embodiment, an Active Shape Model can be used to identify landmarks in the ear pinnae image and to use those landmarks and their geometric relationships and linear distances to identify properties about the user that are relevant to selecting an HRTF from a collection of HRTF datasets, that is, from a candidate pool of HRTF datasets. In other embodiments an RGT model (Regression Tree Model) is used to extract properties. In still other embodiments, machine learning such as neural networks are used to extract properties. One example of a neural network is the Convolutional neural network. A full discussion of several methods for identifying unique physical properties of the new listener is described in Application PCT/SG2016/050621, filed on Dec. 28, 2016 and titled ""A Method for Generating a customized Personalized Head Related Transfer Function"", which disclosure is incorporated fully by reference herein."
224,neural network,Convolutional neural network,hyper,1,"In a preferred embodiment, image sensor 704 acquires the image of the user's ear and processor 706 is configured to extract the pertinent properties for the user and sends them to remote server 710. For example, in one embodiment, an Active Shape Model can be used to identify landmarks in the ear pinnae image and to use those landmarks and their geometric relationships and linear distances to identify properties about the user that are relevant to selecting an HRTF from a collection of HRTF datasets, that is, from a candidate pool of HRTF datasets. In other embodiments an RGT model (Regression Tree Model) is used to extract properties. In still other embodiments, machine learning such as neural networks are used to extract properties. One example of a neural network is the Convolutional neural network. A full discussion of several methods for identifying unique physical properties of the new listener is described in Application PCT/SG2016/050621, filed on Dec. 28, 2016 and titled ""A Method for Generating a customized Personalized Head Related Transfer Function"", which disclosure is incorporated fully by reference herein."
225,load sensor,load sensor,rhyper,-1,"The PHD 122 may determine the level of occupancy of each container 104a-c by comparing the determined weight to a weight stored in the database 132. For example, each container 104a-c may be listed in the database 132 and have a corresponding tare weight (e.g., empty container weight). The PHD 122 may compare the determined weight of the respective container(s) 104a-c measured via a load sensor, such as load sensor 120, to the tare weight of the respective container(s) 104a-c. Based on the comparison, the PHD 122 may determine the level of occupancy of the respective container(s) 104a-c. For example, if the determined weight of the respective container(s) 104a-c is greater than the respective tare weight, the PHD 122 may determine that the respective container(s) 104a-c is occupied. If the determined weight of the respective container(s) 104a-c container is similar to and/or equal to the tare weight, the PHD 122 may determine that the respective container(s) 104a-c is less than full, such as vacant. Accordingly, the database 132 may facilitate determining the level of occupancy of within the container(s) 104a-c."
226,storage media,flash memory,rhyper,-1,"The memory may include at least one type of storage media, such as a flash memory type, a hard disk type, a solid state disk (SDD) type, a silicon disk drive (SDD) type, a multimedia card micro type, a card type memory (for example, an SD, a XD memory, or the like), a random access memory (RAM), a static random access memory (SRAM), a read-only memory (ROM), an electrically erasable programmable read-only memory (EEPROM), a programmable read-only memory (PROM), a magnetic memory, a magnetic disk, and an optical disk. The input/output device 800 may operate in association with a web storage performing a storage function of the memory over the Internet."
227,active identifier,radio frequency identification,rhyper,-1,"FIG. 2a depicts a schematic front view of a marker in the form of a label comprising indicia 18 configured to be applied to an object in accordance with an example. As such, the marker can comprise a physical medium or carrier operable to support or carry thereon the marking indicia. For sake of clarity, the label or indicia 18 is shown as being visible, although such indicia is configured to be invisible to the unaided human eye (human-imperceptible) and is substantially transparent to visible light. FIG. 2b depicts a schematic front view of the label and indicia 18 supported thereon, but illustrated in such a way to indicate invisibility to the unaided human eye, and substantial transparency to visible light. The indicia 18 is machine-readable. Thus, the indicia 18 can include symbols or patterns 38 or both that can be perceived by the camera 26 or image sensor (or other sensor or detector) thereof, and recognized by the robot 22. In one aspect, the indicia 18 can be or can comprise a bar code that is linear or one-dimensional with a series of parallel lines with a variable thickness or spacing to provide a unique code, or a two-dimensional MaxiCode or QR code with a pattern of rectangular or square dots to provide a unique code, etc. In another aspect, the indicia can include symbols, letters, and even arrows 42 or the like. In another aspect, the indicia 18 can be passive, as opposed to an active identifier, such as a radio frequency identification (RFID) device or tag."
228,remote system,the service,rhyper,-1,"In some embodiments, the computer model may be accessed via an object detection module 115. The object detection module 115 may be a mobile application or other form of application running on the client device. In some embodiments, the object detection module 115 may include a trained computer model that can be used for image analysis on the client device 110. In another embodiment, the object detection module 115 may access a computer model or other functionality at a remote system, such as the service provider 140. The object detection module 115 is described in more detail in the description of FIG. 3."
229,Computer components,web services,rhyper,-1,"FIG. 2 is a high-level block diagram illustrating an architecture of a training server, in accordance with an embodiment. The training server 130 includes various modules and data stores for training computer models to detect objects within images. In one embodiment, the training server 130 comprises a training corpus 210, a neural network model 220, a training module 230, and a model distribution module 240. Computer components such as web services, network interfaces, security functions, load balancers, failover services, and management and network operations consoles are not shown so as to not obscure the details of the system architecture. Additionally, the training server 130 may contain more, fewer, or different components than those shown in FIG. 2 and the functionality of the components as described herein may be distributed differently from the description herein."
230,optical sensor,contact image sensor,hyper,1,"In an aspect, in accordance with said example embodiments, the optical system is a barcode verifier and the optical system comprises at least one optical sensor and the at least one optical sensor is a contact image sensor or a linear scan head incorporated in the barcode verifier."
231,label printer,optical system,rhyper,-1,"FIG. 1 illustrates a perspective view representing a label printer including an optical system and another perspective view representing various components of the label printer, in accordance with some example embodiments described herein;FIGS. 2A-2D illustrate a cutaway inside view of an optical system, in accordance with some example embodiments described herein;FIG. 3 illustrates various components of an optical system, in accordance with various example embodiments described herein;FIG. 4 illustrates various components of an optical system communicatively coupled to a printing subsystem, in accordance with another example embodiment described herein;FIG. 5 depicts a plurality of exemplary printed indicia, for example, barcodes, on individual labels of a print media, in accordance with some example embodiments described herein;FIG. 6 depicts an exemplary printed barcode (a portion of a QR Code) comprising a two-dimensional barcode, in accordance with some example embodiments described herein;FIG. 7 schematically depicts an exemplary printer comprising the optical system and operations of the printer, in accordance with some example embodiments described herein;FIG. 8 schematically illustrates a block diagram of a printing and verifying device including the optical system for verifying a printed media, in accordance with some example embodiments described herein;FIG. 9 illustrates examples of labels having properly and improperly printed barcodes that are verified by the optical system, in accordance with some example embodiments described herein;FIG. 10 is a block diagram showing another example embodiment of a printing and verifying device comprising a contact image sensing device, in accordance with some example embodiments;FIG. 11 illustrates a test label that may be used for self-calibration ofthe optical system, in accordance with some example embodiments described herein;FIG. 12 is a flow diagram illustrating a method for printing and verifying labels by the optical system, in accordance with various example embodiments described herein;FIG. 13 schematically depicts a diagram of a lens array of the optical system, in accordance with some example embodiments described herein;FIG. 14 schematically depicts a test pattern, in accordance with some example embodiments described herein;FIG. 15 schematically depicts an aperture size with respect to the test pattern of FIG. 14, in accordance with some example embodiments described herein;FIG. 16 schematically depicts a block diagram of a barcode verifier, in accordance with some example embodiments described herein;FIG. 17 schematically depicts a graph including a scan reflectance profile (SRP) of the test pattern of FIG. 14, in accordance with some example embodiments described herein;FIG. 18 schematically depicts a reference graph correlating element-reflectance values with aperture sizes, in accordance with some example embodiments described herein;FIG. 19 schematically depicts a flow diagram of a method for creating a reference graph, in accordance with some example embodiments described herein;FIG. 20 schematically depicts a flow diagram of a method for characterizing an optical system, in accordance with some example embodiments described herein;FIG. 21 schematically depicts a flow diagram of another method for characterizing an optical system, in accordance with some example embodiments described herein;FIG. 22 schematically depicts a flow diagram of a method for determining an effective aperture size of an optical system using pre-printed calibration card, in accordance with some example embodiments described herein;FIG. 23 schematically depicts a flow diagram of a method for determining an effective aperture size of an optical system using self-printed calibration card, in accordance with some example embodiments described herein;FIGS. 24A, 24B, and 24C illustrate examples of a test pattern that may be printed by a printing subsystem, in accordance with some example embodiments described herein;FIGS. 25A and 25B illustrate example scenarios representing an over-burn, an under-burn, and an optimal printing by a printing subsystem of a printer, in accordance with some example embodiments described herein;FIG. 26 schematically depicts a flow diagram of a method for characterizing an optical system using self-printed calibration card, in accordance with some example embodiments described herein;FIG. 27 schematically depicts a flow diagram of a method for characterizing an optical system using self-printed calibration card, in accordance with some example embodiments described herein;FIGS. 28A, 28B, and28C illustrate example scan reflectance profile (SRPs) generated upon scanning different test patterns by an optical system, in accordance with some example embodiments described herein;FIG 29 schematically depicts a flow diagram of a method for characterizing an optical system using self-printed calibration card, in accordance with another example embodiment described herein;FIG. 30 schematically depicts a flow diagram of a method for determining respective widths of at least one space and at least one bar in a test pattern of a self-printed calibration card used for characterizing an optical system, in accordance with another example embodiment described herein;FIG. 31 illustrates another example scan reflectance profile (SRP) generated upon scanning a test pattern by an optical system, in accordance with some example embodiments described herein;FIG. 32 illustrates an example table accessed by a printing and verifying system for determining count of pixels corresponding to white elements and count of pixels corresponding to black elements upon scanning a test pattern, in accordance with some example embodiments described herein;FIG. 33 schematically depicts a flow diagram of a method for characterizing an optical system based on characteristics of an image to be printed, in accordance with some example embodiments described herein;FIG. 34 illustrates an example reference table used by a printing and verifying system for determining an effective aperture size and correspondingly an effective resolution for calibrating an optical system, in accordance with some example embodiments described herein;FIG. 35 schematically depicts a flow diagram of a method for characterizing an optical system based on characteristics of an image to be printed, in accordance with some example embodiments described herein;FIG. 36A illustrates an example image to be printed by a printer and scanned by a verifier in a printing and verifying system, in accordance with some example embodiments described herein;FIG. 36B illustrates identification of one or more regions in the image based on characteristic data associated with respective regions, in accordance with some example embodiments described herein; andFIG. 37 schematically depicts a flow diagram of a method for characterizing an optical system based on resolution requirements of different regions in a printed image, in accordance with some example embodiments described herein."
232,computing device,smart phone,rhyper,-1,"The term ""server"" is used to refer to any computing device capable of functioning as a server, such as a master exchange server, web server, mail server, document server, or any other type of server. A server may be a dedicated computing device or a computing device including a server module (e.g., running an application which may cause the computing device to operate as a server). A server module (e.g., server application) may be a full function server module, or a light or secondary server module (e.g., light or secondary server application) that is configured to provide synchronization services among the dynamic databases on computing devices. A light server or secondary server may be a slimmed-down version of server type functionality that can be implemented on a computing device, such as a smart phone, thereby enabling it to function as an Internet server (e.g., an enterprise e-mail server) only to the extent necessary to provide the functionality described herein."
233,reference table,effective aperture,rhyper,-1,"In this regard, some example embodiments described herein relate to automatically selecting, in run time, (i.e. while a printer is printing a barcode), an effective aperture size applicable for a printer's verifier from a set of different aperture sizes determined using a software. In this aspect, information of a current image to be printed in the printer's image buffer, (i.e. an image to be currently printed) is used to configure the verifier to select a resolution and the aperture size before scanning the barcode. Selection of an aperture size of the verifier is based on various characteristics of the image (for example, a size of a barcode in a current reference image to be printed). Selection of resolution is performed based on utilizing a reference table including effective aperture sizes of the verifier for different resolutions of linear sensor of the verifier. Accordingly, at the run-time (i.e. while printing a barcode on each label of print media), characteristics including a size of the barcode is accessed from reference image in printer's image buffer and correspondingly an effective aperture size of the verifier applicable for the barcode size is identified. Further, a resolution applicable for the identified aperture size is identified from the reference table. Additionally, post identification of the aperture size based on the barcode size, some level of software blurring and binning of pixels could be performed using conventional approaches to blur the image for better approximating the aperture size to be used for scanning the barcode. Finally, the identified aperture size and the identified resolution are used for scanning the printed barcode for verification."
234,print media,print media,rhyper,-1,"The sensor housing 202, together with the window 204 (on bottom surface), forms an enclosure for protecting the internal components, such as the circuit board 206, the CIS 208, the lens array 210, the light board 212, the plurality of light sources 214 (e.g., light emitting diodes (LEDs)), and the ultrasonic vibrator 216 of the in-line indicia verifier 126. The sensor housing 202 of the printing and verifying system 124 is positioned near the printhead 110 to allow monitoring of the print quality with minimal delay after the print image is imprinted on print media, such as the print media 114. In an embodiment, the in-line indicia verifier 126 includes the sensor housing 202 disposed in the interior of the housing of the in-line indicia verifier 126."
235,test pattern,test pattern,rhyper,-1,"In operation, the optical sensor 1604 optically senses a test pattern, such as the test pattern 1402 of FIG. 14 or other suitable patterns having distinguishable test elements, such as test element 1404. The processing device 1602 utilizes the SRP processing unit 1610 to create an SRP from data obtained from the scanned test pattern. The processing device 1602 may also utilize the SRP processing unit 1610 to determine information from the SRP. For example, according some example embodiments described herein, the processing device 1602 may determine a parameter related to the reflectance characteristics of the elements of the test pattern. One reflectance parameter of interest may be a parameter referred to as Element Reflectance Non-uniformity (ERN)."
236,test pattern,test pattern,rhyper,-1,"In accordance with some example embodiments described herein, the barcode verifier 1600 can be used for characterizing an optical system (i.e., the barcode verifier 1600 can be used as a testing system). The testing system may use a scannable test pattern, such as the test pattern 1402 shown in FIG. 14. The test pattern is configured with a plurality of linear bars and a test element having a width that is narrower than each of the plurality of linear bars or spaces. The optical sensor 1604 is configured to scan the scannable test pattern to obtain a scan reflectance profile (SRP). The processing device 1602 is configured to calculate an element-reflectance value from information in the SRP related to the scanned test element. The barcode verifier 1600 further utilizes a reference graph having a curve that correlates aperture dimensions to element-reflectance values. An example of a reference curve is shown in FIG. 18 and is described below. The processing device 1602 is further configured to interpolate an effective aperture dimension of the optical sensor 1604 from the reference graph based on the calculated element-reflectance value."
237,optical system,optical system,rhyper,-1,"FIG. 27 schematically depicts a flow diagram 2700 of a method for characterizing an optical system, such as the optical system 300, using self-printed calibration card in accordance with some example embodiments described herein."
238,optical system,optical system,rhyper,-1,"FIG 29 schematically depicts a flow diagram 2900 of a method for characterizing an optical system, such as the optical system 300, using self-printed calibration card in accordance with another example embodiment described herein."
239,test pattern,test pattern,rhyper,-1,"In another example embodiment, to calculate a count of pixels representing the white elements (such as the wide white elements 2407 and narrow white elements 2408 of the test pattern 2404) and a count of pixels representing the black elements (such as the wide black elements 2405 and/or the narrow black elements 2406 in the test pattern 2404) in the test pattern, the processing circuitry 304 and/or the processing circuitry 402 may utilize a table. In this regard, upon scanning the test pattern by the optical system 300, the processing circuitry 304 and/or the processing circuitry 402 may determine a pixel value difference from one pixel and a neighboring pixel in a scan profile from the table. In this regard, the table may store values of pixel 3206 representing reflectance values sensed by each of the one or more optical sensors 302 of the optical system 300 upon scanning a printed image including the test pattern, such as the test pattern 2404."
240,optical system,optical system,rhyper,-1,"FIG. 33 schematically depicts a flow diagram of a method 3300 for characterizing an optical system such as the optical system 300 based on characteristics of an image to be printed, in accordance with some example embodiments. In one example embodiment, the method 3300 may start from pointer C that in connection with step 2110 after the step 2108 described in FIG. 21. Referring to the step 2110 of FIG. 21, the processing circuitry 304 and/or the processing circuitry 402 analyzes a printed image or an image to be printed to determine different verification requirements for scanning the printed image by the optical system 300. In this regard, to determine verification requirements with respect to characteristics of at least one indicia to be printed on the image, steps 3302-3306 of the method 3300 may be performed."
241,color schemes,color scheme,rhyper,-1,"A ""perfectly"" printed label has had its size and luminance adjusted according to the present invention, and processed with the difference algorithm. A perfectly printed label will result in a perfectly black image. Any imperfections will be rendered in a grayscale range, where pure white will be a full mismatch in pixel comparison. This approach can be used to quantify printing errors beyond those simply evaluated according to a print quality standard (e.g., ISO/IEC 15415). It may be noted that the above color scheme is merely for exemplary purposes and should not be construed to limit the scope of the disclosure. Other color schemes, such as a color scheme inverse of the exemplary color scheme described above, or an entirely different color scheme may be implemented, without deviation from the scope of the disclosure."
242,digital image,the computer,rhyper,-1,"Some example implementations provide a computer-readable storage medium for use in testing a battery, the computer-readable storage medium being non-transitory and having computer-readable program code stored therein that in response to execution by a processor, causes a computer to at least access a digital image captured during a test of a battery in which the battery is punctured or heated in a test environment to cause the battery to produce a fire having a flame that extends out from the battery, the digital image being of a scene that includes at least a portion of the test environment and the flame that extends out from the battery, the digital image being captured using a digital camera that forms the digital image using visible light where the digital camera is positioned external to the test environment; and determine dimensions of the flame in the scene based on the digital image, including the computer being caused to at least: produce a quiver plot of the scene from the digital image, the quiver plot including a plurality of velocity vectors that represent motion of gases in the scene, the motion of gases including motion of gases in and around the flame, the plurality of velocity vectors originating at a respective plurality of points in the quiver plot and having respective magnitudes and directions; identify a point of the respective plurality of points as an origin point to represent an origin of the flame in the scene; select or receive selection of other points of the respective plurality of points based on similarity of at least the directions of the velocity vectors originating at the origin point and the other points, the origin point and the other points defining a polygon in the quiver plot that is an approximate outline of the flame; determine dimensions of the polygon in the quiver plot; translate the dimensions of the polygon in the quiver plot to corresponding dimensions of a polygon in the digital image; translate the corresponding dimensions of the polygon in the digital image to dimensions of the flame in the scene; and generate a displayable report that includes at least the dimensions of the flame."
243,digital image,the computer,rhyper,-1,"Clause 1. A method of testing a battery comprising:setting up the battery in a test environment;puncturing or heating the battery in the test environment to cause the battery to produce a fire having a flame that extends out from the battery;capturing a digital image of a scene that includes at least a portion of the test environment and the flame that extends out from the battery, the digital image being captured using a digital camera that forms the digital image using visible light where the digital camera is positioned external to the test environment; anduploading the digital image of the scene to a computer configured to determine dimensions of the flame in the scene based thereon, including the computer being configured to at least:produce a quiver plot of the scene from the digital image, the quiver plot including a plurality of velocity vectors that represent motion of gases in the scene, the motion of gases including motion of gases in and around the flame, the plurality of velocity vectors originating at a respective plurality of points in the quiver plot and having respective magnitudes and directions;identify a point of the respective plurality of points as an origin point to represent an origin of the flame in the scene;select or receive selection of other points of the respective plurality of points based on similarity of at least the directions of the velocity vectors originating at the origin point and the other points, the origin point and the other points defining a polygon in the quiver plot that is an approximate outline of the flame;determine dimensions of the polygon in the quiver plot;translate the dimensions of the polygon in the quiver plot to corresponding dimensions of a polygon in the digital image;translate the corresponding dimensions of the polygon in the digital image to dimensions of the flame in the scene; andgenerate a displayable report that includes at least the dimensions of the flame.Clause 2. The method of Clause 1, wherein setting up the battery in the test environment includes setting up the battery between the digital camera and a fixed background within a back region of the test environment, and the digital image of the scene includes at least a portion of the fixed background and the flame that extends out from the battery.Clause 3. The method of Clause 1 or Clause 2, wherein the computer is further configured to:convert the digital image from color to black-and-white before the quiver plot is produced from the digital image.Clause 4. The method of any preceding Clause, wherein the computer is further configured to:identify as abnormal any velocity vectors of the plurality of velocity vectors based on at least a threshold dissimilarity of at least the directions of the velocity vectors and neighboring velocity vectors in the quiver plot;construct new velocity vectors for the velocity vectors identified as abnormal from an interpolation of the neighboring velocity vectors; andreplace the velocity vectors identified as abnormal with the new velocity vectors, the velocity vectors identified as abnormal being replaced before the origin point is identified and the other points are selected.Clause 5. The method of any preceding Clause, wherein the computer being configured to select or receive selection of the other points includes the computer being configured to perform an iterative process in which a first iteration includes the computer being configured to at least:identify for a point in the quiver plot that in the first iteration is the origin point, a velocity vector originating at a neighboring point that is most similar in at least direction to a velocity vector originating at the point; andselect the neighboring point as one of the other points that defines the polygon in the quiver plot,wherein in each of at least some iterations of the iterative process after the first iteration, the point in the quiver plot is the neighboring point selected in an immediately preceding iteration, and the computer being configured to identify the velocity vector excludes the velocity vector originating at the point in the immediately preceding iteration.Clause 6. The method of any of Clauses 1 to 4, wherein the computer being configured to select or receive selection of the other points includes the computer being configured to perform an iterative process in which a first iteration includes the computer being configured to at least:identify for a point in the quiver plot that in the first iteration is the origin point, velocity vectors originating at neighboring points that have at least a threshold similarity of at least direction to a velocity vector originating at the point; andselect the neighboring points as candidate points,wherein in each of at least some iterations of the iterative process after the first iteration, the computer being configured to identify velocity vectors for the point in the quiver plot includes being configured to identify velocity vectors for each of the neighboring points selected in an immediately preceding iteration, andwherein the computer being configured to select or receive selection of the other points further includes being configured to produce a cluster of the candidate points over iterations of the iterative process, and select points on a boundary of the cluster as the other points that with the origin point defines the polygon.Clause 7. The method of any preceding Clause, wherein capturing the digital image of the scene includes capturing a digital video of the scene, the digital video of the scene including a series of digital images, andwherein uploading the digital image of the scene to the computer includes uploading the digital video of the scene to the computer and wherein the computer is further configured to determine the dimensions of the flame in each digital image of the series of digital images, and wherein the computer is further configured to determine a change or a rate of change in a dimension of the dimensions of the flame over the series of digital images, the displayable report further including the change in the dimension of the flame.Clause 8. The method of any preceding Clause, wherein capturing the digital image of the scene includes capturing a digital video of the scene, the digital video of the scene including a series of digital images, andwherein uploading the digital image of the scene to the computer includes uploading the digital video of the scene to the computer and wherein the computer is further configured to determine the dimensions of the flame in each digital image of the series of digital images, and wherein the computer is further configured to determine a duration of the flame based on the dimensions of the flame over the series of digital images, the displayable report further including the duration of the flame.Clause 9. A computer for use in testing a battery, the computer comprising:a memory configured to store a digital image captured during a test of a battery in which the battery is punctured or heated in a test environment to cause the battery to produce a fire having a flame that extends out from the battery, the digital image being of a scene that includes at least a portion of the test environment and the flame that extends out from the battery, the digital image being captured using a digital camera that forms the digital image using visible light where the digital camera is positioned external to the test environment; anda processor coupled to the memory and programmed to access the digital image and determine dimensions of the flame in the scene based thereon, including the processor being programmed to at least:produce a quiver plot of the scene from the digital image, the quiver plot including a plurality of velocity vectors that represent motion of gases in the scene, the motion of gases including motion of gases in and around the flame, the plurality of velocity vectors originating at a respective plurality of points in the quiver plot and having respective magnitudes and directions;identify a point of the respective plurality of points as an origin point to represent an origin of the flame in the scene;select or receive selection of other points of the respective plurality of points based on similarity of at least the directions of the velocity vectors originating at the origin point and the other points, the origin point and the other points defining a polygon in the quiver plot that is an approximate outline of the flame;determine dimensions of the polygon in the quiver plot;translate the dimensions of the polygon in the quiver plot to corresponding dimensions of a polygon in the digital image;translate the corresponding dimensions of the polygon in the digital image to dimensions of the flame in the scene; andgenerate a displayable report that includes at least the dimensions of the flame.Clause 10. The computer of Clause 9, wherein the battery in the test environment includes the battery between the digital camera and a fixed background within a back region of the test environment, and the digital image of the scene includes at least a portion of the fixed background and the flame that extends out from the battery.Clause 11. The computer of Clause 9 or Clause 10, wherein the processor is programmed to further convert the digital image from color to black-and-white before the quiver plot is produced from the digital image.Clause 12. The computer of any of Clauses 9 to 11, wherein the processor is programmed to further:identify as abnormal any velocity vectors of the plurality of velocity vectors based on at least a threshold dissimilarity of at least the directions of the velocity vectors and neighboring velocity vectors in the quiver plot;construct new velocity vectors for the velocity vectors identified as abnormal from an interpolation of the neighboring velocity vectors; andreplace the velocity vectors identified as abnormal with the new velocity vectors, the velocity vectors identified as abnormal being replaced before the origin point is identified and the other points are selected.Clause 13. The computer of any of Clauses 9 to 12, wherein the processor being programmed to select or receive selection of the other points includes the processor being programmed to perform an iterative process in which a first iteration includes the processor being programmed to at least:identify for a point in the quiver plot that in the first iteration is the origin point, a velocity vector originating at a neighboring point that is most similar in at least direction to a velocity vector originating at the point; andselect the neighboring point as one of the other points that defines the polygon in the quiver plot,wherein in each of at least some iterations of the iterative process after the first iteration, the point in the quiver plot is the neighboring point selected in an immediately preceding iteration, and the processor being programmed to identify the velocity vector excludes the velocity vector originating at the point in the immediately preceding iteration.Clause 14. The computer of any of Clauses 9 to 12, wherein the processor being programmed to select or receive selection of the other points includes the processor being programmed to perform an iterative process in which a first iteration includes the processor being programmed to at least:identify for a point in the quiver plot that in the first iteration is the origin point, velocity vectors originating at neighboring points that have at least a threshold similarity of at least direction to a velocity vector originating at the point; andselect the neighboring points as candidate points,wherein in each of at least some iterations of the iterative process after the first iteration, the processor being programmed to identify velocity vectors for the point in the quiver plot includes being programmed to identify velocity vectors for each of the neighboring points selected in an immediately preceding iteration, andwherein the processor being programmed to select or receive selection of the other points further includes being programmed to produce a cluster of the candidate points over iterations of the iterative process, and select points on a boundary of the cluster as the other points that with the origin point defines the polygon.Clause 15. The computer of any of Clauses 9 to 14, wherein the memory being configured to store the digital image of the scene includes the memory being configured to store a digital video of the scene, the digital video of the scene including a series of digital images, andwherein the processor is programmed to access the digital video of the scene and determine the dimensions of the flame in each digital image of the series of digital images, and wherein the processor is further programmed to determine a change or a rate of change in a dimension of the dimensions of the flame over the series of digital images, the displayable report further including the change in the dimension of the flame.Clause 16. The computer of any of Clauses 9 to 15, wherein the memory being configured to store the digital image of the scene includes the memory being configured to store a digital video of the scene, the digital video of the scene including a series of digital images, andwherein the processor is programmed to access the digital video of the scene and determine the dimensions of the flame in each digital image of the series of digital images, and wherein the processor is further programmed to determine a duration of the flame based on the dimensions of the flame over the series of digital images, the displayable report further including the duration of the flame.Clause 17. A computer-readable storage medium for use in testing a battery, the computer-readable storage medium being non-transitory and having computer-readable program code stored therein that in response to execution by a processor, causes a computer to at least:access a digital image captured during a test of a battery in which the battery is punctured or heated in a test environment to cause the battery to produce a fire having a flame that extends out from the battery, the digital image being of a scene that includes at least a portion of the test environment and the flame that extends out from the battery, the digital image being captured using a digital camera that forms the digital image using visible light where the digital camera is positioned external to the test environment; anddetermine dimensions of the flame in the scene based on the digital image, including the computer being caused to at least:produce a quiver plot of the scene from the digital image, the quiver plot including a plurality of velocity vectors that represent motion of gases in the scene, the motion of gases including motion of gases in and around the flame, the plurality of velocity vectors originating at a respective plurality of points in the quiver plot and having respective magnitudes and directions;identify a point of the respective plurality of points as an origin point to represent an origin of the flame in the scene;select or receive selection of other points of the respective plurality of points based on similarity of at least the directions of the velocity vectors originating at the origin point and the other points, the origin point and the other points defining a polygon in the quiver plot that is an approximate outline of the flame;determine dimensions of the polygon in the quiver plot;translate the dimensions of the polygon in the quiver plot to corresponding dimensions of a polygon in the digital image;translate the corresponding dimensions of the polygon in the digital image to dimensions of the flame in the scene; andgenerate a displayable report that includes at least the dimensions of the flame.Clause 18. The computer-readable storage medium of Clause 17, wherein the battery in the test environment includes the battery between the digital camera and a fixed background within a back region of the test environment, and the digital image of the scene includes at least a portion of the fixed background and the flame that extends out from the battery.Clause 19. The computer-readable storage medium of Clause 17 or Clause 18, wherein the computer-readable storage medium has computer-readable program code stored therein that, in response to execution by the processor, causes the computer to further convert the digital image from color to black-and-white before the quiver plot is produced from the digital image.Clause 20. The computer-readable storage medium of any of Clauses 17 to 19, wherein the computer-readable storage medium has computer-readable program code stored therein that, in response to execution by the processor, causes the computer to further:identify as abnormal any velocity vectors of the plurality of velocity vectors based on at least a threshold dissimilarity of at least the directions of the velocity vectors and neighboring velocity vectors in the quiver plot;construct new velocity vectors for the velocity vectors identified as abnormal from an interpolation of the neighboring velocity vectors; andreplace the velocity vectors identified as abnormal with the new velocity vectors, the velocity vectors identified as abnormal being replaced before the origin point is identified and the other points are selected.Clause 21. The computer-readable storage medium of any of Clauses 17 to 20, wherein the computer being caused to select or receive selection of the other points includes the computer being caused to perform an iterative process in which a first iteration includes the computer being caused to at least:identify for a point in the quiver plot that in the first iteration is the origin point, a velocity vector originating at a neighboring point that is most similar in at least direction to a velocity vector originating at the point; andselect the neighboring point as one of the other points that defines the polygon in the quiver plot,wherein in each of at least some iterations of the iterative process after the first iteration, the point in the quiver plot is the neighboring point selected in an immediately preceding iteration, and the computer being programmed to identify the velocity vector excludes the velocity vector originating at the point in the immediately preceding iteration.Clause 22. The computer-readable storage medium of any of Clauses 17 to 20, wherein the computer being caused to select or receive selection of the other points includes the computer being caused to perform an iterative process in which a first iteration includes the computer being caused to at least:identify for a point in the quiver plot that in the first iteration is the origin point, velocity vectors originating at neighboring points that have at least a threshold similarity of at least direction to a velocity vector originating at the point; andselect the neighboring points as candidate points,wherein in each of at least some iterations of the iterative process after the first iteration, the computer being caused to identify velocity vectors for the point in the quiver plot includes being caused to identify velocity vectors for each of the neighboring points selected in an immediately preceding iteration, andwherein the computer being caused to select or receive selection of the other points further includes being caused to produce a cluster of the candidate points over iterations of the iterative process, and select points on a boundary of the cluster as the other points that with the origin point defines the polygon.Clause 23. The computer-readable storage medium of any of Clauses 17 to 22, wherein the memory being configured to store the digital image of the scene includes the memory being configured to store a digital video of the scene, the digital video of the scene including a series of digital images, andwherein the computer is caused to access the digital video of the scene and determine the dimensions of the flame in each digital image of the series of digital images, and wherein the computer is further caused to determine a change or a rate of change in a dimension of the dimensions of the flame over the series of digital images, the displayable report further including the change in the dimension of the flame.Clause 24. The computer-readable storage medium of any of Clauses 17 to 23, wherein the memory being configured to store the digital image of the scene includes the memory being configured to store a digital video of the scene, the digital video of the scene including a series of digital images, andwherein the computer is caused to access the digital video of the scene and determine the dimensions of the flame in each digital image of the series of digital images, and wherein the computer is further caused to determine a duration of the flame based on the dimensions of the flame over the series of digital images, the displayable report further including the duration of the flame.Clause 25. A method of testing a battery comprising:setting up the battery in a test environment;puncturing or heating the battery in the test environment to cause the battery to produce a fire having a flame that extends out from the battery;capturing a digital image of a scene that includes at least a portion of the test environment and the flame that extends out from the battery, the digital image being captured using a digital camera that forms the digital image using visible light where the digital camera is positioned external to the test environment; anduploading the digital image of the scene to a computer, the computer:producing a quiver plot of the scene from the digital image, the quiver plot including a plurality of velocity vectors that represent motion of gases in the scene, the motion of gases including motion of gases in and around the flame, the plurality of velocity vectors originating at a respective plurality of points in the quiver plot and having respective magnitudes and directions;identifying a point of the respective plurality of points as an origin point to represent an origin of the flame in the scene;selecting or receiving selection of other points of the respective plurality of points based on similarity of at least the directions of the velocity vectors originating at the origin point and the other points, the origin point and the other points defining a polygon in the quiver plot that is an approximate outline of the flame;determining dimensions of the polygon in the quiver plot;translating the dimensions of the polygon in the quiver plot to corresponding dimensions of a polygon in the digital image;translating the corresponding dimensions of the polygon in the digital image to dimensions of the flame in the scene; andgenerating a displayable report that includes at least the dimensions of the flame.Clause 26. The method of Clause 25, wherein setting up the battery in the test environment includes setting up the battery between the digital camera and a fixed background within a back region of the test environment, and the digital image of the scene includes at least a portion of the fixed background and the flame that extends out from the battery.Clause 27. The method of Clause 25 or Clause 26, the method further comprising the computer converting the digital image from color to black-and-white before the quiver plot is produced from the digital image.Clause 28. The method of any of Clauses 25 to 27, the method further comprising the computer:identifying as abnormal any velocity vectors of the plurality of velocity vectors based on at least a threshold dissimilarity of at least the directions of the velocity vectors and neighboring velocity vectors in the quiver plot;constructing new velocity vectors for the velocity vectors identified as abnormal from an interpolation of the neighboring velocity vectors; andreplacing the velocity vectors identified as abnormal with the new velocity vectors, the velocity vectors identified as abnormal being replaced before the origin point is identified and the other points are selected.Clause 29. The method of any of Clauses 25 to 28, wherein the computer selecting or receiving selection of the other points includes the computer performing an iterative process in which a first iteration includes the computer:identifying for a point in the quiver plot that in the first iteration is the origin point, a velocity vector originating at a neighboring point that is most similar in at least direction to a velocity vector originating at the point; andselecting the neighboring point as one of the other points that defines the polygon in the quiver plot,wherein in each of at least some iterations of the iterative process after the first iteration, the point in the quiver plot is the neighboring point selected in an immediately preceding iteration, and wherein the computer identifying the velocity vector excludes the velocity vector originating at the point in the immediately preceding iteration.Clause 30. The method of any of Clauses 25 to 28, wherein the computer selecting or receiving selection of the other points includes the computer performing an iterative process in which a first iteration includes the computer:identifying for a point in the quiver plot that in the first iteration is the origin point, velocity vectors originating at neighboring points that have at least a threshold similarity of at least direction to a velocity vector originating at the point; andselecting the neighboring points as candidate points,wherein in each of at least some iterations of the iterative process after the first iteration, the computer identifying velocity vectors for the point in the quiver plot includes identifying velocity vectors for each of the neighboring points selected in an immediately preceding iteration, andwherein the computer selecting or receiving selection of the other points further includes producing a cluster of the candidate points over iterations of the iterative process, and selecting points on a boundary of the cluster as the other points that with the origin point defines the polygon.Clause 31. The method of any of Clauses 25 to 30, wherein capturing the digital image of the scene includes capturing a digital video of the scene, the digital video of the scene including a series of digital images, andwherein uploading the digital image of the scene to the computer includes uploading the digital video of the scene to the computer, and wherein the method further comprises the computer:determining the dimensions of the flame in each digital image of the series of digital images, anddetermining a change or a rate of change in a dimension of the dimensions of the flame over the series of digital images, the displayable report further including the change in the dimension of the flame.Clause 32. The method of any of Clauses 25 to 31, wherein capturing the digital image of the scene includes capturing a digital video of the scene, the digital video of the scene including a series of digital images, andwherein uploading the digital image of the scene to the computer includes uploading the digital video of the scene to the computer, and wherein the method further comprises the computer:determining the dimensions of the flame in each digital image of the series of digital images, anddetermining a duration of the flame based on the dimensions of the flame over the series of digital images, the displayable report further including the duration of the flame."
244,image sensor,complementary metal oxide semiconductor,rhyper,-1,"The imaging unit 13 has an image sensor such as a complementary metal oxide semiconductor (CMOS) or a charge coupled device (CCD). The image sensor has a plurality of imaging modules and each imaging module has a plurality of light receiving elements that detect light of a plurality of colors including a specific color. As a specific color to be detected by the light receiving element, for example, red (R) suitable for detection of the tail lamp can be selected."
245,text processing,optical character recognition,rhyper,-1,"Characters may be identified and extracted from the image data using a variety of methods that are known to the skilled person. Such techniques are commonly used in image to text processing such as optical character recognition methods. Preferably, a convolutional neural network will be used."
246,text field,text field,hyper,1,"Aspect 10. The computer-implemented method of aspect 7 or 8, wherein at least one text field is a text field."
